{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "30ea520d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "import nltk, re, pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "35a7fe20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'>\n",
      "1359028\n",
      "b'\\xef\\xbb\\xbfThe Project Gutenberg eBook of Crime and Punishment\\r\\n    \\r\\nThis ebook is for the use of anyone anywhere in the United States and\\r\\nmost other parts of the world at no cost and with almost no restrictions\\r\\nwhatsoever. You may copy it, give it away or re-use it under the terms\\r\\nof the Project Gutenberg License included with this ebook or online\\r\\nat www.gutenberg.org. If you are not located in the United States,\\r\\nyou will have to check the laws of the country where you are located\\r\\nbefore using this eBook.\\r\\n\\r\\nTitle: Crime and Punishment\\r\\n\\r\\n\\r\\nAuthor: Fyodor Dostoyevsky\\r\\n\\r\\nTranslator: Constance Garnett\\r\\n\\r\\nRelease date: March 28, 2006 [eBook #2554]\\r\\n                Most recently updated: August '\n"
     ]
    }
   ],
   "source": [
    "from urllib.request import urlopen\n",
    "url = \"https://www.gutenberg.org/cache/epub/2554/pg2554.txt\"\n",
    "raw = urlopen(url).read()\n",
    "rawStr = str(raw)\n",
    "print(type(rawStr))\n",
    "print(len(rawStr))\n",
    "print(raw[:700])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c92edfa",
   "metadata": {},
   "outputs": [],
   "source": [
    " #proxies = {'http': 'http://www.someproxy.com:3128'}\n",
    " #raw = urlopen(url, proxies=proxies).read( )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6ca182a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "225109\n",
      "[\"b'\\\\xef\\\\xbb\\\\xbfThe\", 'Project', 'Gutenberg', 'eBook', 'of', 'Crime', 'and', 'Punishment\\\\r\\\\n', '\\\\r\\\\nThis', 'ebook']\n"
     ]
    }
   ],
   "source": [
    "tokens = nltk.word_tokenize(rawStr)\n",
    "print(type(tokens))\n",
    "print(len(tokens))\n",
    "print( tokens[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1e2b5fce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'nltk.text.Text'>\n",
      "Project\n",
      "Katerina Ivanovna; Pyotr Petrovitch; Avdotya Romanovna; Pulcheria\n",
      "Alexandrovna; Rodion Romanovitch; Marfa Petrovna; \\xe2\\x80\\x9d cried;\n",
      "Sofya Semyonovna; \\xe2\\x80\\x9d said; old woman; \\xe2\\x80\\x9d\n",
      "Raskolnikov; Porfiry Petrovitch; Project Gutenberg\\xe2\\x84\\xa2;\n",
      "don\\xe2\\x80\\x99t know; great deal; Amalia Ivanovna; young man; Hay\n",
      "Market; police station; Nikodim Fomitch\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "text = nltk.Text(tokens)\n",
    "print(type(text))\n",
    "print(text[1])\n",
    "print(text.collocations())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "05bf5d72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6019\n",
      "-1\n",
      "\\r\\nvast multitude of mourners, who \\xe2\\x80\\x9cgave the hapless man the funeral of a\\r\\nking.\\xe2\\x80\\x9d He is still probably the most widely read writer in Russia.\\r\\n\\r\\nIn the words of a Russian critic, who seeks to explain the feeling\\r\\ninspired by Dostoevsky: \\xe2\\x80\\x9cHe was one of ourselves, a man of our blood and\\r\\nour bone, but one who has suffered and has seen so much more deeply than\\r\\nwe have his insight impresses us as wisdom... that wisdom of the heart\\r\\nwhich we seek that we may learn from it how to live. All his other\\r\\ngifts came to him from nature, this he won for himself and through it he\\r\\nbecame great.\\xe2\\x80\\x9d\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\nCRIME AND PUNISHMENT\\r\\n\\r\\n\\r\\n\\r\\n\\r\\nPART I\\r\\n\\r\\n\\r\\n\\r\\nCHAPTER I\\r\\n\\r\\nOn an exceptionally hot evening early in July a young man came out of\\r\\nthe garret in which he lodged in S. Place and walked slowly, as though\\r\\nin hesitation, towards K. bridge.\\r\\n\\r\\nHe had successfully avoided meeting his landlady on the staircase. His\\r\\ngarret was under the roof of a high, five-storied house and was more\\r\\nlike a cupboard than a room. The landlady who provided him with garret,\\r\\ndinners, and attendance, lived on the floor below, and every time\\r\\nhe went out he was obliged to pass her kitchen, the door of which\\r\\ninvariably stood open. And each time he passed, the young man had a\\r\\nsick, frightened feeling, which made him scowl and feel ashamed. He was\\r\\nhopelessly in debt to his landlady, and was afraid of meeting her.\\r\\n\\r\\nThis was not because he was cowardly and abject, quite the contrary; but\\r\\nfor some time past he had been in an overstrained irritable condition,\\r\\nverging on hypochondria. He had become so completely absorbed in\\r\\nhimself, and isolated from his fellows that he dreaded meeting, not\\r\\nonly his landlady, but anyone at all. He was crushed by poverty, but the\\r\\nanxieties of his position had of late ceased to weigh upon him. He had\\r\\ngiven up attending to matters of practical importance; he had lost all\\r\\ndesire to do so. Nothing that any landlady could do had a real terror\\r\\nfor him. But to be stopped on the stairs, to be forced to listen to her\\r\\ntrivial, irrelevant gossip, to pestering demands for payment, threats\\r\\nand complaints, and to rack his brains for excuses, to prevaricate, to\\r\\nlie--no, rather than that, he would creep down the stairs like a cat and\\r\\nslip out unseen.\\r\\n\\r\\nThis evening, however, on coming out into the street, he became acutely\\r\\naware of his fears.\\r\\n\\r\\n\\xe2\\x80\\x9cI want to attempt a thing _like that_ and am frightened by these\\r\\ntrifles,\\xe2\\x80\\x9d he thought, with an odd smile. \\xe2\\x80\\x9cHm... yes, all is in a man\\xe2\\x80\\x99s\\r\\nhands and he lets it all slip from cowardice, that\\xe2\\x80\\x99s an axiom. It would\\r\\nbe interesting to know what it is men are most afraid of. Taking a new\\r\\nstep, uttering a new word is what they fear most.... But I am talking\\r\\ntoo much. It\\xe2\\x80\\x99s because I chatter that I do nothing. Or perhaps it is\\r\\nthat I chatter because I do nothing. I\\xe2\\x80\\x99ve learned to chatter this\\r\\nlast month, lying for days together in my den thinking... of Jack the\\r\\nGiant-killer. Why am I going there now? Am I capable of _that_? Is\\r\\n_that_ serious? It is not serious at all. It\\xe2\\x80\\x99s simply a fantasy to amuse\\r\\nmyself; a plaything! Yes, maybe it is a plaything.\\xe2\\x80\\x9d\\r\\n\\r\\nThe heat in the street was terrible: and the airlessness, the bustle\\r\\nand the plaster, scaffolding, bricks, and dust all about him, and that\\r\\nspecial Petersburg stench, so familiar to all who are unable to get out\\r\\nof town in summer--all worked painfully upon the young man\\xe2\\x80\\x99s already\\r\\noverwrought nerves. The insufferable stench from the pot-houses, which\\r\\nare particularly numerous in that part of the town, and the drunken men\\r\\nwhom he met continually, although it was a working day, completed\\r\\nthe revolting misery of the picture. An expression of the profoundest\\r\\ndisgust gleamed for a moment in the young man\\xe2\\x80\\x99s refined face. He was,\\r\\nby the way, exceptionally handsome, above the average in height, slim,\\r\\nwell-built, with beautiful dark eyes and dark brown hair. Soon he sank\\r\\ninto deep thought, or more accurately speaking into a complete blankness\\r\\nof mind; he walked along not observing what was about him and not caring\\r\\nto observe it. From time to time, he would mutter something, from the\\r\\nhabit of talking to himself, to which he had just confessed. At these\\r\\nmoments he would become conscious that his ideas were sometimes in a\\r\\ntangle and that he was very weak; for two days he had scarcely tasted\\r\\nfood.\\r\\n\\r\\\n",
      "716\n"
     ]
    }
   ],
   "source": [
    "print(rawStr.find(\"PART I\"))\n",
    "print(rawStr.rfind(\"End of Project Gutenberg's Crime\"))\n",
    "rawStr = rawStr[5303:10000]\n",
    "print(rawStr)\n",
    "print(rawStr.find(\"PART I\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ead515a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b'<!doctype html public \"-//W3C//DTD HTML 4.0 Transitional//EN'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url = \"http://news.bbc.co.uk/2/hi/health/2284783.stm\"\n",
    "html = urlopen(url).read()\n",
    "html[:60]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "76277a46",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f531cd33",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['BBC',\n",
       " 'NEWS',\n",
       " '|',\n",
       " 'Health',\n",
       " '|',\n",
       " 'Blondes',\n",
       " \"'to\",\n",
       " 'die',\n",
       " 'out',\n",
       " 'in',\n",
       " '200',\n",
       " \"years'\",\n",
       " 'NEWS',\n",
       " 'SPORT',\n",
       " 'WEATHER',\n",
       " 'WORLD',\n",
       " 'SERVICE',\n",
       " 'A-Z',\n",
       " 'INDEX',\n",
       " 'SEARCH',\n",
       " 'You',\n",
       " 'are',\n",
       " 'in',\n",
       " ':',\n",
       " 'Health',\n",
       " 'News',\n",
       " 'Front',\n",
       " 'Page',\n",
       " 'Africa',\n",
       " 'Americas',\n",
       " 'Asia-Pacific',\n",
       " 'Europe',\n",
       " 'Middle',\n",
       " 'East',\n",
       " 'South',\n",
       " 'Asia',\n",
       " 'UK',\n",
       " 'Business',\n",
       " 'Entertainment',\n",
       " 'Science/Nature',\n",
       " 'Technology',\n",
       " 'Health',\n",
       " 'Medical',\n",
       " 'notes',\n",
       " '--',\n",
       " '--',\n",
       " '--',\n",
       " '--',\n",
       " '--',\n",
       " '--',\n",
       " '-',\n",
       " 'Talking',\n",
       " 'Point',\n",
       " '--',\n",
       " '--',\n",
       " '--',\n",
       " '--',\n",
       " '--',\n",
       " '--',\n",
       " '-',\n",
       " 'Country',\n",
       " 'Profiles',\n",
       " 'In',\n",
       " 'Depth',\n",
       " '--',\n",
       " '--',\n",
       " '--',\n",
       " '--',\n",
       " '--',\n",
       " '--',\n",
       " '-',\n",
       " 'Programmes',\n",
       " '--',\n",
       " '--',\n",
       " '--',\n",
       " '--',\n",
       " '--',\n",
       " '--',\n",
       " '-',\n",
       " 'SERVICES',\n",
       " 'Daily',\n",
       " 'E-mail',\n",
       " 'News',\n",
       " 'Ticker',\n",
       " 'Mobile/PDAs',\n",
       " '--',\n",
       " '--',\n",
       " '--',\n",
       " '--',\n",
       " '--',\n",
       " '--',\n",
       " '-',\n",
       " 'Text',\n",
       " 'Only',\n",
       " 'Feedback',\n",
       " 'Help',\n",
       " 'EDITIONS',\n",
       " 'Change',\n",
       " 'to',\n",
       " 'UK',\n",
       " 'Friday',\n",
       " ',',\n",
       " '27',\n",
       " 'September',\n",
       " ',',\n",
       " '2002',\n",
       " ',',\n",
       " '11:51',\n",
       " 'GMT',\n",
       " '12:51',\n",
       " 'UK',\n",
       " 'Blondes',\n",
       " \"'to\",\n",
       " 'die',\n",
       " 'out',\n",
       " 'in',\n",
       " '200',\n",
       " \"years'\",\n",
       " 'Scientists',\n",
       " 'believe',\n",
       " 'the',\n",
       " 'last',\n",
       " 'blondes',\n",
       " 'will',\n",
       " 'be',\n",
       " 'in',\n",
       " 'Finland',\n",
       " 'The',\n",
       " 'last',\n",
       " 'natural',\n",
       " 'blondes',\n",
       " 'will',\n",
       " 'die',\n",
       " 'out',\n",
       " 'within',\n",
       " '200',\n",
       " 'years',\n",
       " ',',\n",
       " 'scientists',\n",
       " 'believe',\n",
       " '.',\n",
       " 'A',\n",
       " 'study',\n",
       " 'by',\n",
       " 'experts',\n",
       " 'in',\n",
       " 'Germany',\n",
       " 'suggests',\n",
       " 'people',\n",
       " 'with',\n",
       " 'blonde',\n",
       " 'hair',\n",
       " 'are',\n",
       " 'an',\n",
       " 'endangered',\n",
       " 'species',\n",
       " 'and',\n",
       " 'will',\n",
       " 'become',\n",
       " 'extinct',\n",
       " 'by',\n",
       " '2202',\n",
       " '.',\n",
       " 'Researchers',\n",
       " 'predict',\n",
       " 'the',\n",
       " 'last',\n",
       " 'truly',\n",
       " 'natural',\n",
       " 'blonde',\n",
       " 'will',\n",
       " 'be',\n",
       " 'born',\n",
       " 'in',\n",
       " 'Finland',\n",
       " '-',\n",
       " 'the',\n",
       " 'country',\n",
       " 'with',\n",
       " 'the',\n",
       " 'highest',\n",
       " 'proportion',\n",
       " 'of',\n",
       " 'blondes',\n",
       " '.',\n",
       " 'The',\n",
       " 'frequency',\n",
       " 'of',\n",
       " 'blondes',\n",
       " 'may',\n",
       " 'drop',\n",
       " 'but',\n",
       " 'they',\n",
       " 'wo',\n",
       " \"n't\",\n",
       " 'disappear',\n",
       " 'Prof',\n",
       " 'Jonathan',\n",
       " 'Rees',\n",
       " ',',\n",
       " 'University',\n",
       " 'of',\n",
       " 'Edinburgh',\n",
       " 'But',\n",
       " 'they',\n",
       " 'say',\n",
       " 'too',\n",
       " 'few',\n",
       " 'people',\n",
       " 'now',\n",
       " 'carry',\n",
       " 'the',\n",
       " 'gene',\n",
       " 'for',\n",
       " 'blondes',\n",
       " 'to',\n",
       " 'last',\n",
       " 'beyond',\n",
       " 'the',\n",
       " 'next',\n",
       " 'two',\n",
       " 'centuries',\n",
       " '.',\n",
       " 'The',\n",
       " 'problem',\n",
       " 'is',\n",
       " 'that',\n",
       " 'blonde',\n",
       " 'hair',\n",
       " 'is',\n",
       " 'caused',\n",
       " 'by',\n",
       " 'a',\n",
       " 'recessive',\n",
       " 'gene',\n",
       " '.',\n",
       " 'In',\n",
       " 'order',\n",
       " 'for',\n",
       " 'a',\n",
       " 'child',\n",
       " 'to',\n",
       " 'have',\n",
       " 'blonde',\n",
       " 'hair',\n",
       " ',',\n",
       " 'it',\n",
       " 'must',\n",
       " 'have',\n",
       " 'the',\n",
       " 'gene',\n",
       " 'on',\n",
       " 'both',\n",
       " 'sides',\n",
       " 'of',\n",
       " 'the',\n",
       " 'family',\n",
       " 'in',\n",
       " 'the',\n",
       " 'grandparents',\n",
       " \"'\",\n",
       " 'generation',\n",
       " '.',\n",
       " 'Dyed',\n",
       " 'rivals',\n",
       " 'The',\n",
       " 'researchers',\n",
       " 'also',\n",
       " 'believe',\n",
       " 'that',\n",
       " 'so-called',\n",
       " 'bottle',\n",
       " 'blondes',\n",
       " 'may',\n",
       " 'be',\n",
       " 'to',\n",
       " 'blame',\n",
       " 'for',\n",
       " 'the',\n",
       " 'demise',\n",
       " 'of',\n",
       " 'their',\n",
       " 'natural',\n",
       " 'rivals',\n",
       " '.',\n",
       " 'They',\n",
       " 'suggest',\n",
       " 'that',\n",
       " 'dyed-blondes',\n",
       " 'are',\n",
       " 'more',\n",
       " 'attractive',\n",
       " 'to',\n",
       " 'men',\n",
       " 'who',\n",
       " 'choose',\n",
       " 'them',\n",
       " 'as',\n",
       " 'partners',\n",
       " 'over',\n",
       " 'true',\n",
       " 'blondes',\n",
       " '.',\n",
       " 'Bottle-blondes',\n",
       " 'like',\n",
       " 'Ann',\n",
       " 'Widdecombe',\n",
       " 'may',\n",
       " 'be',\n",
       " 'to',\n",
       " 'blame',\n",
       " 'But',\n",
       " 'Jonathan',\n",
       " 'Rees',\n",
       " ',',\n",
       " 'professor',\n",
       " 'of',\n",
       " 'dermatology',\n",
       " 'at',\n",
       " 'the',\n",
       " 'University',\n",
       " 'of',\n",
       " 'Edinburgh',\n",
       " 'said',\n",
       " 'it',\n",
       " 'was',\n",
       " 'unlikely',\n",
       " 'blondes',\n",
       " 'would',\n",
       " 'die',\n",
       " 'out',\n",
       " 'completely',\n",
       " '.',\n",
       " '``',\n",
       " 'Genes',\n",
       " 'do',\n",
       " \"n't\",\n",
       " 'die',\n",
       " 'out',\n",
       " 'unless',\n",
       " 'there',\n",
       " 'is',\n",
       " 'a',\n",
       " 'disadvantage',\n",
       " 'of',\n",
       " 'having',\n",
       " 'that',\n",
       " 'gene',\n",
       " 'or',\n",
       " 'by',\n",
       " 'chance',\n",
       " '.',\n",
       " 'They',\n",
       " 'do',\n",
       " \"n't\",\n",
       " 'disappear',\n",
       " ',',\n",
       " \"''\",\n",
       " 'he',\n",
       " 'told',\n",
       " 'BBC',\n",
       " 'News',\n",
       " 'Online',\n",
       " '.',\n",
       " '``',\n",
       " 'The',\n",
       " 'only',\n",
       " 'reason',\n",
       " 'blondes',\n",
       " 'would',\n",
       " 'disappear',\n",
       " 'is',\n",
       " 'if',\n",
       " 'having',\n",
       " 'the',\n",
       " 'gene',\n",
       " 'was',\n",
       " 'a',\n",
       " 'disadvantage',\n",
       " 'and',\n",
       " 'I',\n",
       " 'do',\n",
       " 'not',\n",
       " 'think',\n",
       " 'that',\n",
       " 'is',\n",
       " 'the',\n",
       " 'case',\n",
       " '.',\n",
       " '``',\n",
       " 'The',\n",
       " 'frequency',\n",
       " 'of',\n",
       " 'blondes',\n",
       " 'may',\n",
       " 'drop',\n",
       " 'but',\n",
       " 'they',\n",
       " 'wo',\n",
       " \"n't\",\n",
       " 'disappear',\n",
       " '.',\n",
       " \"''\",\n",
       " 'See',\n",
       " 'also',\n",
       " ':',\n",
       " '28',\n",
       " 'Mar',\n",
       " '01',\n",
       " '|',\n",
       " 'Education',\n",
       " 'What',\n",
       " 'is',\n",
       " 'it',\n",
       " 'about',\n",
       " 'blondes',\n",
       " '?',\n",
       " '09',\n",
       " 'Apr',\n",
       " '99',\n",
       " '|',\n",
       " 'Health',\n",
       " 'Platinum',\n",
       " 'blondes',\n",
       " 'are',\n",
       " 'labelled',\n",
       " 'as',\n",
       " 'dumb',\n",
       " '17',\n",
       " 'Apr',\n",
       " '02',\n",
       " '|',\n",
       " 'Health',\n",
       " 'Hair',\n",
       " 'dye',\n",
       " 'cancer',\n",
       " 'alert',\n",
       " 'Internet',\n",
       " 'links',\n",
       " ':',\n",
       " 'University',\n",
       " 'of',\n",
       " 'Edinburgh',\n",
       " 'The',\n",
       " 'BBC',\n",
       " 'is',\n",
       " 'not',\n",
       " 'responsible',\n",
       " 'for',\n",
       " 'the',\n",
       " 'content',\n",
       " 'of',\n",
       " 'external',\n",
       " 'internet',\n",
       " 'sites',\n",
       " 'Top',\n",
       " 'Health',\n",
       " 'stories',\n",
       " 'now',\n",
       " ':',\n",
       " 'Heart',\n",
       " 'risk',\n",
       " 'link',\n",
       " 'to',\n",
       " 'big',\n",
       " 'families',\n",
       " 'Back',\n",
       " 'pain',\n",
       " 'drug',\n",
       " \"'may\",\n",
       " 'aid',\n",
       " \"diabetics'\",\n",
       " 'Congo',\n",
       " 'Ebola',\n",
       " 'outbreak',\n",
       " 'confirmed',\n",
       " 'Vegetables',\n",
       " 'ward',\n",
       " 'off',\n",
       " \"Alzheimer's\",\n",
       " 'Polio',\n",
       " 'campaign',\n",
       " 'launched',\n",
       " 'in',\n",
       " 'Iraq',\n",
       " 'Gene',\n",
       " 'defect',\n",
       " 'explains',\n",
       " 'high',\n",
       " 'blood',\n",
       " 'pressure',\n",
       " 'Botox',\n",
       " \"'may\",\n",
       " 'cause',\n",
       " 'new',\n",
       " \"wrinkles'\",\n",
       " 'Alien',\n",
       " \"'abductees\",\n",
       " \"'\",\n",
       " 'show',\n",
       " 'real',\n",
       " 'symptoms',\n",
       " 'Links',\n",
       " 'to',\n",
       " 'more',\n",
       " 'Health',\n",
       " 'stories',\n",
       " 'are',\n",
       " 'at',\n",
       " 'the',\n",
       " 'foot',\n",
       " 'of',\n",
       " 'the',\n",
       " 'page',\n",
       " '.',\n",
       " 'E-mail',\n",
       " 'this',\n",
       " 'story',\n",
       " 'to',\n",
       " 'a',\n",
       " 'friend',\n",
       " 'Links',\n",
       " 'to',\n",
       " 'more',\n",
       " 'Health',\n",
       " 'stories',\n",
       " 'In',\n",
       " 'This',\n",
       " 'Section',\n",
       " 'Heart',\n",
       " 'risk',\n",
       " 'link',\n",
       " 'to',\n",
       " 'big',\n",
       " 'families',\n",
       " 'Back',\n",
       " 'pain',\n",
       " 'drug',\n",
       " \"'may\",\n",
       " 'aid',\n",
       " \"diabetics'\",\n",
       " 'Congo',\n",
       " 'Ebola',\n",
       " 'outbreak',\n",
       " 'confirmed',\n",
       " 'Vegetables',\n",
       " 'ward',\n",
       " 'off',\n",
       " \"Alzheimer's\",\n",
       " 'Polio',\n",
       " 'campaign',\n",
       " 'launched',\n",
       " 'in',\n",
       " 'Iraq',\n",
       " 'Gene',\n",
       " 'defect',\n",
       " 'explains',\n",
       " 'high',\n",
       " 'blood',\n",
       " 'pressure',\n",
       " 'Botox',\n",
       " \"'may\",\n",
       " 'cause',\n",
       " 'new',\n",
       " \"wrinkles'\",\n",
       " 'Alien',\n",
       " \"'abductees\",\n",
       " \"'\",\n",
       " 'show',\n",
       " 'real',\n",
       " 'symptoms',\n",
       " 'How',\n",
       " 'sperm',\n",
       " 'wriggle',\n",
       " 'Bollywood',\n",
       " 'told',\n",
       " 'to',\n",
       " 'stub',\n",
       " 'it',\n",
       " 'out',\n",
       " 'Fears',\n",
       " 'over',\n",
       " 'tuna',\n",
       " 'health',\n",
       " 'risk',\n",
       " 'to',\n",
       " 'babies',\n",
       " 'Public',\n",
       " 'can',\n",
       " 'be',\n",
       " 'taught',\n",
       " 'to',\n",
       " 'spot',\n",
       " 'strokes',\n",
       " '^^',\n",
       " 'Back',\n",
       " 'to',\n",
       " 'top',\n",
       " 'News',\n",
       " 'Front',\n",
       " 'Page',\n",
       " '|',\n",
       " 'Africa',\n",
       " '|',\n",
       " 'Americas',\n",
       " '|',\n",
       " 'Asia-Pacific',\n",
       " '|',\n",
       " 'Europe',\n",
       " '|',\n",
       " 'Middle',\n",
       " 'East',\n",
       " '|',\n",
       " 'South',\n",
       " 'Asia',\n",
       " '|',\n",
       " 'UK',\n",
       " '|',\n",
       " 'Business',\n",
       " '|',\n",
       " 'Entertainment',\n",
       " '|',\n",
       " 'Science/Nature',\n",
       " '|',\n",
       " 'Technology',\n",
       " '|',\n",
       " 'Health',\n",
       " '|',\n",
       " 'Talking',\n",
       " 'Point',\n",
       " '|',\n",
       " 'Country',\n",
       " 'Profiles',\n",
       " '|',\n",
       " 'In',\n",
       " 'Depth',\n",
       " '|',\n",
       " 'Programmes',\n",
       " '--',\n",
       " '--',\n",
       " '--',\n",
       " '--',\n",
       " '--',\n",
       " '--',\n",
       " '--',\n",
       " '--',\n",
       " '--',\n",
       " '--',\n",
       " '--',\n",
       " '--',\n",
       " '--',\n",
       " '--',\n",
       " '--',\n",
       " '--',\n",
       " '--',\n",
       " '--',\n",
       " '--',\n",
       " '--',\n",
       " '--',\n",
       " '--',\n",
       " '--',\n",
       " '--',\n",
       " '--',\n",
       " '--',\n",
       " '--',\n",
       " '--',\n",
       " '--',\n",
       " '--',\n",
       " '--',\n",
       " '--',\n",
       " '--',\n",
       " '--',\n",
       " '--',\n",
       " '--',\n",
       " '--',\n",
       " '--',\n",
       " '--',\n",
       " '--',\n",
       " '--',\n",
       " 'To',\n",
       " 'BBC',\n",
       " 'Sport',\n",
       " '>',\n",
       " '>',\n",
       " '|',\n",
       " 'To',\n",
       " 'BBC',\n",
       " 'Weather',\n",
       " '>',\n",
       " '>',\n",
       " '|',\n",
       " 'To',\n",
       " 'BBC',\n",
       " 'World',\n",
       " 'Service',\n",
       " '>',\n",
       " '>',\n",
       " '--',\n",
       " '--',\n",
       " '--',\n",
       " '--',\n",
       " '--',\n",
       " '--',\n",
       " '--',\n",
       " '--',\n",
       " '--',\n",
       " '--',\n",
       " '--',\n",
       " '--',\n",
       " '--',\n",
       " '--',\n",
       " '--',\n",
       " '--',\n",
       " '--',\n",
       " '--',\n",
       " '--',\n",
       " '--',\n",
       " '--',\n",
       " '--',\n",
       " '--',\n",
       " '--',\n",
       " '--',\n",
       " '--',\n",
       " '--',\n",
       " '--',\n",
       " '--',\n",
       " '--',\n",
       " '--',\n",
       " '--',\n",
       " '--',\n",
       " '--',\n",
       " '--',\n",
       " '--',\n",
       " '--',\n",
       " '--',\n",
       " '--',\n",
       " '--',\n",
       " '--',\n",
       " '©',\n",
       " 'MMIII',\n",
       " '|',\n",
       " 'News',\n",
       " 'Sources',\n",
       " '|',\n",
       " 'Privacy']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup = BeautifulSoup(html, 'html.parser')\n",
    "raw = soup.get_text()\n",
    "tokens = nltk.word_tokenize(raw)\n",
    "tokens\n",
    "#NotImplementedError: To remove HTML markup, use BeautifulSoup's get_text() function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "12238ed5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Displaying 5 of 5 matches:\n",
      "hey say too few people now carry the gene for blondes to last beyond the next \n",
      "blonde hair is caused by a recessive gene . In order for a child to have blond\n",
      " have blonde hair , it must have the gene on both sides of the family in the g\n",
      "ere is a disadvantage of having that gene or by chance . They do n't disappear\n",
      "des would disappear is if having the gene was a disadvantage and I do not thin\n"
     ]
    }
   ],
   "source": [
    "tokens = tokens[96:399]\n",
    "text = nltk.Text(tokens)\n",
    "text.concordance('gene')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea41e42c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "#\n",
    "# Feedparser is no more\n",
    "#\n",
    "#\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b1249c46",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Time flies like an arrow.\\nFruit flies like a banana.'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f = open('document.txt')\n",
    "raw = f.read()\n",
    "raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "68e152f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time flies like an arrow.\n",
      "Fruit flies like a banana.\n"
     ]
    }
   ],
   "source": [
    "f = open('document.txt')\n",
    "for line in f:\n",
    "    print(line.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ee8457dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/vickieduong/nltk_data/corpora/gutenberg/melville-moby_dick.txt\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'[Moby Dick by Herman Melville 1851]\\n\\n\\nETYMOLOGY.\\n\\n(Supplied by a Late Consumptive Usher to a Grammar'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = nltk.data.find('corpora//gutenberg/melville-moby_dick.txt')\n",
    "print(path)\n",
    "raw = open(path).read()\n",
    "raw[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b5d67686",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You typed 7 words.\n",
      "['Enter', 'some', 'text', ':', 'Capture', 'user', 'input']\n"
     ]
    }
   ],
   "source": [
    "s = \"Enter some text: Capture user input \"\n",
    "print(\"You typed\", len(nltk.word_tokenize(s)), \"words.\")\n",
    "print(nltk.word_tokenize(s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "22e5738a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw = open('document.txt').read()\n",
    "type(raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9c4ff1ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = nltk.word_tokenize(raw)\n",
    "type(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0d258e12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = [w.lower() for w in tokens]\n",
    "type(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "153ca693",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab = sorted(set(words))\n",
    "type(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4399f53f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['.', 'a', 'an', 'arrow', 'banana', 'flies', 'fruit', 'like', 'time']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2e37a1cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab.append('blog')\n",
    "#raw.append('blog')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "67454176",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Who knows?', 'john', 'paul', 'george', 'ringo']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = 'Who knows?'\n",
    "beatles = ['john', 'paul', 'george', 'ringo']\n",
    "[query] + beatles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "2bbe944b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Monty Python'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "monty = 'Monty Python'\n",
    "monty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "2fcec39f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Monty Python's Flying Circus\""
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "circus = \"Monty Python's Flying Circus\"\n",
    "circus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "1215aaac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Monty Python's Flying Circus\""
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "circus = 'Monty Python\\'s Flying Circus'\n",
    "circus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "681679df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Monty Pythons Flying Circus'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "circus = 'Monty Python''s Flying Circus'\n",
    "circus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "9a64a0e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shall I compare thee to a Summer's day?Thou are more lovely and more temperate:\n"
     ]
    }
   ],
   "source": [
    "couplet = \"Shall I compare thee to a Summer's day?\"\\\n",
    "          \"Thou are more lovely and more temperate:\"\n",
    "print(couplet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "f390c069",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rough winds do shake the darling buds of May,And Summer's lease hath all too short a date:\n"
     ]
    }
   ],
   "source": [
    "couplet = (\"Rough winds do shake the darling buds of May,\"\n",
    "           \"And Summer's lease hath all too short a date:\") \n",
    "print(couplet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "f7eb716b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shall I compare thee to a Summer's day?Thou are more lovely and more temperate:\n"
     ]
    }
   ],
   "source": [
    "couplet = \"\"\"Shall I compare thee to a Summer's day?\n",
    "Thou are more lovely and more temperate:\"\"\"\n",
    "\n",
    "print(couplet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "ed1a1638",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rough winds do shake the darling buds of May,\n",
      "And Summer's lease hath all too short a date:\n"
     ]
    }
   ],
   "source": [
    "couplet = '''Rough winds do shake the darling buds of May,\n",
    "And Summer's lease hath all too short a date:'''\n",
    "print(couplet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "c46003e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'veryveryvery'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'very' + 'very' + 'very'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "8e900a77",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'veryveryvery'"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'very' * 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "90406f01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            very\n",
      "          veryvery\n",
      "        veryveryvery\n",
      "      veryveryveryvery\n",
      "    veryveryveryveryvery\n",
      "  veryveryveryveryveryvery\n",
      "veryveryveryveryveryveryvery\n",
      "  veryveryveryveryveryvery\n",
      "    veryveryveryveryvery\n",
      "      veryveryveryvery\n",
      "        veryveryvery\n",
      "          veryvery\n",
      "            very\n"
     ]
    }
   ],
   "source": [
    "a = [1, 2, 3, 4, 5, 6, 7, 6, 5, 4, 3, 2, 1]\n",
    "b = [' ' * 2 * (7 - i) + 'very' * i for i in a]\n",
    "for line in b:\n",
    "    print(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "14e98a12",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for -: 'str' and 'str'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[51], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvery\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m-\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124my\u001b[39m\u001b[38;5;124m'\u001b[39m\n",
      "\u001b[0;31mTypeError\u001b[0m: unsupported operand type(s) for -: 'str' and 'str'"
     ]
    }
   ],
   "source": [
    "'very' - 'y'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "7fabfbcd",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for /: 'str' and 'int'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[52], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvery\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m2\u001b[39m\n",
      "\u001b[0;31mTypeError\u001b[0m: unsupported operand type(s) for /: 'str' and 'int'"
     ]
    }
   ],
   "source": [
    "'very' / 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "84b6df90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Monty Python\n"
     ]
    }
   ],
   "source": [
    "print(monty)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "703f3001",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Monty PythonHoly Grail\n",
      "Monty Python Holy Grail\n",
      "Monty Python and the Holy Grail\n"
     ]
    }
   ],
   "source": [
    "grail = 'Holy Grail'\n",
    "print(monty + grail)\n",
    "print(monty, grail)\n",
    "print(monty, \"and the\", grail)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "49fe7694",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "M\n",
      "t\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "string index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[55], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28mprint\u001b[39m(monty[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(monty[\u001b[38;5;241m3\u001b[39m])\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28mprint\u001b[39m(monty[\u001b[38;5;241m20\u001b[39m])\n",
      "\u001b[0;31mIndexError\u001b[0m: string index out of range"
     ]
    }
   ],
   "source": [
    "print(monty[0])\n",
    "print(monty[3])\n",
    "print(monty[20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "10f03674",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "o\n"
     ]
    }
   ],
   "source": [
    "print(monty[-2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "92f73392",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c\n",
      "o\n",
      "l\n",
      "o\n",
      "r\n",
      "l\n",
      "e\n",
      "s\n",
      "s\n",
      " \n",
      "g\n",
      "r\n",
      "e\n",
      "e\n",
      "n\n",
      " \n",
      "i\n",
      "d\n",
      "e\n",
      "a\n",
      "s\n",
      " \n",
      "s\n",
      "l\n",
      "e\n",
      "e\n",
      "p\n",
      " \n",
      "f\n",
      "u\n",
      "r\n",
      "i\n",
      "o\n",
      "u\n",
      "s\n",
      "l\n",
      "y\n"
     ]
    }
   ],
   "source": [
    "sent = 'colorless green ideas sleep furiously'\n",
    "for char in sent:\n",
    "    print(char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "19676ff6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['m', 'o', 'b', 'y', 'd', 'i', 'c', 'k', 'h', 'e', 'r', 'a', 'n', 'l', 'v', 't', 'g', 's', 'u', 'p', 'w', 'x', 'q', 'f', 'j', 'z'])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import gutenberg\n",
    "raw = gutenberg.raw('melville-moby_dick.txt')\n",
    "fdist = nltk.FreqDist(ch.lower() for ch in raw if ch.isalpha())\n",
    "fdist.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "a7ccea32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Pyth'"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "monty[6:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "ca7a24c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Monty'"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "monty[-12:-7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "8eb7b449",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Monty\n",
      "Python\n"
     ]
    }
   ],
   "source": [
    "print(monty[:5])\n",
    "print(monty[6:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "7098b2c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found \"thing\"\n"
     ]
    }
   ],
   "source": [
    "phrase = 'And now for something completely different'\n",
    "if 'thing' in phrase:\n",
    "    print('found \"thing\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "aef2fd28",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "monty.find('Python')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85f14004",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = 'Who knows?'\n",
    "beatles = ['John', 'Paul', 'George', 'Ringo']\n",
    "query[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "5c5f7928",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['John Lennon', 'paul', 'george']"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "beatles[0] = \"John Lennon\"\n",
    "del beatles[-1]\n",
    "beatles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "ee87881b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'Pruska Biblioteka Pa\\\\u0144stwowa. Jej dawne zbiory znane pod nazw\\\\u0105'\n",
      "b'\"Berlinka\" to skarb kultury i sztuki niemieckiej. Przewiezione przez'\n",
      "b'Niemc\\\\xf3w pod koniec II wojny \\\\u015bwiatowej na Dolny \\\\u015al\\\\u0105sk, zosta\\\\u0142y'\n",
      "b'odnalezione po 1945 r. na terytorium Polski. Trafi\\\\u0142y do Biblioteki'\n",
      "b'Jagiello\\\\u0144skiej w Krakowie, obejmuj\\\\u0105 ponad 500 tys. zabytkowych'\n",
      "b'archiwali\\\\xf3w, m.in. manuskrypty Goethego, Mozarta, Beethovena, Bacha.'\n"
     ]
    }
   ],
   "source": [
    "path = nltk.data.find('corpora/unicode_samples/polish-lat2.txt')\n",
    "import codecs\n",
    "f = codecs.open(path, encoding='latin2')\n",
    "\n",
    "for line in f:\n",
    "    line = line.strip()\n",
    "    print(line.encode('unicode_escape'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "4357ebec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "97"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ord('a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "a4c381b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'a'"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = u'\\u0061'\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "86c5cb1e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ń'"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nacute = u'\\u0144'\n",
    "nacute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "806f53bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'\\xc5\\x84'\n"
     ]
    }
   ],
   "source": [
    "nacute_utf = nacute.encode('utf8')\n",
    "print(repr(nacute_utf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "0263ec55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'Niemc\\\\xf3w pod koniec II wojny \\\\u015bwiatowej na Dolny \\\\u015al\\\\u0105sk, zosta\\\\u0142y\\\\n'\n"
     ]
    }
   ],
   "source": [
    "import unicodedata\n",
    "lines = codecs.open(path, encoding='latin2').readlines()\n",
    "line = lines[2]\n",
    "print(line.encode('unicode_escape'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5ae2c5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for c in line:\n",
    "    if ord(c) > 127:\n",
    "        print('%r U+%04x %s' % (c.encode('utf8'), ord(c), unicodedata.name(c)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "3b4155ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "54"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "line.find(u'zosta\\u0142y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "511ae643",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'niemc\\\\xf3w pod koniec ii wojny \\\\u015bwiatowej na dolny \\\\u015bl\\\\u0105sk, zosta\\\\u0142y\\\\n'\n"
     ]
    }
   ],
   "source": [
    "line = line.lower()\n",
    "print(line.encode('unicode_escape'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "6b8812da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'światowej'"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = re.search(u'\\u015b\\w*', line)\n",
    "m.group()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "2210621a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['niemców',\n",
       " 'pod',\n",
       " 'koniec',\n",
       " 'ii',\n",
       " 'wojny',\n",
       " 'światowej',\n",
       " 'na',\n",
       " 'dolny',\n",
       " 'śląsk',\n",
       " ',',\n",
       " 'zostały']"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.word_tokenize(line) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "5d27c45b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a',\n",
       " 'aa',\n",
       " 'aal',\n",
       " 'aalii',\n",
       " 'aam',\n",
       " 'aardvark',\n",
       " 'aardwolf',\n",
       " 'aba',\n",
       " 'abac',\n",
       " 'abaca',\n",
       " 'abacate',\n",
       " 'abacay',\n",
       " 'abacinate',\n",
       " 'abacination',\n",
       " 'abaciscus',\n",
       " 'abacist',\n",
       " 'aback',\n",
       " 'abactinal',\n",
       " 'abactinally',\n",
       " 'abaction',\n",
       " 'abactor',\n",
       " 'abaculus',\n",
       " 'abacus',\n",
       " 'abaff',\n",
       " 'abaft',\n",
       " 'abaisance',\n",
       " 'abaiser',\n",
       " 'abaissed',\n",
       " 'abalienate',\n",
       " 'abalienation',\n",
       " 'abalone',\n",
       " 'abampere',\n",
       " 'abandon',\n",
       " 'abandonable',\n",
       " 'abandoned',\n",
       " 'abandonedly',\n",
       " 'abandonee',\n",
       " 'abandoner',\n",
       " 'abandonment',\n",
       " 'abaptiston',\n",
       " 'abarthrosis',\n",
       " 'abarticular',\n",
       " 'abarticulation',\n",
       " 'abas',\n",
       " 'abase',\n",
       " 'abased',\n",
       " 'abasedly',\n",
       " 'abasedness',\n",
       " 'abasement',\n",
       " 'abaser',\n",
       " 'abash',\n",
       " 'abashed',\n",
       " 'abashedly',\n",
       " 'abashedness',\n",
       " 'abashless',\n",
       " 'abashlessly',\n",
       " 'abashment',\n",
       " 'abasia',\n",
       " 'abasic',\n",
       " 'abask',\n",
       " 'abastardize',\n",
       " 'abatable',\n",
       " 'abate',\n",
       " 'abatement',\n",
       " 'abater',\n",
       " 'abatis',\n",
       " 'abatised',\n",
       " 'abaton',\n",
       " 'abator',\n",
       " 'abattoir',\n",
       " 'abature',\n",
       " 'abave',\n",
       " 'abaxial',\n",
       " 'abaxile',\n",
       " 'abaze',\n",
       " 'abb',\n",
       " 'abbacomes',\n",
       " 'abbacy',\n",
       " 'abbas',\n",
       " 'abbasi',\n",
       " 'abbassi',\n",
       " 'abbatial',\n",
       " 'abbatical',\n",
       " 'abbess',\n",
       " 'abbey',\n",
       " 'abbeystede',\n",
       " 'abbot',\n",
       " 'abbotcy',\n",
       " 'abbotnullius',\n",
       " 'abbotship',\n",
       " 'abbreviate',\n",
       " 'abbreviately',\n",
       " 'abbreviation',\n",
       " 'abbreviator',\n",
       " 'abbreviatory',\n",
       " 'abbreviature',\n",
       " 'abcoulomb',\n",
       " 'abdal',\n",
       " 'abdat',\n",
       " 'abdest',\n",
       " 'abdicable',\n",
       " 'abdicant',\n",
       " 'abdicate',\n",
       " 'abdication',\n",
       " 'abdicative',\n",
       " 'abdicator',\n",
       " 'abditive',\n",
       " 'abditory',\n",
       " 'abdomen',\n",
       " 'abdominal',\n",
       " 'abdominalian',\n",
       " 'abdominally',\n",
       " 'abdominoanterior',\n",
       " 'abdominocardiac',\n",
       " 'abdominocentesis',\n",
       " 'abdominocystic',\n",
       " 'abdominogenital',\n",
       " 'abdominohysterectomy',\n",
       " 'abdominohysterotomy',\n",
       " 'abdominoposterior',\n",
       " 'abdominoscope',\n",
       " 'abdominoscopy',\n",
       " 'abdominothoracic',\n",
       " 'abdominous',\n",
       " 'abdominovaginal',\n",
       " 'abdominovesical',\n",
       " 'abduce',\n",
       " 'abducens',\n",
       " 'abducent',\n",
       " 'abduct',\n",
       " 'abduction',\n",
       " 'abductor',\n",
       " 'abeam',\n",
       " 'abear',\n",
       " 'abearance',\n",
       " 'abecedarian',\n",
       " 'abecedarium',\n",
       " 'abecedary',\n",
       " 'abed',\n",
       " 'abeigh',\n",
       " 'abele',\n",
       " 'abelite',\n",
       " 'abelmosk',\n",
       " 'abeltree',\n",
       " 'abenteric',\n",
       " 'abepithymia',\n",
       " 'aberdevine',\n",
       " 'aberrance',\n",
       " 'aberrancy',\n",
       " 'aberrant',\n",
       " 'aberrate',\n",
       " 'aberration',\n",
       " 'aberrational',\n",
       " 'aberrator',\n",
       " 'aberrometer',\n",
       " 'aberroscope',\n",
       " 'aberuncator',\n",
       " 'abet',\n",
       " 'abetment',\n",
       " 'abettal',\n",
       " 'abettor',\n",
       " 'abevacuation',\n",
       " 'abey',\n",
       " 'abeyance',\n",
       " 'abeyancy',\n",
       " 'abeyant',\n",
       " 'abfarad',\n",
       " 'abhenry',\n",
       " 'abhiseka',\n",
       " 'abhominable',\n",
       " 'abhor',\n",
       " 'abhorrence',\n",
       " 'abhorrency',\n",
       " 'abhorrent',\n",
       " 'abhorrently',\n",
       " 'abhorrer',\n",
       " 'abhorrible',\n",
       " 'abhorring',\n",
       " 'abidal',\n",
       " 'abidance',\n",
       " 'abide',\n",
       " 'abider',\n",
       " 'abidi',\n",
       " 'abiding',\n",
       " 'abidingly',\n",
       " 'abidingness',\n",
       " 'abietate',\n",
       " 'abietene',\n",
       " 'abietic',\n",
       " 'abietin',\n",
       " 'abietineous',\n",
       " 'abietinic',\n",
       " 'abigail',\n",
       " 'abigailship',\n",
       " 'abigeat',\n",
       " 'abigeus',\n",
       " 'abilao',\n",
       " 'ability',\n",
       " 'abilla',\n",
       " 'abilo',\n",
       " 'abintestate',\n",
       " 'abiogenesis',\n",
       " 'abiogenesist',\n",
       " 'abiogenetic',\n",
       " 'abiogenetical',\n",
       " 'abiogenetically',\n",
       " 'abiogenist',\n",
       " 'abiogenous',\n",
       " 'abiogeny',\n",
       " 'abiological',\n",
       " 'abiologically',\n",
       " 'abiology',\n",
       " 'abiosis',\n",
       " 'abiotic',\n",
       " 'abiotrophic',\n",
       " 'abiotrophy',\n",
       " 'abir',\n",
       " 'abirritant',\n",
       " 'abirritate',\n",
       " 'abirritation',\n",
       " 'abirritative',\n",
       " 'abiston',\n",
       " 'abiuret',\n",
       " 'abject',\n",
       " 'abjectedness',\n",
       " 'abjection',\n",
       " 'abjective',\n",
       " 'abjectly',\n",
       " 'abjectness',\n",
       " 'abjoint',\n",
       " 'abjudge',\n",
       " 'abjudicate',\n",
       " 'abjudication',\n",
       " 'abjunction',\n",
       " 'abjunctive',\n",
       " 'abjuration',\n",
       " 'abjuratory',\n",
       " 'abjure',\n",
       " 'abjurement',\n",
       " 'abjurer',\n",
       " 'abkar',\n",
       " 'abkari',\n",
       " 'ablach',\n",
       " 'ablactate',\n",
       " 'ablactation',\n",
       " 'ablare',\n",
       " 'ablastemic',\n",
       " 'ablastous',\n",
       " 'ablate',\n",
       " 'ablation',\n",
       " 'ablatitious',\n",
       " 'ablatival',\n",
       " 'ablative',\n",
       " 'ablator',\n",
       " 'ablaut',\n",
       " 'ablaze',\n",
       " 'able',\n",
       " 'ableeze',\n",
       " 'ablegate',\n",
       " 'ableness',\n",
       " 'ablepharia',\n",
       " 'ablepharon',\n",
       " 'ablepharous',\n",
       " 'ablepsia',\n",
       " 'ableptical',\n",
       " 'ableptically',\n",
       " 'abler',\n",
       " 'ablest',\n",
       " 'ablewhackets',\n",
       " 'ablins',\n",
       " 'abloom',\n",
       " 'ablow',\n",
       " 'ablude',\n",
       " 'abluent',\n",
       " 'ablush',\n",
       " 'ablution',\n",
       " 'ablutionary',\n",
       " 'abluvion',\n",
       " 'ably',\n",
       " 'abmho',\n",
       " 'abnegate',\n",
       " 'abnegation',\n",
       " 'abnegative',\n",
       " 'abnegator',\n",
       " 'abnerval',\n",
       " 'abnet',\n",
       " 'abneural',\n",
       " 'abnormal',\n",
       " 'abnormalism',\n",
       " 'abnormalist',\n",
       " 'abnormality',\n",
       " 'abnormalize',\n",
       " 'abnormally',\n",
       " 'abnormalness',\n",
       " 'abnormity',\n",
       " 'abnormous',\n",
       " 'abnumerable',\n",
       " 'aboard',\n",
       " 'abode',\n",
       " 'abodement',\n",
       " 'abody',\n",
       " 'abohm',\n",
       " 'aboil',\n",
       " 'abolish',\n",
       " 'abolisher',\n",
       " 'abolishment',\n",
       " 'abolition',\n",
       " 'abolitionary',\n",
       " 'abolitionism',\n",
       " 'abolitionist',\n",
       " 'abolitionize',\n",
       " 'abolla',\n",
       " 'aboma',\n",
       " 'abomasum',\n",
       " 'abomasus',\n",
       " 'abominable',\n",
       " 'abominableness',\n",
       " 'abominably',\n",
       " 'abominate',\n",
       " 'abomination',\n",
       " 'abominator',\n",
       " 'abomine',\n",
       " 'aboon',\n",
       " 'aborad',\n",
       " 'aboral',\n",
       " 'aborally',\n",
       " 'abord',\n",
       " 'aboriginal',\n",
       " 'aboriginality',\n",
       " 'aboriginally',\n",
       " 'aboriginary',\n",
       " 'aborigine',\n",
       " 'abort',\n",
       " 'aborted',\n",
       " 'aborticide',\n",
       " 'abortient',\n",
       " 'abortifacient',\n",
       " 'abortin',\n",
       " 'abortion',\n",
       " 'abortional',\n",
       " 'abortionist',\n",
       " 'abortive',\n",
       " 'abortively',\n",
       " 'abortiveness',\n",
       " 'abortus',\n",
       " 'abouchement',\n",
       " 'abound',\n",
       " 'abounder',\n",
       " 'abounding',\n",
       " 'aboundingly',\n",
       " 'about',\n",
       " 'abouts',\n",
       " 'above',\n",
       " 'aboveboard',\n",
       " 'abovedeck',\n",
       " 'aboveground',\n",
       " 'aboveproof',\n",
       " 'abovestairs',\n",
       " 'abox',\n",
       " 'abracadabra',\n",
       " 'abrachia',\n",
       " 'abradant',\n",
       " 'abrade',\n",
       " 'abrader',\n",
       " 'abraid',\n",
       " 'abranchial',\n",
       " 'abranchialism',\n",
       " 'abranchian',\n",
       " 'abranchiate',\n",
       " 'abranchious',\n",
       " 'abrasax',\n",
       " 'abrase',\n",
       " 'abrash',\n",
       " 'abrasiometer',\n",
       " 'abrasion',\n",
       " 'abrasive',\n",
       " 'abrastol',\n",
       " 'abraum',\n",
       " 'abraxas',\n",
       " 'abreact',\n",
       " 'abreaction',\n",
       " 'abreast',\n",
       " 'abrenounce',\n",
       " 'abret',\n",
       " 'abrico',\n",
       " 'abridge',\n",
       " 'abridgeable',\n",
       " 'abridged',\n",
       " 'abridgedly',\n",
       " 'abridger',\n",
       " 'abridgment',\n",
       " 'abrim',\n",
       " 'abrin',\n",
       " 'abristle',\n",
       " 'abroach',\n",
       " 'abroad',\n",
       " 'abrocome',\n",
       " 'abrogable',\n",
       " 'abrogate',\n",
       " 'abrogation',\n",
       " 'abrogative',\n",
       " 'abrogator',\n",
       " 'abrook',\n",
       " 'abrotanum',\n",
       " 'abrotine',\n",
       " 'abrupt',\n",
       " 'abruptedly',\n",
       " 'abruption',\n",
       " 'abruptly',\n",
       " 'abruptness',\n",
       " 'absampere',\n",
       " 'absarokite',\n",
       " 'abscess',\n",
       " 'abscessed',\n",
       " 'abscession',\n",
       " 'abscessroot',\n",
       " 'abscind',\n",
       " 'abscise',\n",
       " 'abscision',\n",
       " 'absciss',\n",
       " 'abscissa',\n",
       " 'abscissae',\n",
       " 'abscisse',\n",
       " 'abscission',\n",
       " 'absconce',\n",
       " 'abscond',\n",
       " 'absconded',\n",
       " 'abscondedly',\n",
       " 'abscondence',\n",
       " 'absconder',\n",
       " 'absconsa',\n",
       " 'abscoulomb',\n",
       " 'absence',\n",
       " 'absent',\n",
       " 'absentation',\n",
       " 'absentee',\n",
       " 'absenteeism',\n",
       " 'absenteeship',\n",
       " 'absenter',\n",
       " 'absently',\n",
       " 'absentment',\n",
       " 'absentmindedly',\n",
       " 'absentness',\n",
       " 'absfarad',\n",
       " 'abshenry',\n",
       " 'absinthe',\n",
       " 'absinthial',\n",
       " 'absinthian',\n",
       " 'absinthiate',\n",
       " 'absinthic',\n",
       " 'absinthin',\n",
       " 'absinthine',\n",
       " 'absinthism',\n",
       " 'absinthismic',\n",
       " 'absinthium',\n",
       " 'absinthol',\n",
       " 'absit',\n",
       " 'absmho',\n",
       " 'absohm',\n",
       " 'absolute',\n",
       " 'absolutely',\n",
       " 'absoluteness',\n",
       " 'absolution',\n",
       " 'absolutism',\n",
       " 'absolutist',\n",
       " 'absolutistic',\n",
       " 'absolutistically',\n",
       " 'absolutive',\n",
       " 'absolutization',\n",
       " 'absolutize',\n",
       " 'absolutory',\n",
       " 'absolvable',\n",
       " 'absolvatory',\n",
       " 'absolve',\n",
       " 'absolvent',\n",
       " 'absolver',\n",
       " 'absolvitor',\n",
       " 'absolvitory',\n",
       " 'absonant',\n",
       " 'absonous',\n",
       " 'absorb',\n",
       " 'absorbability',\n",
       " 'absorbable',\n",
       " 'absorbed',\n",
       " 'absorbedly',\n",
       " 'absorbedness',\n",
       " 'absorbefacient',\n",
       " 'absorbency',\n",
       " 'absorbent',\n",
       " 'absorber',\n",
       " 'absorbing',\n",
       " 'absorbingly',\n",
       " 'absorbition',\n",
       " 'absorpt',\n",
       " 'absorptance',\n",
       " 'absorptiometer',\n",
       " 'absorptiometric',\n",
       " 'absorption',\n",
       " 'absorptive',\n",
       " 'absorptively',\n",
       " 'absorptiveness',\n",
       " 'absorptivity',\n",
       " 'absquatulate',\n",
       " 'abstain',\n",
       " 'abstainer',\n",
       " 'abstainment',\n",
       " 'abstemious',\n",
       " 'abstemiously',\n",
       " 'abstemiousness',\n",
       " 'abstention',\n",
       " 'abstentionist',\n",
       " 'abstentious',\n",
       " 'absterge',\n",
       " 'abstergent',\n",
       " 'abstersion',\n",
       " 'abstersive',\n",
       " 'abstersiveness',\n",
       " 'abstinence',\n",
       " 'abstinency',\n",
       " 'abstinent',\n",
       " 'abstinential',\n",
       " 'abstinently',\n",
       " 'abstract',\n",
       " 'abstracted',\n",
       " 'abstractedly',\n",
       " 'abstractedness',\n",
       " 'abstracter',\n",
       " 'abstraction',\n",
       " 'abstractional',\n",
       " 'abstractionism',\n",
       " 'abstractionist',\n",
       " 'abstractitious',\n",
       " 'abstractive',\n",
       " 'abstractively',\n",
       " 'abstractiveness',\n",
       " 'abstractly',\n",
       " 'abstractness',\n",
       " 'abstractor',\n",
       " 'abstrahent',\n",
       " 'abstricted',\n",
       " 'abstriction',\n",
       " 'abstruse',\n",
       " 'abstrusely',\n",
       " 'abstruseness',\n",
       " 'abstrusion',\n",
       " 'abstrusity',\n",
       " 'absume',\n",
       " 'absumption',\n",
       " 'absurd',\n",
       " 'absurdity',\n",
       " 'absurdly',\n",
       " 'absurdness',\n",
       " 'absvolt',\n",
       " 'abterminal',\n",
       " 'abthain',\n",
       " 'abthainrie',\n",
       " 'abthainry',\n",
       " 'abthanage',\n",
       " 'abu',\n",
       " 'abucco',\n",
       " 'abulia',\n",
       " 'abulic',\n",
       " 'abulomania',\n",
       " 'abuna',\n",
       " 'abundance',\n",
       " 'abundancy',\n",
       " 'abundant',\n",
       " 'abundantly',\n",
       " 'abura',\n",
       " 'aburabozu',\n",
       " 'aburban',\n",
       " 'aburst',\n",
       " 'aburton',\n",
       " 'abusable',\n",
       " 'abuse',\n",
       " 'abusedly',\n",
       " 'abusee',\n",
       " 'abuseful',\n",
       " 'abusefully',\n",
       " 'abusefulness',\n",
       " 'abuser',\n",
       " 'abusion',\n",
       " 'abusious',\n",
       " 'abusive',\n",
       " 'abusively',\n",
       " 'abusiveness',\n",
       " 'abut',\n",
       " 'abutment',\n",
       " 'abuttal',\n",
       " 'abutter',\n",
       " 'abutting',\n",
       " 'abuzz',\n",
       " 'abvolt',\n",
       " 'abwab',\n",
       " 'aby',\n",
       " 'abysm',\n",
       " 'abysmal',\n",
       " 'abysmally',\n",
       " 'abyss',\n",
       " 'abyssal',\n",
       " 'abyssobenthonic',\n",
       " 'abyssolith',\n",
       " 'abyssopelagic',\n",
       " 'acacatechin',\n",
       " 'acacatechol',\n",
       " 'acacetin',\n",
       " 'acaciin',\n",
       " 'acacin',\n",
       " 'academe',\n",
       " 'academial',\n",
       " 'academian',\n",
       " 'academic',\n",
       " 'academical',\n",
       " 'academically',\n",
       " 'academicals',\n",
       " 'academician',\n",
       " 'academicism',\n",
       " 'academism',\n",
       " 'academist',\n",
       " 'academite',\n",
       " 'academization',\n",
       " 'academize',\n",
       " 'academy',\n",
       " 'acadialite',\n",
       " 'acajou',\n",
       " 'acaleph',\n",
       " 'acalephan',\n",
       " 'acalephoid',\n",
       " 'acalycal',\n",
       " 'acalycine',\n",
       " 'acalycinous',\n",
       " 'acalyculate',\n",
       " 'acalyptrate',\n",
       " 'acampsia',\n",
       " 'acana',\n",
       " 'acanaceous',\n",
       " 'acanonical',\n",
       " 'acanth',\n",
       " 'acantha',\n",
       " 'acanthaceous',\n",
       " 'acanthad',\n",
       " 'acanthial',\n",
       " 'acanthin',\n",
       " 'acanthine',\n",
       " 'acanthion',\n",
       " 'acanthite',\n",
       " 'acanthocarpous',\n",
       " 'acanthocephalan',\n",
       " 'acanthocephalous',\n",
       " 'acanthocladous',\n",
       " 'acanthodean',\n",
       " 'acanthodian',\n",
       " 'acanthoid',\n",
       " 'acanthological',\n",
       " 'acanthology',\n",
       " 'acantholysis',\n",
       " 'acanthoma',\n",
       " 'acanthon',\n",
       " 'acanthophorous',\n",
       " 'acanthopod',\n",
       " 'acanthopodous',\n",
       " 'acanthopomatous',\n",
       " 'acanthopore',\n",
       " 'acanthopteran',\n",
       " 'acanthopterous',\n",
       " 'acanthopterygian',\n",
       " 'acanthosis',\n",
       " 'acanthous',\n",
       " 'acanthus',\n",
       " 'acapnia',\n",
       " 'acapnial',\n",
       " 'acapsular',\n",
       " 'acapu',\n",
       " 'acapulco',\n",
       " 'acara',\n",
       " 'acardia',\n",
       " 'acardiac',\n",
       " 'acari',\n",
       " 'acarian',\n",
       " 'acariasis',\n",
       " 'acaricidal',\n",
       " 'acaricide',\n",
       " 'acarid',\n",
       " 'acaridean',\n",
       " 'acaridomatium',\n",
       " 'acariform',\n",
       " 'acarine',\n",
       " 'acarinosis',\n",
       " 'acarocecidium',\n",
       " 'acarodermatitis',\n",
       " 'acaroid',\n",
       " 'acarol',\n",
       " 'acarologist',\n",
       " 'acarology',\n",
       " 'acarophilous',\n",
       " 'acarophobia',\n",
       " 'acarotoxic',\n",
       " 'acarpelous',\n",
       " 'acarpous',\n",
       " 'acatalectic',\n",
       " 'acatalepsia',\n",
       " 'acatalepsy',\n",
       " 'acataleptic',\n",
       " 'acatallactic',\n",
       " 'acatamathesia',\n",
       " 'acataphasia',\n",
       " 'acataposis',\n",
       " 'acatastasia',\n",
       " 'acatastatic',\n",
       " 'acate',\n",
       " 'acategorical',\n",
       " 'acatery',\n",
       " 'acatharsia',\n",
       " 'acatharsy',\n",
       " 'acatholic',\n",
       " 'acaudal',\n",
       " 'acaudate',\n",
       " 'acaulescent',\n",
       " 'acauline',\n",
       " 'acaulose',\n",
       " 'acaulous',\n",
       " 'acca',\n",
       " 'accede',\n",
       " 'accedence',\n",
       " 'acceder',\n",
       " 'accelerable',\n",
       " 'accelerando',\n",
       " 'accelerant',\n",
       " 'accelerate',\n",
       " 'accelerated',\n",
       " 'acceleratedly',\n",
       " 'acceleration',\n",
       " 'accelerative',\n",
       " 'accelerator',\n",
       " 'acceleratory',\n",
       " 'accelerograph',\n",
       " 'accelerometer',\n",
       " 'accend',\n",
       " 'accendibility',\n",
       " 'accendible',\n",
       " 'accension',\n",
       " 'accensor',\n",
       " 'accent',\n",
       " 'accentless',\n",
       " 'accentor',\n",
       " 'accentuable',\n",
       " 'accentual',\n",
       " 'accentuality',\n",
       " 'accentually',\n",
       " 'accentuate',\n",
       " 'accentuation',\n",
       " 'accentuator',\n",
       " 'accentus',\n",
       " 'accept',\n",
       " 'acceptability',\n",
       " 'acceptable',\n",
       " 'acceptableness',\n",
       " 'acceptably',\n",
       " 'acceptance',\n",
       " 'acceptancy',\n",
       " 'acceptant',\n",
       " 'acceptation',\n",
       " 'accepted',\n",
       " 'acceptedly',\n",
       " 'accepter',\n",
       " 'acceptilate',\n",
       " 'acceptilation',\n",
       " 'acception',\n",
       " 'acceptive',\n",
       " 'acceptor',\n",
       " 'acceptress',\n",
       " 'accerse',\n",
       " 'accersition',\n",
       " 'accersitor',\n",
       " 'access',\n",
       " 'accessarily',\n",
       " 'accessariness',\n",
       " 'accessary',\n",
       " 'accessaryship',\n",
       " 'accessibility',\n",
       " 'accessible',\n",
       " 'accessibly',\n",
       " 'accession',\n",
       " 'accessional',\n",
       " 'accessioner',\n",
       " 'accessive',\n",
       " 'accessively',\n",
       " 'accessless',\n",
       " 'accessorial',\n",
       " 'accessorily',\n",
       " 'accessoriness',\n",
       " 'accessorius',\n",
       " 'accessory',\n",
       " 'accidence',\n",
       " 'accidency',\n",
       " 'accident',\n",
       " 'accidental',\n",
       " 'accidentalism',\n",
       " 'accidentalist',\n",
       " 'accidentality',\n",
       " 'accidentally',\n",
       " 'accidentalness',\n",
       " 'accidented',\n",
       " 'accidential',\n",
       " 'accidentiality',\n",
       " 'accidently',\n",
       " 'accidia',\n",
       " 'accidie',\n",
       " 'accinge',\n",
       " 'accipient',\n",
       " 'accipitral',\n",
       " 'accipitrary',\n",
       " 'accipitrine',\n",
       " 'accismus',\n",
       " 'accite',\n",
       " 'acclaim',\n",
       " 'acclaimable',\n",
       " 'acclaimer',\n",
       " 'acclamation',\n",
       " 'acclamator',\n",
       " 'acclamatory',\n",
       " 'acclimatable',\n",
       " 'acclimatation',\n",
       " 'acclimate',\n",
       " 'acclimatement',\n",
       " 'acclimation',\n",
       " 'acclimatizable',\n",
       " 'acclimatization',\n",
       " 'acclimatize',\n",
       " 'acclimatizer',\n",
       " 'acclimature',\n",
       " 'acclinal',\n",
       " 'acclinate',\n",
       " 'acclivitous',\n",
       " 'acclivity',\n",
       " 'acclivous',\n",
       " 'accloy',\n",
       " 'accoast',\n",
       " 'accoil',\n",
       " 'accolade',\n",
       " 'accoladed',\n",
       " 'accolated',\n",
       " 'accolent',\n",
       " 'accolle',\n",
       " 'accombination',\n",
       " 'accommodable',\n",
       " 'accommodableness',\n",
       " 'accommodate',\n",
       " 'accommodately',\n",
       " 'accommodateness',\n",
       " 'accommodating',\n",
       " 'accommodatingly',\n",
       " 'accommodation',\n",
       " 'accommodational',\n",
       " 'accommodative',\n",
       " 'accommodativeness',\n",
       " 'accommodator',\n",
       " 'accompanier',\n",
       " 'accompaniment',\n",
       " 'accompanimental',\n",
       " 'accompanist',\n",
       " 'accompany',\n",
       " 'accompanyist',\n",
       " 'accompletive',\n",
       " 'accomplice',\n",
       " 'accompliceship',\n",
       " 'accomplicity',\n",
       " 'accomplish',\n",
       " 'accomplishable',\n",
       " 'accomplished',\n",
       " 'accomplisher',\n",
       " 'accomplishment',\n",
       " 'accomplisht',\n",
       " 'accompt',\n",
       " 'accord',\n",
       " 'accordable',\n",
       " 'accordance',\n",
       " 'accordancy',\n",
       " 'accordant',\n",
       " 'accordantly',\n",
       " 'accorder',\n",
       " 'according',\n",
       " 'accordingly',\n",
       " 'accordion',\n",
       " 'accordionist',\n",
       " 'accorporate',\n",
       " 'accorporation',\n",
       " 'accost',\n",
       " 'accostable',\n",
       " 'accosted',\n",
       " 'accouche',\n",
       " 'accouchement',\n",
       " 'accoucheur',\n",
       " 'accoucheuse',\n",
       " 'account',\n",
       " 'accountability',\n",
       " 'accountable',\n",
       " 'accountableness',\n",
       " 'accountably',\n",
       " 'accountancy',\n",
       " 'accountant',\n",
       " 'accountantship',\n",
       " 'accounting',\n",
       " 'accountment',\n",
       " 'accouple',\n",
       " 'accouplement',\n",
       " 'accouter',\n",
       " 'accouterment',\n",
       " 'accoy',\n",
       " 'accredit',\n",
       " 'accreditate',\n",
       " 'accreditation',\n",
       " 'accredited',\n",
       " 'accreditment',\n",
       " 'accrementitial',\n",
       " 'accrementition',\n",
       " 'accresce',\n",
       " 'accrescence',\n",
       " 'accrescent',\n",
       " 'accretal',\n",
       " 'accrete',\n",
       " 'accretion',\n",
       " 'accretionary',\n",
       " 'accretive',\n",
       " 'accroach',\n",
       " 'accroides',\n",
       " 'accrual',\n",
       " 'accrue',\n",
       " 'accruement',\n",
       " 'accruer',\n",
       " 'accubation',\n",
       " 'accubitum',\n",
       " 'accubitus',\n",
       " 'accultural',\n",
       " 'acculturate',\n",
       " 'acculturation',\n",
       " 'acculturize',\n",
       " 'accumbency',\n",
       " 'accumbent',\n",
       " 'accumber',\n",
       " 'accumulable',\n",
       " 'accumulate',\n",
       " 'accumulation',\n",
       " 'accumulativ',\n",
       " 'accumulative',\n",
       " 'accumulatively',\n",
       " 'accumulativeness',\n",
       " 'accumulator',\n",
       " 'accuracy',\n",
       " 'accurate',\n",
       " 'accurately',\n",
       " 'accurateness',\n",
       " 'accurse',\n",
       " 'accursed',\n",
       " 'accursedly',\n",
       " 'accursedness',\n",
       " 'accusable',\n",
       " 'accusably',\n",
       " 'accusal',\n",
       " 'accusant',\n",
       " 'accusation',\n",
       " 'accusatival',\n",
       " 'accusative',\n",
       " 'accusatively',\n",
       " 'accusatorial',\n",
       " 'accusatorially',\n",
       " 'accusatory',\n",
       " 'accusatrix',\n",
       " 'accuse',\n",
       " 'accused',\n",
       " 'accuser',\n",
       " 'accusingly',\n",
       " 'accusive',\n",
       " 'accustom',\n",
       " 'accustomed',\n",
       " 'accustomedly',\n",
       " 'accustomedness',\n",
       " 'ace',\n",
       " 'aceacenaphthene',\n",
       " 'aceanthrene',\n",
       " 'aceanthrenequinone',\n",
       " 'acecaffine',\n",
       " 'aceconitic',\n",
       " 'acedia',\n",
       " 'acediamine',\n",
       " 'acediast',\n",
       " 'acedy',\n",
       " 'acenaphthene',\n",
       " 'acenaphthenyl',\n",
       " 'acenaphthylene',\n",
       " 'acentric',\n",
       " 'acentrous',\n",
       " 'aceologic',\n",
       " 'aceology',\n",
       " 'acephal',\n",
       " 'acephalan',\n",
       " 'acephalia',\n",
       " 'acephaline',\n",
       " 'acephalism',\n",
       " 'acephalist',\n",
       " ...]"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "wordlist = [w for w in nltk.corpus.words.words('en') if w.islower()]\n",
    "wordlist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "fc72777a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ia', 'iou']"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word = 'supercalifragilisticexpialidocious'\n",
    "re.findall(r'[aeiou]{2,}', word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "3a6a885f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ea',\n",
       " 'oi',\n",
       " 'ea',\n",
       " 'ou',\n",
       " 'oi',\n",
       " 'ea',\n",
       " 'ea',\n",
       " 'oi',\n",
       " 'oi',\n",
       " 'ea',\n",
       " 'io',\n",
       " 'ea',\n",
       " 'ea',\n",
       " 'ea',\n",
       " 'oi',\n",
       " 'ea',\n",
       " 'ea',\n",
       " 'ea',\n",
       " 'ea',\n",
       " 'ea',\n",
       " 'ea',\n",
       " 'ea',\n",
       " 'ee',\n",
       " 'ea',\n",
       " 'ea',\n",
       " 'ea',\n",
       " 'ea',\n",
       " 'ea',\n",
       " 'ea',\n",
       " 'ea',\n",
       " 'ea',\n",
       " 'oi',\n",
       " 'ea',\n",
       " 'ea',\n",
       " 'ou',\n",
       " 'ou',\n",
       " 'ou',\n",
       " 'ie',\n",
       " 'ui',\n",
       " 'io',\n",
       " 'ua',\n",
       " 'io',\n",
       " 'ai',\n",
       " 'ai',\n",
       " 'ai',\n",
       " 'io',\n",
       " 'ie',\n",
       " 'ue',\n",
       " 'ue',\n",
       " 'ia',\n",
       " 'ie',\n",
       " 'ea',\n",
       " 'ai',\n",
       " 'ou',\n",
       " 'ia',\n",
       " 'ei',\n",
       " 'ie',\n",
       " 'ea',\n",
       " 'ea',\n",
       " 'ie',\n",
       " 'ia',\n",
       " 'ia',\n",
       " 'ua',\n",
       " 'ie',\n",
       " 'io',\n",
       " 'ea',\n",
       " 'ia',\n",
       " 'io',\n",
       " 'ui',\n",
       " 'ia',\n",
       " 'ia',\n",
       " 'ea',\n",
       " 'iai',\n",
       " 'ai',\n",
       " 'ia',\n",
       " 'ia',\n",
       " 'ia',\n",
       " 'ia',\n",
       " 'ia',\n",
       " 'io',\n",
       " 'oo',\n",
       " 'io',\n",
       " 'ia',\n",
       " 'ia',\n",
       " 'ia',\n",
       " 'ia',\n",
       " 'ue',\n",
       " 'ea',\n",
       " 'ai',\n",
       " 'ai',\n",
       " 'ue',\n",
       " 'ie',\n",
       " 'au',\n",
       " 'ea',\n",
       " 'ea',\n",
       " 'ea',\n",
       " 'ea',\n",
       " 'eau',\n",
       " 'au',\n",
       " 'ei',\n",
       " 'ei',\n",
       " 'ei',\n",
       " 'ei',\n",
       " 'ei',\n",
       " 'ia',\n",
       " 'ie',\n",
       " 'io',\n",
       " 'ue',\n",
       " 'oa',\n",
       " 'oei',\n",
       " 'oe',\n",
       " 'ia',\n",
       " 'oo',\n",
       " 'oo',\n",
       " 'oo',\n",
       " 'eau',\n",
       " 'ou',\n",
       " 'ou',\n",
       " 'ai',\n",
       " 'ou',\n",
       " 'ai',\n",
       " 'oo',\n",
       " 'ea',\n",
       " 'au',\n",
       " 'ia',\n",
       " 'ea',\n",
       " 'ea',\n",
       " 'ee',\n",
       " 'ia',\n",
       " 'ai',\n",
       " 'oa',\n",
       " 'oo',\n",
       " 'oo',\n",
       " 'oo',\n",
       " 'ei',\n",
       " 'ei',\n",
       " 'ea',\n",
       " 'ui',\n",
       " 'ui',\n",
       " 'eau',\n",
       " 'ie',\n",
       " 'ia',\n",
       " 'ia',\n",
       " 'ai',\n",
       " 'eau',\n",
       " 'ia',\n",
       " 'ea',\n",
       " 'ie',\n",
       " 'ie',\n",
       " 'oo',\n",
       " 'ai',\n",
       " 'ai',\n",
       " 'io',\n",
       " 'ie',\n",
       " 'ie',\n",
       " 'eau',\n",
       " 'ee',\n",
       " 'ee',\n",
       " 'ea',\n",
       " 'io',\n",
       " 'oo',\n",
       " 'ia',\n",
       " 'ie',\n",
       " 'ui',\n",
       " 'io',\n",
       " 'io',\n",
       " 'io',\n",
       " 'ai',\n",
       " 'ai',\n",
       " 'au',\n",
       " 'oa',\n",
       " 'oi',\n",
       " 'ia',\n",
       " 'io',\n",
       " 'io',\n",
       " 'ee',\n",
       " 'ie',\n",
       " 'ea',\n",
       " 'io',\n",
       " 'io',\n",
       " 'ie',\n",
       " 'ou',\n",
       " 'io',\n",
       " 'io',\n",
       " 'ue',\n",
       " 'io',\n",
       " 'io',\n",
       " 'ai',\n",
       " 'ue',\n",
       " 'ui',\n",
       " 'io',\n",
       " 'oo',\n",
       " 'oo',\n",
       " 'io',\n",
       " 'ou',\n",
       " 'ou',\n",
       " 'ou',\n",
       " 'ou',\n",
       " 'ie',\n",
       " 'ou',\n",
       " 'ou',\n",
       " 'ou',\n",
       " 'ou',\n",
       " 'ou',\n",
       " 'ee',\n",
       " 'ui',\n",
       " 'ee',\n",
       " 'ia',\n",
       " 'ai',\n",
       " 'ai',\n",
       " 'ai',\n",
       " 'ie',\n",
       " 'ie',\n",
       " 'oi',\n",
       " 'ie',\n",
       " 'io',\n",
       " 'ea',\n",
       " 'ea',\n",
       " 'ea',\n",
       " 'ee',\n",
       " 'au',\n",
       " 'io',\n",
       " 'ai',\n",
       " 'io',\n",
       " 'oi',\n",
       " 'ia',\n",
       " 'ia',\n",
       " 'eo',\n",
       " 'ie',\n",
       " 'ea',\n",
       " 'io',\n",
       " 'oa',\n",
       " 'oe',\n",
       " 'ai',\n",
       " 'io',\n",
       " 'ie',\n",
       " 'ue',\n",
       " 'ou',\n",
       " 'oi',\n",
       " 'ue',\n",
       " 'ee',\n",
       " 'ie',\n",
       " 'ia',\n",
       " 'ie',\n",
       " 'io',\n",
       " 'io',\n",
       " 'ia',\n",
       " 'io',\n",
       " 'ie',\n",
       " 'ou',\n",
       " 'ou',\n",
       " 'ee',\n",
       " 'ea',\n",
       " 'ai',\n",
       " 'ua',\n",
       " 'ui',\n",
       " 'ui',\n",
       " 'ui',\n",
       " 'uu',\n",
       " 'ea',\n",
       " 'ea',\n",
       " 'io',\n",
       " 'ui',\n",
       " 'ie',\n",
       " 'ia',\n",
       " 'ie',\n",
       " 'ie',\n",
       " 'ei',\n",
       " 'ai',\n",
       " 'ai',\n",
       " 'ai',\n",
       " 'ai',\n",
       " 'ai',\n",
       " 'ie',\n",
       " 'au',\n",
       " 'ea',\n",
       " 'ua',\n",
       " 'io',\n",
       " 'ee',\n",
       " 'ee',\n",
       " 'ie',\n",
       " 'ee',\n",
       " 'ia',\n",
       " 'ia',\n",
       " 'io',\n",
       " 'oo',\n",
       " 'oo',\n",
       " 'oo',\n",
       " 'ei',\n",
       " 'ei',\n",
       " 'ou',\n",
       " 'io',\n",
       " 'ou',\n",
       " 'ou',\n",
       " 'ai',\n",
       " 'ou',\n",
       " 'ou',\n",
       " 'ee',\n",
       " 'ou',\n",
       " 'ie',\n",
       " 'ee',\n",
       " 'ee',\n",
       " 'eu',\n",
       " 'eu',\n",
       " 'ie',\n",
       " 'ie',\n",
       " 'ue',\n",
       " 'ai',\n",
       " 'ie',\n",
       " 'eo',\n",
       " 'eo',\n",
       " 'eo',\n",
       " 'ia',\n",
       " 'eo',\n",
       " 'ia',\n",
       " 'ia',\n",
       " 'ia',\n",
       " 'ie',\n",
       " 'ie',\n",
       " 'au',\n",
       " 'iu',\n",
       " 'ia',\n",
       " 'au',\n",
       " 'oo',\n",
       " 'oo',\n",
       " 'ie',\n",
       " 'ua',\n",
       " 'ua',\n",
       " 'ai',\n",
       " 'ea',\n",
       " 'ee',\n",
       " 'ee',\n",
       " 'ee',\n",
       " 'ee',\n",
       " 'ee',\n",
       " 'ou',\n",
       " 'ou',\n",
       " 'ou',\n",
       " 'ua',\n",
       " 'ee',\n",
       " 'ua',\n",
       " 'ee',\n",
       " 'ua',\n",
       " 'ue',\n",
       " 'ui',\n",
       " 'ui',\n",
       " 'ui',\n",
       " 'ea',\n",
       " 'oo',\n",
       " 'ou',\n",
       " 'ou',\n",
       " 'au',\n",
       " 'io',\n",
       " 'aii',\n",
       " 'aiia',\n",
       " 'ea',\n",
       " 'ea',\n",
       " 'ea',\n",
       " 'ea',\n",
       " 'ea',\n",
       " 'ei',\n",
       " 'ei',\n",
       " 'ei',\n",
       " 'ei',\n",
       " 'oo',\n",
       " 'ie',\n",
       " 'oo',\n",
       " 'ou',\n",
       " 'ou',\n",
       " 'ou',\n",
       " 'ou',\n",
       " 'ou',\n",
       " 'ou',\n",
       " 'ea',\n",
       " 'oi',\n",
       " 'ee',\n",
       " 'ia',\n",
       " 'ia',\n",
       " 'ia',\n",
       " 'ua',\n",
       " 'ua',\n",
       " 'ia',\n",
       " 'ia',\n",
       " 'ie',\n",
       " 'io',\n",
       " 'ia',\n",
       " 'ou',\n",
       " 'ea',\n",
       " 'io',\n",
       " 'io',\n",
       " 'eu',\n",
       " 'ia',\n",
       " 'io',\n",
       " 'ie',\n",
       " 'ia',\n",
       " 'ie',\n",
       " 'ae',\n",
       " 'ue',\n",
       " 'ia',\n",
       " 'ua',\n",
       " 'ai',\n",
       " 'aa',\n",
       " 'aa',\n",
       " 'ai',\n",
       " 'ua',\n",
       " 'oa',\n",
       " 'oe',\n",
       " 'oe',\n",
       " 'ou',\n",
       " 'ou',\n",
       " 'ia',\n",
       " 'ia',\n",
       " 'ie',\n",
       " 'ia',\n",
       " 'oo',\n",
       " 'ee',\n",
       " 'ei',\n",
       " 'ea',\n",
       " 'ea',\n",
       " 'ee',\n",
       " 'ei',\n",
       " 'ei',\n",
       " 'ee',\n",
       " 'au',\n",
       " 'ei',\n",
       " 'oi',\n",
       " 'oi',\n",
       " 'ea',\n",
       " 'ua',\n",
       " 'ie',\n",
       " 'oui',\n",
       " 'ieu',\n",
       " 'ou',\n",
       " 'au',\n",
       " 'au',\n",
       " 'au',\n",
       " 'ea',\n",
       " 'ea',\n",
       " 'ea',\n",
       " 'ee',\n",
       " 'ou',\n",
       " 'io',\n",
       " 'ei',\n",
       " 'ei',\n",
       " 'ei',\n",
       " 'eo',\n",
       " 'eo',\n",
       " 'io',\n",
       " 'ie',\n",
       " 'ou',\n",
       " 'oa',\n",
       " 'oe',\n",
       " 'oe',\n",
       " 'oo',\n",
       " 'ai',\n",
       " 'oui',\n",
       " 'oui',\n",
       " 'ia',\n",
       " 'oui',\n",
       " 'ia',\n",
       " 'oui',\n",
       " 'ie',\n",
       " 'ae',\n",
       " 'ai',\n",
       " 'ia',\n",
       " 'ia',\n",
       " 'ie',\n",
       " 'ie',\n",
       " 'oui',\n",
       " 'ie',\n",
       " 'io',\n",
       " 'io',\n",
       " 'io',\n",
       " 'ia',\n",
       " 'au',\n",
       " 'ui',\n",
       " 'eo',\n",
       " 'ea',\n",
       " 'ea',\n",
       " 'ia',\n",
       " 'ee',\n",
       " 'ei',\n",
       " 'ia',\n",
       " 'ia',\n",
       " 'ia',\n",
       " 'ae',\n",
       " 'ae',\n",
       " 'io',\n",
       " 'ue',\n",
       " 'ai',\n",
       " 'au',\n",
       " 'ee',\n",
       " 'ea',\n",
       " 'ea',\n",
       " 'io',\n",
       " 'ou',\n",
       " 'ui',\n",
       " 'io',\n",
       " 'oi',\n",
       " 'ou',\n",
       " 'ie',\n",
       " 'oo',\n",
       " 'oo',\n",
       " 'oo',\n",
       " 'eo',\n",
       " 'ou',\n",
       " 'ua',\n",
       " 'oi',\n",
       " 'ia',\n",
       " 'ia',\n",
       " 'ia',\n",
       " 'io',\n",
       " 'io',\n",
       " 'io',\n",
       " 'io',\n",
       " 'ea',\n",
       " 'ea',\n",
       " 'ea',\n",
       " 'ee',\n",
       " 'ia',\n",
       " 'ei',\n",
       " 'ei',\n",
       " 'oo',\n",
       " 'eu',\n",
       " 'ou',\n",
       " 'ee',\n",
       " 'ua',\n",
       " 'ie',\n",
       " 'ei',\n",
       " 'ai',\n",
       " 'ie',\n",
       " 'ea',\n",
       " 'ia',\n",
       " 'ou',\n",
       " 'ie',\n",
       " 'ou',\n",
       " 'ei',\n",
       " 'io',\n",
       " 'io',\n",
       " 'ea',\n",
       " 'ia',\n",
       " 'io',\n",
       " 'io',\n",
       " 'ou',\n",
       " 'ia',\n",
       " 'io',\n",
       " 'io',\n",
       " 'io',\n",
       " 'io',\n",
       " 'io',\n",
       " 'ea',\n",
       " 'ea',\n",
       " 'ea',\n",
       " 'ai',\n",
       " 'ia',\n",
       " 'ua',\n",
       " 'ou',\n",
       " 'ia',\n",
       " 'ie',\n",
       " 'ia',\n",
       " 'au',\n",
       " 'au',\n",
       " 'ou',\n",
       " 'ea',\n",
       " 'ia',\n",
       " 'ie',\n",
       " 'eo',\n",
       " 'eo',\n",
       " 'ia',\n",
       " 'oi',\n",
       " 'io',\n",
       " 'ua',\n",
       " 'eu',\n",
       " 'ao',\n",
       " 'ia',\n",
       " 'ia',\n",
       " 'oe',\n",
       " 'ie',\n",
       " 'ie',\n",
       " 'io',\n",
       " 'ai',\n",
       " 'io',\n",
       " 'ie',\n",
       " 'oo',\n",
       " 'oo',\n",
       " 'io',\n",
       " 'ua',\n",
       " 'iou',\n",
       " 'ie',\n",
       " 'ia',\n",
       " 'iou',\n",
       " 'io',\n",
       " 'ee',\n",
       " 'io',\n",
       " 'io',\n",
       " 'ua',\n",
       " 'io',\n",
       " 'ia',\n",
       " 'ia',\n",
       " 'io',\n",
       " 'io',\n",
       " 'ue',\n",
       " 'ua',\n",
       " 'uee',\n",
       " 'ia',\n",
       " 'io',\n",
       " 'ae',\n",
       " 'ai',\n",
       " 'ai',\n",
       " 'oa',\n",
       " 'ei',\n",
       " 'ie',\n",
       " 'au',\n",
       " 'oo',\n",
       " 'ea',\n",
       " 'ea',\n",
       " 'ea',\n",
       " 'ea',\n",
       " 'ea',\n",
       " 'oo',\n",
       " 'ee',\n",
       " 'io',\n",
       " 'ia',\n",
       " 'ai',\n",
       " 'ee',\n",
       " 'ea',\n",
       " 'ea',\n",
       " 'ou',\n",
       " 'au',\n",
       " 'ai',\n",
       " 'eu',\n",
       " 'eu',\n",
       " 'eu',\n",
       " 'ue',\n",
       " 'ie',\n",
       " 'io',\n",
       " 'ou',\n",
       " 'ie',\n",
       " 'ie',\n",
       " 'ie',\n",
       " 'ie',\n",
       " 'ie',\n",
       " 'oe',\n",
       " 'ee',\n",
       " 'oo',\n",
       " 'oo',\n",
       " 'oo',\n",
       " 'ie',\n",
       " 'ou',\n",
       " 'ou',\n",
       " 'ie',\n",
       " 'ia',\n",
       " 'ai',\n",
       " 'ai',\n",
       " 'ue',\n",
       " 'au',\n",
       " 'au',\n",
       " 'au',\n",
       " 'au',\n",
       " 'ia',\n",
       " 'ae',\n",
       " 'oo',\n",
       " 'ei',\n",
       " 'ie',\n",
       " 'ie',\n",
       " 'ia',\n",
       " 'ea',\n",
       " 'ea',\n",
       " 'ea',\n",
       " 'ea',\n",
       " 'ea',\n",
       " 'ea',\n",
       " 'ia',\n",
       " 'ia',\n",
       " 'io',\n",
       " 'ie',\n",
       " 'ee',\n",
       " 'ou',\n",
       " 'io',\n",
       " 'io',\n",
       " 'eou',\n",
       " 'ia',\n",
       " 'ie',\n",
       " 'io',\n",
       " 'ou',\n",
       " 'ea',\n",
       " 'ee',\n",
       " 'ie',\n",
       " 'oo',\n",
       " 'ai',\n",
       " 'ia',\n",
       " 'ie',\n",
       " 'ie',\n",
       " 'eo',\n",
       " 'ie',\n",
       " 'oa',\n",
       " 'ia',\n",
       " 'ie',\n",
       " 'aia',\n",
       " 'io',\n",
       " 'oo',\n",
       " 'ou',\n",
       " 'ou',\n",
       " 'ou',\n",
       " 'ou',\n",
       " 'ou',\n",
       " 'ou',\n",
       " 'ea',\n",
       " 'ou',\n",
       " 'ie',\n",
       " 'ie',\n",
       " 'ai',\n",
       " 'io',\n",
       " 'ee',\n",
       " 'ie',\n",
       " 'ie',\n",
       " 'oo',\n",
       " 'ea',\n",
       " 'ie',\n",
       " 'uie',\n",
       " 'iu',\n",
       " 'iu',\n",
       " 'ea',\n",
       " 'ee',\n",
       " 'ee',\n",
       " 'ei',\n",
       " 'ie',\n",
       " 'ie',\n",
       " 'ie',\n",
       " 'ai',\n",
       " 'ee',\n",
       " 'ua',\n",
       " 'ui',\n",
       " 'ai',\n",
       " 'ea',\n",
       " 'ai',\n",
       " 'ai',\n",
       " 'ei',\n",
       " 'ai',\n",
       " 'ai',\n",
       " 'ai',\n",
       " 'eo',\n",
       " 'au',\n",
       " 'ea',\n",
       " 'ea',\n",
       " 'ei',\n",
       " 'io',\n",
       " 'io',\n",
       " 'ai',\n",
       " 'ai',\n",
       " 'ei',\n",
       " 'eo',\n",
       " 'ou',\n",
       " 'ou',\n",
       " 'ee',\n",
       " 'oo',\n",
       " 'io',\n",
       " 'oo',\n",
       " 'ou',\n",
       " 'ou',\n",
       " 'io',\n",
       " 'io',\n",
       " 'ai',\n",
       " 'ai',\n",
       " 'io',\n",
       " 'ea',\n",
       " 'ie',\n",
       " 'ee',\n",
       " 'au',\n",
       " 'ou',\n",
       " 'eau',\n",
       " 'ue',\n",
       " 'ou',\n",
       " 'ai',\n",
       " 'oo',\n",
       " 'oo',\n",
       " 'io',\n",
       " 'ie',\n",
       " 'ie',\n",
       " 'ue',\n",
       " 'ia',\n",
       " 'eo',\n",
       " 'ia',\n",
       " 'ia',\n",
       " 'ua',\n",
       " 'oi',\n",
       " 'ae',\n",
       " 'ae',\n",
       " 'ui',\n",
       " 'ou',\n",
       " 'oo',\n",
       " 'ea',\n",
       " 'ee',\n",
       " 'ei',\n",
       " 'ei',\n",
       " 'ie',\n",
       " 'ei',\n",
       " 'ou',\n",
       " 'ee',\n",
       " 'ea',\n",
       " 'ia',\n",
       " 'ia',\n",
       " 'ia',\n",
       " 'ou',\n",
       " 'oo',\n",
       " 'oo',\n",
       " 'oo',\n",
       " 'oo',\n",
       " 'oo',\n",
       " 'ee',\n",
       " 'ia',\n",
       " 'iao',\n",
       " 'ai',\n",
       " 'ea',\n",
       " 'ea',\n",
       " 'ie',\n",
       " 'ou',\n",
       " 'ou',\n",
       " 'ou',\n",
       " 'ou',\n",
       " 'ou',\n",
       " 'ue',\n",
       " 'ea',\n",
       " 'oa',\n",
       " 'io',\n",
       " 'io',\n",
       " 'io',\n",
       " 'io',\n",
       " 'ou',\n",
       " 'ou',\n",
       " 'oa',\n",
       " 'io',\n",
       " 'ie',\n",
       " 'io',\n",
       " 'ou',\n",
       " 'ou',\n",
       " 'ou',\n",
       " 'ou',\n",
       " 'ou',\n",
       " 'ou',\n",
       " 'ou',\n",
       " 'ue',\n",
       " 'ue',\n",
       " 'ie',\n",
       " 'ie',\n",
       " 'ie',\n",
       " 'ie',\n",
       " 'ie',\n",
       " 'ai',\n",
       " 'ui',\n",
       " 'ui',\n",
       " 'ui',\n",
       " 'ui',\n",
       " 'ui',\n",
       " 'ui',\n",
       " 'ui',\n",
       " 'io',\n",
       " 'ui',\n",
       " 'io',\n",
       " 'ui',\n",
       " 'io',\n",
       " 'io',\n",
       " 'io',\n",
       " 'ie',\n",
       " 'ua',\n",
       " 'ua',\n",
       " 'io',\n",
       " 'io',\n",
       " 'io',\n",
       " 'io',\n",
       " 'ua',\n",
       " 'ua',\n",
       " 'io',\n",
       " 'io',\n",
       " 'ia',\n",
       " 'ae',\n",
       " 'ae',\n",
       " 'ai',\n",
       " 'ai',\n",
       " 'ia',\n",
       " 'ia',\n",
       " 'ai',\n",
       " 'oo',\n",
       " 'ai',\n",
       " 'ai',\n",
       " 'ie',\n",
       " 'ee',\n",
       " 'ee',\n",
       " 'ee',\n",
       " 'eei',\n",
       " 'ee',\n",
       " 'ee',\n",
       " 'ee',\n",
       " 'ea',\n",
       " 'ai',\n",
       " 'ai',\n",
       " 'ai',\n",
       " 'ai',\n",
       " 'ai',\n",
       " 'ai',\n",
       " 'ai',\n",
       " 'ai',\n",
       " 'ai',\n",
       " 'ai',\n",
       " 'ai',\n",
       " 'ai',\n",
       " 'ai',\n",
       " 'ai',\n",
       " 'ai',\n",
       " 'ai',\n",
       " 'ai',\n",
       " 'ai',\n",
       " 'ai',\n",
       " 'ie',\n",
       " 'io',\n",
       " 'ia',\n",
       " 'ie',\n",
       " 'io',\n",
       " 'ea',\n",
       " 'ou',\n",
       " 'ui',\n",
       " 'io',\n",
       " 'iou',\n",
       " 'io',\n",
       " 'ou',\n",
       " 'ou',\n",
       " 'ou',\n",
       " 'ie',\n",
       " 'ou',\n",
       " 'ou',\n",
       " 'ou',\n",
       " 'ou',\n",
       " 'ua',\n",
       " 'ua',\n",
       " 'ua',\n",
       " 'io',\n",
       " 'io',\n",
       " 'ia',\n",
       " 'eo',\n",
       " 'io',\n",
       " 'ie',\n",
       " 'ie',\n",
       " 'iou',\n",
       " 'ie',\n",
       " 'ea',\n",
       " 'ea',\n",
       " 'ea',\n",
       " 'ea',\n",
       " 'ea',\n",
       " 'ea',\n",
       " 'ea',\n",
       " 'ea',\n",
       " 'au',\n",
       " 'ia',\n",
       " 'ia',\n",
       " 'io',\n",
       " 'io',\n",
       " 'ie',\n",
       " 'oi',\n",
       " 'oi',\n",
       " 'oi',\n",
       " 'oi',\n",
       " 'ia',\n",
       " 'io',\n",
       " 'oa',\n",
       " 'oa',\n",
       " 'oa',\n",
       " 'ia',\n",
       " 'ia',\n",
       " 'ia',\n",
       " 'io',\n",
       " 'ia',\n",
       " 'io',\n",
       " 'ia',\n",
       " 'ea',\n",
       " 'ea',\n",
       " 'ue',\n",
       " 'ue',\n",
       " 'ue',\n",
       " 'ui',\n",
       " 'ou',\n",
       " 'io',\n",
       " 'au',\n",
       " 'au',\n",
       " 'io',\n",
       " 'io',\n",
       " 'ua',\n",
       " 'io',\n",
       " 'ia',\n",
       " 'ia',\n",
       " 'ia',\n",
       " 'ia',\n",
       " 'io',\n",
       " 'io',\n",
       " 'io',\n",
       " 'ie',\n",
       " 'io',\n",
       " 'au',\n",
       " 'io',\n",
       " 'au',\n",
       " ...]"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wsj = sorted(set(nltk.corpus.treebank.words()))\n",
    "\n",
    "[vs for word in wsj\n",
    "for vs in re.findall(r'[aeiou]{2,}', word)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "93d26050",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_items([('ea', 476), ('oi', 65), ('ou', 329), ('io', 549), ('ee', 217), ('ie', 331), ('ui', 95), ('ua', 109), ('ai', 261), ('ue', 105), ('ia', 253), ('ei', 86), ('iai', 1), ('oo', 174), ('au', 106), ('eau', 10), ('oa', 59), ('oei', 1), ('oe', 15), ('eo', 39), ('uu', 1), ('eu', 18), ('iu', 14), ('aii', 1), ('aiia', 1), ('ae', 11), ('aa', 3), ('oui', 6), ('ieu', 3), ('ao', 6), ('iou', 27), ('uee', 4), ('eou', 5), ('aia', 1), ('uie', 3), ('iao', 1), ('eei', 2), ('uo', 8), ('uou', 5), ('eea', 1), ('ueui', 1), ('ioa', 1), ('ooi', 1)])"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wsj = sorted(set(nltk.corpus.treebank.words()))\n",
    "\n",
    "fd = nltk.FreqDist(vs for word in wsj\n",
    "for vs in re.findall(r'[aeiou]{2,}', word))\n",
    "fd.items()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "3151acb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unvrsl Dclrtn of Hmn Rghts Prmble Whrs rcgntn of the inhrnt dgnty and\n",
      "of the eql and inlnble rghts of all mmbrs of the hmn fmly is the fndtn\n",
      "of frdm , jstce and pce in the wrld , Whrs dsrgrd and cntmpt fr hmn\n",
      "rghts hve rsltd in brbrs acts whch hve outrgd the cnscnce of mnknd ,\n",
      "and the advnt of a wrld in whch hmn bngs shll enjy frdm of spch and\n"
     ]
    }
   ],
   "source": [
    "regexp = r'^[AEIOUaeiou]+|[AEIOUaeiou]+$|[^AEIOUaeiou]'\n",
    "def compress(word):\n",
    "    pieces = re.findall(regexp, word)\n",
    "    return ''.join(pieces)\n",
    "\n",
    "english_udhr = nltk.corpus.udhr.words('English-Latin1')\n",
    "print(nltk.tokenwrap(compress(w) for w in english_udhr[:75]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "ba5aefdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    a   e   i   o   u \n",
      "k 418 148  94 420 173 \n",
      "p  83  31 105  34  51 \n",
      "r 187  63  84  89  79 \n",
      "s   0   0 100   2   1 \n",
      "t  47   8   0 148  37 \n",
      "v  93  27 105  48  49 \n"
     ]
    }
   ],
   "source": [
    "rotokas_words = nltk.corpus.toolbox.words('rotokas.dic')\n",
    "cvs = [cv for w in rotokas_words for cv in re.findall(r'[ptksvr][aeiou]', w)]\n",
    "cfd = nltk.ConditionalFreqDist(cvs)\n",
    "cfd.tabulate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "83e81429",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['kasuari']\n",
      "['kaapo', 'kaapopato', 'kaipori', 'kaiporipie', 'kaiporivira', 'kapo', 'kapoa', 'kapokao', 'kapokapo', 'kapokapo', 'kapokapoa', 'kapokapoa', 'kapokapora', 'kapokapora', 'kapokaporo', 'kapokaporo', 'kapokari', 'kapokarito', 'kapokoa', 'kapoo', 'kapooto', 'kapoovira', 'kapopaa', 'kaporo', 'kaporo', 'kaporopa', 'kaporoto', 'kapoto', 'karokaropo', 'karopo', 'kepo', 'kepoi', 'keposi', 'kepoto']\n"
     ]
    }
   ],
   "source": [
    "cv_word_pairs = [(cv, w) for w in rotokas_words for cv in re.findall(r'[ptksvr][aeiou]', w)]\n",
    "cv_index = nltk.Index(cv_word_pairs)\n",
    "print(cv_index['su'])\n",
    "print(cv_index['po'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "b95c3652",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reinstate\n"
     ]
    }
   ],
   "source": [
    "def stem(word):\n",
    "    for suffix in ['ing', 'ly', 'ed', 'ious', 'ies', 'ive', 'es', 's', 'ment']:\n",
    "        if word.endswith(suffix):\n",
    "            return word[:-len(suffix)]\n",
    "    return word\n",
    "\n",
    "print(stem('reinstatement'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "54e2b5b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ing']"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.findall(r'^.*(ing|ly|ed|ious|ies|ive|es|s|ment)$', 'processing')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "85f5e03a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('language', '')]"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.findall(r'^(.*?)(ing|ly|ed|ious|ies|ive|es|s|ment)?$', 'language')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "2cd52484",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reinstate\n"
     ]
    }
   ],
   "source": [
    "def stem(word):\n",
    "    regexp = r'^(.*?)(ing|ly|ed|ious|ies|ive|es|s|ment)?$'\n",
    "    stem, suffix = re.findall(regexp, word)[0]\n",
    "    return stem\n",
    "\n",
    "print(stem('reinstatement'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "e7b9b824",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['DENNIS',\n",
       " ':',\n",
       " 'Listen',\n",
       " ',',\n",
       " 'strange',\n",
       " 'women',\n",
       " 'ly',\n",
       " 'in',\n",
       " 'pond',\n",
       " 'distribut',\n",
       " 'sword',\n",
       " 'i',\n",
       " 'no',\n",
       " 'basi',\n",
       " 'for',\n",
       " 'a',\n",
       " 'system',\n",
       " 'of',\n",
       " 'govern',\n",
       " '.',\n",
       " 'Supreme',\n",
       " 'execut',\n",
       " 'power',\n",
       " 'deriv',\n",
       " 'from',\n",
       " 'a',\n",
       " 'mandate',\n",
       " 'from',\n",
       " 'the',\n",
       " 'mass',\n",
       " ',',\n",
       " 'not',\n",
       " 'from',\n",
       " 'some',\n",
       " 'farcical',\n",
       " 'aquatic',\n",
       " 'ceremony',\n",
       " '.']"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw = \"\"\"DENNIS: Listen, strange women lying in ponds distributing swords\n",
    "... is no basis for a system of government.  Supreme executive power derives from\n",
    "... a mandate from the masses, not from some farcical aquatic ceremony.\"\"\"\n",
    "tokens = nltk.word_tokenize(raw)\n",
    "[stem(t) for t in tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "21da560d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "monied; nervous; dangerous; white; white; white; pious; queer; good;\n",
      "mature; white; Cape; great; wise; wise; butterless; white; fiendish;\n",
      "pale; furious; better; certain; complete; dismasted; younger; brave;\n",
      "brave; brave; brave\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import gutenberg, nps_chat\n",
    "moby = nltk.Text(gutenberg.words('melville-moby_dick.txt'))\n",
    "moby.findall(r\"<a> (<.*>) <man>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "484cef40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "you rule bro; telling you bro; u twizted bro\n"
     ]
    }
   ],
   "source": [
    "chat = nltk.Text(nps_chat.words())\n",
    "chat.findall(r\"<.*> <.*> <bro>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "2baae3cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lol lol lol; lmao lol lol; lol lol lol; la la la la la; la la la; la\n",
      "la la; lovely lol lol love; lol lol lol.; la la la; la la la\n"
     ]
    }
   ],
   "source": [
    "chat.findall(r\"<l.*>{3,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "66da3ed6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "speed and other activities; water and other liquids; tomb and other\n",
      "landmarks; Statues and other monuments; pearls and other jewels;\n",
      "charts and other items; roads and other features; figures and other\n",
      "objects; military and other areas; demands and other factors;\n",
      "abstracts and other compilations; iron and other metals\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import brown\n",
    "hobbies_learned = nltk.Text(brown.words(categories=['hobbies', 'learned']))\n",
    "hobbies_learned.findall(r\"<\\w*> <and> <other> <\\w*s>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "789f7347",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['denni', ':', 'listen', ',', 'strang', 'women', 'lie', 'in', 'pond', 'distribut', 'sword', 'is', 'no', 'basi', 'for', 'a', 'system', 'of', 'govern', '.', 'suprem', 'execut', 'power', 'deriv', 'from', 'a', 'mandat', 'from', 'the', 'mass', ',', 'not', 'from', 'some', 'farcic', 'aquat', 'ceremoni', '.']\n",
      "['den', ':', 'list', ',', 'strange', 'wom', 'lying', 'in', 'pond', 'distribut', 'sword', 'is', 'no', 'bas', 'for', 'a', 'system', 'of', 'govern', '.', 'suprem', 'execut', 'pow', 'der', 'from', 'a', 'mand', 'from', 'the', 'mass', ',', 'not', 'from', 'som', 'farc', 'aqu', 'ceremony', '.']\n"
     ]
    }
   ],
   "source": [
    "porter = nltk.PorterStemmer()\n",
    "lancaster = nltk.LancasterStemmer()\n",
    "print([porter.stem(t) for t in tokens])\n",
    "print([lancaster.stem(t) for t in tokens])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "db45db16",
   "metadata": {},
   "outputs": [],
   "source": [
    "class IndexedText(object):\n",
    "\n",
    "    def __init__(self, stemmer, text):\n",
    "        self._text = text\n",
    "        self._stemmer = stemmer\n",
    "        self._index = nltk.Index((self._stem(word), i)\n",
    "                                 for (i, word) in enumerate(text))\n",
    "\n",
    "    def concordance(self, word, width=40):\n",
    "        key = self._stem(word)\n",
    "        wc = width               # words of context\n",
    "        for i in self._index[key]:\n",
    "            lcontext = ' '.join(self._text[i-wc:i])\n",
    "            rcontext = ' '.join(self._text[i:i+wc])\n",
    "            ldisplay = '%*s'  % (width, lcontext[-width:])\n",
    "            rdisplay = '%-*s' % (width, rcontext[:width])\n",
    "            print(ldisplay, rdisplay)\n",
    "\n",
    "    def _stem(self, word):\n",
    "        return self._stemmer.stem(word).lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "178611e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "r king ! DENNIS : Listen , strange women lying in ponds distributing swords is no\n",
      " beat a very brave retreat . ROBIN : All lies ! MINSTREL : [ singing ] Bravest of\n",
      "ghts . Nay . Nay . Come . Come . You may lie here . Oh , but you are wounded ! GA\n",
      "doctors immediately ! No , no , please ! Lie down . [ clap clap ] PIGLET : Well ,\n",
      "ere is much danger , for beyond the cave lies the Gorge of Eternal Peril , which \n",
      "nk you . Oh ... TIM : To the north there lies a cave -- the cave of Caerbannog --\n",
      "h it and lived ! Bones of full fifty men lie strewn about its lair . So , brave k\n",
      "not stop our fight ' til each one of you lies dead , and the Holy Grail returns t\n"
     ]
    }
   ],
   "source": [
    "porter = nltk.PorterStemmer()\n",
    "grail = nltk.corpus.webtext.words('grail.txt')\n",
    "text = IndexedText(porter, grail)\n",
    "text.concordance('lie')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "2401b7e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['DENNIS',\n",
       " ':',\n",
       " 'Listen',\n",
       " ',',\n",
       " 'strange',\n",
       " 'woman',\n",
       " 'lying',\n",
       " 'in',\n",
       " 'pond',\n",
       " 'distributing',\n",
       " 'sword',\n",
       " 'is',\n",
       " 'no',\n",
       " 'basis',\n",
       " 'for',\n",
       " 'a',\n",
       " 'system',\n",
       " 'of',\n",
       " 'government',\n",
       " '.',\n",
       " 'Supreme',\n",
       " 'executive',\n",
       " 'power',\n",
       " 'derives',\n",
       " 'from',\n",
       " 'a',\n",
       " 'mandate',\n",
       " 'from',\n",
       " 'the',\n",
       " 'mass',\n",
       " ',',\n",
       " 'not',\n",
       " 'from',\n",
       " 'some',\n",
       " 'farcical',\n",
       " 'aquatic',\n",
       " 'ceremony',\n",
       " '.']"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wnl = nltk.WordNetLemmatizer()\n",
    "[wnl.lemmatize(t) for t in tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "f38e1e48",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"'When\",\n",
       " \"I'M\",\n",
       " 'a',\n",
       " \"Duchess,'\",\n",
       " 'she',\n",
       " 'said',\n",
       " 'to',\n",
       " 'herself,',\n",
       " '(not',\n",
       " 'in',\n",
       " 'a',\n",
       " 'very',\n",
       " 'hopeful',\n",
       " 'tone\\nthough),',\n",
       " \"'I\",\n",
       " \"won't\",\n",
       " 'have',\n",
       " 'any',\n",
       " 'pepper',\n",
       " 'in',\n",
       " 'my',\n",
       " 'kitchen',\n",
       " 'AT',\n",
       " 'ALL.',\n",
       " 'Soup',\n",
       " 'does',\n",
       " 'very\\nwell',\n",
       " 'without--Maybe',\n",
       " \"it's\",\n",
       " 'always',\n",
       " 'pepper',\n",
       " 'that',\n",
       " 'makes',\n",
       " 'people',\n",
       " \"hot-tempered,'...\"]"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw = \"\"\"'When I'M a Duchess,' she said to herself, (not in a very hopeful tone\n",
    "... though), 'I won't have any pepper in my kitchen AT ALL. Soup does very\n",
    "... well without--Maybe it's always pepper that makes people hot-tempered,'...\"\"\"\n",
    "re.split(r' ', raw) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "c826de0a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['',\n",
       " 'When',\n",
       " 'I',\n",
       " 'M',\n",
       " 'a',\n",
       " 'Duchess',\n",
       " 'she',\n",
       " 'said',\n",
       " 'to',\n",
       " 'herself',\n",
       " 'not',\n",
       " 'in',\n",
       " 'a',\n",
       " 'very',\n",
       " 'hopeful',\n",
       " 'tone',\n",
       " 'though',\n",
       " 'I',\n",
       " 'won',\n",
       " 't',\n",
       " 'have',\n",
       " 'any',\n",
       " 'pepper',\n",
       " 'in',\n",
       " 'my',\n",
       " 'kitchen',\n",
       " 'AT',\n",
       " 'ALL',\n",
       " 'Soup',\n",
       " 'does',\n",
       " 'very',\n",
       " 'well',\n",
       " 'without',\n",
       " 'Maybe',\n",
       " 'it',\n",
       " 's',\n",
       " 'always',\n",
       " 'pepper',\n",
       " 'that',\n",
       " 'makes',\n",
       " 'people',\n",
       " 'hot',\n",
       " 'tempered',\n",
       " '']"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.split(r'\\W+', raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "28c495a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"'\", 'When', \"I'M\", 'a', 'Duchess', ',', \"'\", 'she', 'said', 'to', 'herself', ',', '(', 'not', 'in', 'a', 'very', 'hopeful', 'tone', 'though', ')', ',', \"'\", 'I', \"won't\", 'have', 'any', 'pepper', 'in', 'my', 'kitchen', 'AT', 'ALL', '.', 'Soup', 'does', 'very', 'well', 'without', '--', 'Maybe', \"it's\", 'always', 'pepper', 'that', 'makes', 'people', 'hot-tempered', ',', \"'\", '...']\n"
     ]
    }
   ],
   "source": [
    "print(re.findall(r\"\\w+(?:[-']\\w+)*|'|[-.(]+|\\S\\w*\", raw))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "32532385",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('', '', ''), ('', '', ''), ('', '-print', ''), ('', '', ''), ('', '', '')]"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"That U.S.A. poster-print costs \\$12.40.\"\n",
    "pattern = r'''(?x)    # set flag to allow verbose regexps<br>\n",
    "...     ([A-Z]\\.)+        # abbreviations, e.g. U.S.A.<br>\n",
    "...   | \\w+(-\\w+)*        # words with optional internal hyphens<br>\n",
    "...   | \\$?\\d+(\\.\\d+)?%?  # currency and percentages, e.g. \\$12.40, 82%<br>\n",
    "...   | \\.\\.\\.            # ellipsis<br>\n",
    "...   | [][.,;\"'?():-_`]  # these are separate tokens<br>\n",
    "... '''\n",
    "nltk.regexp_tokenize(text, pattern)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "02658409",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20.250994070456922"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(nltk.corpus.brown.words()) / len(nltk.corpus.brown.sents())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "c92a7f82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['In the wild events which were to follow this girl had no\\n'\n",
      " 'part at all; he never saw her again until all his tale was over.',\n",
      " 'And yet, in some indescribable way, she kept recurring like a\\n'\n",
      " 'motive in music through all his mad adventures afterwards, and the\\n'\n",
      " 'glory of her strange hair ran like a red thread through those dark\\n'\n",
      " 'and ill-drawn tapestries of the night.',\n",
      " 'For what followed was so\\nimprobable, that it might well have been a dream.',\n",
      " 'When Syme went out into the starlit street, he found it for the\\n'\n",
      " 'moment empty.',\n",
      " 'Then he realised (in some odd way) that the silence\\n'\n",
      " 'was rather a living silence than a dead one.',\n",
      " 'Directly outside the\\n'\n",
      " 'door stood a street lamp, whose gleam gilded the leaves of the tree\\n'\n",
      " 'that bent out over the fence behind him.',\n",
      " 'About a foot from the\\n'\n",
      " 'lamp-post stood a figure almost as rigid and motionless as the\\n'\n",
      " 'lamp-post itself.',\n",
      " 'The tall hat and long frock coat were black; the\\n'\n",
      " 'face, in an abrupt shadow, was almost as dark.',\n",
      " 'Only a fringe of\\n'\n",
      " 'fiery hair against the light, and also something aggressive in the\\n'\n",
      " 'attitude, proclaimed that it was the poet Gregory.',\n",
      " 'He had something\\n'\n",
      " 'of the look of a masked bravo waiting sword in hand for his foe.']\n"
     ]
    }
   ],
   "source": [
    "sent_tokenizer=nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "text = nltk.corpus.gutenberg.raw('chesterton-thursday.txt')\n",
    "sents = sent_tokenizer.tokenize(text)\n",
    "pprint.pprint(sents[171:181])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "cee62e6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"doyouseethekittyseethedoggydoyoulikethekittylikethedoggy\"\n",
    "seg1 = \"0000000000000001000000000010000000000000000100000000000\"\n",
    "seg2 = \"0100100100100001001001000010100100010010000100010010000\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "c1223fa6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['doyouseethekitty', 'seethedoggy', 'doyoulikethekitty', 'likethedoggy']"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " def segment(text, segs):\n",
    "    words = []\n",
    "    last = 0\n",
    "    for i in range(len(segs)):\n",
    "        if segs[i] == '1':\n",
    "            words.append(text[last:i+1])\n",
    "            last = i+1\n",
    "    words.append(text[last:])\n",
    "    return words\n",
    "\n",
    "segment(text, seg1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "b3737068",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "47"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def evaluate(text, segs):\n",
    "    words = segment(text, segs)\n",
    "    text_size = len(words)\n",
    "    lexicon_size = len(' '.join(list(set(words))))\n",
    "    return text_size + lexicon_size\n",
    "\n",
    "evaluate(text, seg2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "6bb84e21",
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import randint\n",
    "\n",
    "def flip(segs, pos):\n",
    "    return segs[:pos] + str(1-int(segs[pos])) + segs[pos+1:] \n",
    "\n",
    "def flip_n(segs, n):\n",
    "    for i in range(n):\n",
    "        segs = flip(segs, randint(0,len(segs)-1))\n",
    "    return segs\n",
    "\n",
    "def anneal(text, segs, iterations, cooling_rate):\n",
    "    temperature = float(len(segs))\n",
    "    while temperature > 0.5:\n",
    "        best_segs, best = segs, evaluate(text, segs)\n",
    "        for i in range(iterations):\n",
    "            guess = flip_n(segs, int(round(temperature)))\n",
    "            score = evaluate(text, guess)\n",
    "            if score < best:\n",
    "                best, best_segs = score, guess\n",
    "        score, segs = best, best_segs\n",
    "        temperature = temperature / cooling_rate\n",
    "        print(evaluate(text, segs), segment(text, segs))\n",
    "    print()\n",
    "    return segs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "4a536231",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63 ['doyouseethekitty', 'seethedoggy', 'doyoulikethekitty', 'likethedoggy']\n",
      "63 ['doyouseethekitty', 'seethedoggy', 'doyoulikethekitty', 'likethedoggy']\n",
      "63 ['doyouseethekitty', 'seethedoggy', 'doyoulikethekitty', 'likethedoggy']\n",
      "63 ['doyouseethekitty', 'seethedoggy', 'doyoulikethekitty', 'likethedoggy']\n",
      "63 ['doyouseethekitty', 'seethedoggy', 'doyoulikethekitty', 'likethedoggy']\n",
      "63 ['doyouseethekitty', 'seethedoggy', 'doyoulikethekitty', 'likethedoggy']\n",
      "63 ['doyouseethekitty', 'seethedoggy', 'doyoulikethekitty', 'likethedoggy']\n",
      "60 ['doyouseethe', 'kittyse', 'e', 'thedoggy', 'do', 'youlikethekittylike', 'thedoggy']\n",
      "60 ['doyouseethe', 'kittyse', 'e', 'thedoggy', 'do', 'youlikethekittylike', 'thedoggy']\n",
      "60 ['doyouseethe', 'kittyse', 'e', 'thedoggy', 'do', 'youlikethekittylike', 'thedoggy']\n",
      "60 ['doyouseethe', 'kittyse', 'e', 'thedoggy', 'do', 'youlikethekittylike', 'thedoggy']\n",
      "58 ['doyou', 's', 'e', 'ethe', 'kittyse', 'e', 'thedoggy', 'doyou', 'likethekittylike', 'thedoggy']\n",
      "56 ['doyou', 'se', 'ethe', 'kittysee', 'thedoggy', 'doyou', 'likethekittylike', 'thedoggy']\n",
      "53 ['doyou', 'seethe', 'kittysee', 'thedoggy', 'doyou', 'like', 'thekitty', 'like', 'thedoggy']\n",
      "51 ['doyou', 'see', 'the', 'kitty', 'se', 'e', 'thedoggy', 'doyou', 'like', 'the', 'kitty', 'like', 'thedoggy']\n",
      "50 ['doyou', 'see', 'the', 'kitty', 'see', 'the', 'doggy', 'doyou', 'like', 'the', 'kitty', 'like', 'th', 'e', 'doggy']\n",
      "44 ['doyou', 'see', 'the', 'kitty', 'see', 'the', 'doggy', 'doyou', 'like', 'the', 'kitty', 'like', 'the', 'doggy']\n",
      "44 ['doyou', 'see', 'the', 'kitty', 'see', 'the', 'doggy', 'doyou', 'like', 'the', 'kitty', 'like', 'the', 'doggy']\n",
      "44 ['doyou', 'see', 'the', 'kitty', 'see', 'the', 'doggy', 'doyou', 'like', 'the', 'kitty', 'like', 'the', 'doggy']\n",
      "44 ['doyou', 'see', 'the', 'kitty', 'see', 'the', 'doggy', 'doyou', 'like', 'the', 'kitty', 'like', 'the', 'doggy']\n",
      "44 ['doyou', 'see', 'the', 'kitty', 'see', 'the', 'doggy', 'doyou', 'like', 'the', 'kitty', 'like', 'the', 'doggy']\n",
      "44 ['doyou', 'see', 'the', 'kitty', 'see', 'the', 'doggy', 'doyou', 'like', 'the', 'kitty', 'like', 'the', 'doggy']\n",
      "44 ['doyou', 'see', 'the', 'kitty', 'see', 'the', 'doggy', 'doyou', 'like', 'the', 'kitty', 'like', 'the', 'doggy']\n",
      "44 ['doyou', 'see', 'the', 'kitty', 'see', 'the', 'doggy', 'doyou', 'like', 'the', 'kitty', 'like', 'the', 'doggy']\n",
      "44 ['doyou', 'see', 'the', 'kitty', 'see', 'the', 'doggy', 'doyou', 'like', 'the', 'kitty', 'like', 'the', 'doggy']\n",
      "44 ['doyou', 'see', 'the', 'kitty', 'see', 'the', 'doggy', 'doyou', 'like', 'the', 'kitty', 'like', 'the', 'doggy']\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'0000100100100001001001000010000100010010000100010010000'"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"doyouseethekittyseethedoggydoyoulikethekittylikethedoggy\"\n",
    "seg1 = \"0000000000000001000000000010000000000000000100000000000\"\n",
    "anneal(text, seg1, 5000, 1.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "23c847e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We called him Tortoise because he taught us .\n",
      "We;called;him;Tortoise;because;he;taught;us;.\n",
      "WecalledhimTortoisebecausehetaughtus.\n"
     ]
    }
   ],
   "source": [
    "silly = ['We', 'called', 'him', 'Tortoise', 'because', 'he', 'taught', 'us', '.']\n",
    "print(' '.join(silly))\n",
    "print(';'.join(silly))\n",
    "print(''.join(silly))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "61af1308",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dog -> 4 ;\n",
      "cat -> 3 ;\n",
      "snake -> 1 ;\n"
     ]
    }
   ],
   "source": [
    "fdist = nltk.FreqDist(['dog', 'cat', 'dog', 'cat', 'dog', 'snake', 'dog', 'cat'])\n",
    "for word in fdist:\n",
    "    print(word, '->', fdist[word], ';',)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "4adad379",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dog->4;\n",
      "cat->3;\n",
      "snake->1;\n"
     ]
    }
   ],
   "source": [
    "for word in fdist:\n",
    "    print('%s->%d;' % (word, fdist[word]),)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "b929e68c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lee wants a sandwich right now\n",
      "Lee wants a spam fritter right now\n",
      "Lee wants a pancake right now\n"
     ]
    }
   ],
   "source": [
    "template = 'Lee wants a %s right now'\n",
    "menu = ['sandwich', 'spam fritter', 'pancake']\n",
    "for snack in menu:\n",
    "    print(template % snack)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "3424c29c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'accuracy for 9375 words: 34.1867%'"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count, total = 3205, 9375\n",
    "\"accuracy for %d words: %2.4f%%\" % (total, 100 * count / total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "fb863fec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tabulate(cfdist, words, categories):\n",
    "    print('%-16s' % 'Category',)\n",
    "    for word in words:                                  # column headings\n",
    "        print('%6s' % word,)\n",
    "    print()\n",
    "    for category in categories:\n",
    "        print('%-16s' % category,)                      # row heading\n",
    "        for word in words:                              # for each word\n",
    "            print('%6d' % cfdist[category][word],)      # print table cell\n",
    "        print()                                         # end the row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "61b9d4fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Category        \n",
      "   can\n",
      " could\n",
      "   may\n",
      " might\n",
      "  must\n",
      "  will\n",
      "\n",
      "news            \n",
      "    93\n",
      "    86\n",
      "    66\n",
      "    38\n",
      "    50\n",
      "   389\n",
      "\n",
      "religion        \n",
      "    82\n",
      "    59\n",
      "    78\n",
      "    12\n",
      "    54\n",
      "    71\n",
      "\n",
      "hobbies         \n",
      "   268\n",
      "    58\n",
      "   131\n",
      "    22\n",
      "    83\n",
      "   264\n",
      "\n",
      "science_fiction \n",
      "    16\n",
      "    49\n",
      "     4\n",
      "    12\n",
      "     8\n",
      "    16\n",
      "\n",
      "romance         \n",
      "    74\n",
      "   193\n",
      "    11\n",
      "    51\n",
      "    45\n",
      "    43\n",
      "\n",
      "humor           \n",
      "    16\n",
      "    30\n",
      "     8\n",
      "     8\n",
      "     9\n",
      "    13\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import brown\n",
    "cfd = nltk.ConditionalFreqDist(\n",
    "(genre, word)\n",
    "for genre in brown.categories()\n",
    "for word in brown.words(categories=genre))\n",
    "\n",
    "genres = ['news', 'religion', 'hobbies', 'science_fiction', 'romance', 'humor']\n",
    "modals = ['can', 'could', 'may', 'might', 'must', 'will']\n",
    "tabulate(cfd, modals, genres)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "72b83038",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_file = open('output.txt', 'w')\n",
    "words = set(nltk.corpus.genesis.words('english-kjv.txt'))\n",
    "for word in sorted(words):\n",
    "    output_file.write(word + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "a1a3b061",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(words)\n",
    "2789\n",
    "str(len(words))\n",
    "output_file.write(str(len(words)) + \"\\n\")\n",
    "output_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "60538221",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After (5),\n",
      "all (3),\n",
      "is (2),\n",
      "said (4),\n",
      "and (3),\n",
      "done (4),\n",
      ", (1),\n",
      "more (4),\n",
      "is (2),\n",
      "said (4),\n",
      "than (4),\n",
      "done (4),\n",
      ". (1),\n"
     ]
    }
   ],
   "source": [
    "saying = ['After', 'all', 'is', 'said', 'and', 'done', ',','more', 'is', 'said', 'than', 'done', '.']\n",
    "for word in saying:\n",
    "    print(word, '(' + str(len(word)) + '),',)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "b3eebbc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After (5), all (3), is (2), said (4), and (3), done (4), , (1), more\n",
      "(4), is (2), said (4), than (4), done (4), . (1),\n"
     ]
    }
   ],
   "source": [
    "from textwrap import fill\n",
    "format = '%s (%d),'\n",
    "pieces = [format % (word, len(word)) for word in saying]\n",
    "output = ' '.join(pieces)\n",
    "wrapped = fill(output)\n",
    "print(wrapped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80d9e21b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5c8673d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b46d47f7",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "○ Define a string s = 'colorless'. Write a Python statement that changes this to “colourless” using only the slice and concatenation operations.\n",
    "\n",
    "○ We can use the slice notation to remove morphological endings on words. For example, 'dogs'[:-1] removes the last character of dogs, leaving dog. Use slice notation to remove the affixes from these words (we’ve inserted a hyphen to indicate the affix boundary, but omit this from your strings): dish-es, run-ning, nation-ality, un-do, pre-heat.\n",
    "\n",
    "○ We saw how we can generate an IndexError by indexing beyond the end of a string. Is it possible to construct an index that goes too far to the left, before the start of the string?\n",
    "\n",
    "○ We can specify a “step” size for the slice. The following returns every second character within the slice: monty[6:11:2]. It also works in the reverse direction: monty[10:5:-2]. Try these for yourself, and then experiment with different step values.\n",
    "\n",
    "○ What happens if you ask the interpreter to evaluate monty[::-1]? Explain why this is a reasonable result.\n",
    "\n",
    "○ Describe the class of strings matched by the following regular expressions:\n",
    "<ul>\n",
    "<li>[a-zA-Z]+</li>\n",
    "\n",
    "<li>[A-Z][a-z]*</li>\n",
    "\n",
    "<li>p[aeiou]{,2}t</li>\n",
    "\n",
    "<li>\\d+(\\.\\d+)?</li>\n",
    "\n",
    "<li>([^aeiou][aeiou][^aeiou])*</li>\n",
    "\n",
    "<li>\\w+|[^\\w\\s]+</li>\n",
    "</ul>\n",
    "Test your answers using nltk.re_show().\n",
    "\n",
    "○ Write regular expressions to match the following classes of strings:\n",
    "\n",
    "A single determiner (assume that a, an, and the are the only determiners)\n",
    "\n",
    "An arithmetic expression using integers, addition, and multiplication, such as 2*3+8\n",
    "\n",
    "○ Write a utility function that takes a URL as its argument, and returns the contents of the URL, with all HTML markup removed. Use urllib.urlopen to access the contents of the URL, e.g.:\n",
    "\n",
    "raw_contents = urllib.urlopen('http://www.nltk.org/').read()\n",
    "○ Save some text into a file corpus.txt. Define a function load(f) that reads from the file named in its sole argument, and returns a string containing the text of the file.\n",
    "\n",
    "Use nltk.regexp_tokenize() to create a tokenizer that tokenizes the various kinds of punctuation in this text. Use one multiline regular expression inline comments, using the verbose flag (?x).\n",
    "\n",
    "Use nltk.regexp_tokenize() to create a tokenizer that tokenizes the following kinds of expressions: monetary amounts; dates; names of people and organizations.\n",
    "\n",
    "○ Rewrite the following loop as a list comprehension:\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    ">>> sent = ['The', 'dog', 'gave', 'John', 'the', 'newspaper']<br>\n",
    ">>> result = []<br>\n",
    ">>> for word in sent:<br>\n",
    "...     word_len = (word, len(word))<br>\n",
    "...     result.append(word_len)<br>\n",
    ">>> result<br>\n",
    "[('The', 3), ('dog', 3), ('gave', 4), ('John', 4), ('the', 3), ('newspaper', 9)]\n",
    "</div>    \n",
    "    \n",
    "○ Define a string raw containing a sentence of your own choosing. Now, split raw on some character other than space, such as 's'.\n",
    "\n",
    "○ Write a for loop to print out the characters of a string, one per line.\n",
    "\n",
    "○ What is the difference between calling split on a string with no argument and one with ' ' as the argument, e.g., sent.split() versus sent.split(' ')? What happens when the string being split contains tab characters, consecutive space characters, or a sequence of tabs and spaces? (In IDLE you will need to use '\\t' to enter a tab character.)\n",
    "\n",
    "○ Create a variable words containing a list of words. Experiment with words.sort() and sorted(words). What is the difference?\n",
    "\n",
    "○ Explore the difference between strings and integers by typing the following at a Python prompt: \"3\" * 7 and 3 * 7. Try converting between strings and integers using int(\"3\") and str(3).\n",
    "\n",
    "○ Earlier, we asked you to use a text editor to create a file called test.py, containing the single line monty = 'Monty Python'. If you haven’t already done this (or can’t find the file), go ahead and do it now. Next, start up a new session with the Python interpreter, and enter the expression monty at the prompt. You will get an error from the interpreter. Now, try the following (note that you have to leave off the .py part of the filename):\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    ">>> from test import msg<br>\n",
    ">>> msg<br>\n",
    "</div>    \n",
    "    \n",
    "This time, Python should return with a value. You can also try import test, in which case Python should be able to evaluate the expression test.monty at the prompt.\n",
    "\n",
    "○ What happens when the formatting strings %6s and %-6s are used to display strings that are longer than six characters?\n",
    "\n",
    " Read in some text from a corpus, tokenize it, and print the list of all wh-word types that occur. (wh-words in English are used in questions, relative clauses, and exclamations: who, which, what, and so on.) Print them in order. Are any words duplicated in this list, because of the presence of case distinctions or punctuation?\n",
    "\n",
    " Create a file consisting of words and (made up) frequencies, where each line consists of a word, the space character, and a positive integer, e.g., fuzzy 53. Read the file into a Python list using open(filename).readlines(). Next, break each line into its two fields using split(), and convert the number into an integer using int(). The result should be a list of the form: [['fuzzy', 53], ...].\n",
    "\n",
    " Write code to access a favorite web page and extract some text from it. For example, access a weather site and extract the forecast top temperature for your town or city today.\n",
    "\n",
    " Write a function unknown() that takes a URL as its argument, and returns a list of unknown words that occur on that web page. In order to do this, extract all substrings consisting of lowercase letters (using re.findall()) and remove any items from this set that occur in the Words Corpus (nltk.corpus.words). Try to categorize these words manually and discuss your findings.\n",
    "\n",
    " Examine the results of processing the URL http://news.bbc.co.uk/ using the regular expressions suggested above. You will see that there is still a fair amount of non-textual data there, particularly JavaScript commands. You may also find that sentence breaks have not been properly preserved. Define further regular expressions that improve the extraction of text from this web page.\n",
    "\n",
    " Are you able to write a regular expression to tokenize text in such a way that the word don’t is tokenized into do and n’t? Explain why this regular expression won’t work: «n't|\\w+».\n",
    "\n",
    " Try to write code to convert text into hAck3r, using regular expressions and substitution, where e → 3, i → 1, o → 0, l → |, s → 5, . → 5w33t!, ate → 8. Normalize the text to lowercase before converting it. Add more substitutions of your own. Now try to map s to two different values: $ for word-initial s, and 5 for word-internal s.\n",
    "\n",
    " Pig Latin is a simple transformation of English text. Each word of the text is converted as follows: move any consonant (or consonant cluster) that appears at the start of the word to the end, then append ay, e.g., string → ingstray, idle → idleay (see http://en.wikipedia.org/wiki/Pig_Latin).\n",
    "\n",
    "Write a function to convert a word to Pig Latin.\n",
    "\n",
    "Write code that converts text, instead of individual words.\n",
    "\n",
    "Extend it further to preserve capitalization, to keep qu together (so that quiet becomes ietquay, for example), and to detect when y is used as a consonant (e.g., yellow) versus a vowel (e.g., style).\n",
    "\n",
    " Download some text from a language that has vowel harmony (e.g., Hungarian), extract the vowel sequences of words, and create a vowel bigram table.\n",
    "\n",
    " Python’s random module includes a function choice() which randomly chooses an item from a sequence; e.g., choice(\"aehh \") will produce one of four possible characters, with the letter h being twice as frequent as the others. Write a generator expression that produces a sequence of 500 randomly chosen letters drawn from the string \"aehh \", and put this expression inside a call to the ''.join() function, to concatenate them into one long string. You should get a result that looks like uncontrolled sneezing or maniacal laughter: he haha ee heheeh eha. Use split() and join() again to normalize the whitespace in this string.\n",
    "\n",
    " Consider the numeric expressions in the following sentence from the MedLine Corpus: The corresponding free cortisol fractions in these sera were 4.53 +/- 0.15% and 8.16 +/- 0.23%, respectively. Should we say that the numeric expression 4.53 +/- 0.15% is three words? Or should we say that it’s a single compound word? Or should we say that it is actually nine words, since it’s read “four point five three, plus or minus fifteen percent”? Or should we say that it’s not a “real” word at all, since it wouldn’t appear in any dictionary? Discuss these different possibilities. Can you think of application domains that motivate at least two of these answers?\n",
    "\n",
    " Readability measures are used to score the reading difficulty of a text, for the purposes of selecting texts of appropriate difficulty for language learners. Let us define μw to be the average number of letters per word, and μs to be the average number of words per sentence, in a given text. The Automated Readability Index (ARI) of the text is defined to be: 4.71 μw + 0.5 μs - 21.43. Compute the ARI score for various sections of the Brown Corpus, including section f (popular lore) and j (learned). Make use of the fact that nltk.corpus.brown.words() produces a sequence of words, whereas nltk.corpus.brown.sents() produces a sequence of sentences.\n",
    "\n",
    " Use the Porter Stemmer to normalize some tokenized text, calling the stemmer on each word. Do the same thing with the Lancaster Stemmer, and see if you observe any differences.\n",
    "\n",
    " Define the variable saying to contain the list ['After', 'all', 'is', 'said', 'and', 'done', ',', 'more', 'is', 'said', 'than', 'done', '.']. Process the list using a for loop, and store the result in a new list lengths. Hint: begin by assigning the empty list to lengths, using lengths = []. Then each time through the loop, use append() to add another length value to the list.\n",
    "\n",
    " Define a variable silly to contain the string: 'newly formed bland ideas are inexpressible in an infuriating way'. (This happens to be the legitimate interpretation that bilingual English-Spanish speakers can assign to Chomsky’s famous nonsense phrase colorless green ideas sleep furiously, according to Wikipedia). Now write code to perform the following tasks:\n",
    "\n",
    "Split silly into a list of strings, one per word, using Python’s split() operation, and save this to a variable called bland.\n",
    "\n",
    "Extract the second letter of each word in silly and join them into a string, to get 'eoldrnnnna'.\n",
    "\n",
    "Combine the words in bland back into a single string, using join(). Make sure the words in the resulting string are separated with whitespace.\n",
    "\n",
    "Print the words of silly in alphabetical order, one per line.\n",
    "\n",
    " The index() function can be used to look up items in sequences. For example, 'inexpressible'.index('e') tells us the index of the first position of the letter e.\n",
    "\n",
    "What happens when you look up a substring, e.g., 'inexpressible'.index('re')?\n",
    "\n",
    "Define a variable words containing a list of words. Now use words.index() to look up the position of an individual word.\n",
    "\n",
    "Define a variable silly as in Exercise 32. Use the index() function in combination with list slicing to build a list phrase consisting of all the words up to (but not including) in in silly.\n",
    "\n",
    " Write code to convert nationality adjectives such as Canadian and Australian to their corresponding nouns Canada and Australia (see http://en.wikipedia.org/wiki/List_of_adjectival_forms_of_place_names).\n",
    "\n",
    " Read the LanguageLog post on phrases of the form as best as p can and as best p can, where p is a pronoun. Investigate this phenomenon with the help of a corpus and the findall() method for searching tokenized text described in Useful Applications of Regular Expressions. The post is at http://itre.cis.upenn.edu/~myl/languagelog/archives/002733.html.\n",
    "\n",
    " Study the lolcat version of the book of Genesis, accessible as nltk.corpus.genesis.words('lolcat.txt'), and the rules for converting text into lolspeak at http://www.lolcatbible.com/index.php?title=How_to_speak_lolcat. Define regular expressions to convert English words into corresponding lolspeak words.\n",
    "\n",
    " Read about the re.sub() function for string substitution using regular expressions, using help(re.sub) and by consulting the further readings for this chapter. Use re.sub in writing code to remove HTML tags from an HTML file, and to normalize whitespace.\n",
    "\n",
    "● An interesting challenge for tokenization is words that have been split across a linebreak. E.g., if long-term is split, then we have the string long-\\nterm.\n",
    "\n",
    "Write a regular expression that identifies words that are hyphenated at a line-break. The expression will need to include the \\n character.\n",
    "\n",
    "Use re.sub() to remove the \\n character from these words.\n",
    "\n",
    "How might you identify words that should not remain hyphenated once the newline is removed, e.g., 'encyclo-\\npedia'?\n",
    "\n",
    "● Read the Wikipedia entry on Soundex. Implement this algorithm in Python.\n",
    "\n",
    "● Obtain raw texts from two or more genres and compute their respective reading difficulty scores as in the earlier exercise on reading difficulty. E.g., compare ABC Rural News and ABC Science News (nltk.corpus.abc). Use Punkt to perform sentence segmentation.\n",
    "\n",
    "● Rewrite the following nested loop as a nested list comprehension:\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    ">>> words = ['attribution', 'confabulation', 'elocution',<br>\n",
    "...          'sequoia', 'tenacious', 'unidirectional']<br>\n",
    ">>> vsequences = set()<br>\n",
    ">>> for word in words:<br>\n",
    "...     vowels = []<br>\n",
    "...     for char in word:<br>\n",
    "...         if char in 'aeiou':<br>\n",
    "...             vowels.append(char)<br>\n",
    "...     vsequences.add(''.join(vowels))<br>\n",
    ">>> sorted(vsequences)<br>\n",
    "['aiuio', 'eaiou', 'eouio', 'euoia', 'oauaio', 'uiieioa']\n",
    "</div>\n",
    "\n",
    "● Use WordNet to create a semantic index for a text collection. Extend the concordance search program in Example 3-1, indexing each word using the offset of its first synset, e.g., wn.synsets('dog')[0].offset (and optionally the offset of some of its ancestors in the hypernym hierarchy).\n",
    "\n",
    "● With the help of a multilingual corpus such as the Universal Declaration of Human Rights Corpus (nltk.corpus.udhr), along with NLTK’s frequency distribution and rank correlation functionality (nltk.FreqDist, nltk.spearman_correlation), develop a system that guesses the language of a previously unseen text. For simplicity, work with a single character encoding and just a few languages.\n",
    "\n",
    "● Write a program that processes a text and discovers cases where a word has been used with a novel sense. For each word, compute the WordNet similarity between all synsets of the word and all synsets of the words in its context. (Note that this is a crude approach; doing it well is a difficult, open research problem.)\n",
    "\n",
    "● Read the article on normalization of non-standard words (Sproat et al., 2001), and implement a similar system for text normalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca45165a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
