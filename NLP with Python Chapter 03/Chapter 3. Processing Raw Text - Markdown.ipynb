{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "66fce456",
   "metadata": {},
   "source": [
    "# Chapter 3. Processing Raw Text\n",
    "\n",
    "The most important source of texts is undoubtedly the Web. It’s convenient to have existing text collections to explore, such as the corpora we saw in the previous chapters. However, you probably have your own text sources in mind, and need to learn how to access them.\n",
    "\n",
    "The goal of this chapter is to answer the following questions:\n",
    "\n",
    "<ol>\n",
    "<li>How can we write programs to access text from local files and from the Web, in order to get hold of an unlimited range of language material?</li>\n",
    "\n",
    "<li>How can we split documents up into individual words and punctuation symbols, so we can carry out the same kinds of analysis we did with text corpora in earlier chapters?</li> \n",
    "\n",
    "<li>How can we write programs to produce formatted output and save it in a file?</li>\n",
    "</ol>\n",
    "\n",
    "In order to address these questions, we will be covering key concepts in NLP, including tokenization and stemming. Along the way you will consolidate your Python knowledge and learn about strings, files, and regular expressions. Since so much text on the Web is in HTML format, we will also see how to dispense with markup.\n",
    "\n",
    "***\n",
    "    NOTE\n",
    "        Important: From this chapter onwards, our program samples will assume you begin your\n",
    "        interactive session or your program with the following import statements:\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "    >>> from __future__ import division<br>\n",
    "    >>> import nltk, re, pprint\n",
    "</div>\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50b924b1",
   "metadata": {},
   "source": [
    "## Accessing Text from the Web and from Disk\n",
    "\n",
    "### Electronic Books\n",
    "\n",
    "A small sample of texts from Project Gutenberg appears in the NLTK corpus collection. However, you may be interested in analyzing other texts from Project Gutenberg. You can browse the catalog of 25,000 free online books at http://www.gutenberg.org/catalog/, and obtain a URL to an ASCII text file. Although 90% of the texts in Project Gutenberg are in English, it includes material in over 50 other languages, including Catalan, Chinese, Dutch, Finnish, French, German, Italian, Portuguese, and Spanish (with more than 100 texts each).\n",
    "\n",
    "Text number 2554 is an English translation of Crime and Punishment, and we can access it as follows.\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    ">>> from urllib import urlopen<br>\n",
    ">>> url = \"http://www.gutenberg.org/files/2554/2554.txt\"<br>\n",
    ">>> raw = urlopen(url).read()<br>\n",
    ">>> type(raw)<br>\n",
    "<type 'str'><br>\n",
    ">>> len(raw)<br>\n",
    "1176831<br>\n",
    ">>> raw[:75]<br>\n",
    "'The Project Gutenberg EBook of Crime and Punishment, by Fyodor Dostoevsky\\r\\n'\n",
    "</div>\n",
    "    \n",
    "***    \n",
    "    NOTE\n",
    "        The read( ) process will take a few seconds as it downloads this large book. \n",
    "        If you’re using an Internet proxy that is not correctly detected by Python, you may need to specify the proxy manually as follows:\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">    \n",
    ">>> proxies = {'http': 'http://www.someproxy.com:3128'}<br>\n",
    ">>> raw = urlopen(url, proxies=proxies).read( )<br>\n",
    "</div> \n",
    "    \n",
    "***\n",
    "    \n",
    "The variable raw contains a string with 1,176,831 characters. (We can see that it is a string, using type(raw).) This is the raw content of the book, including many details we are not interested in, such as whitespace, line breaks, and blank lines. Notice the \\r and \\n in the opening line of the file, which is how Python displays the special carriage return and line-feed characters (the file must have been created on a Windows machine). For our language processing, we want to break up the string into words and punctuation, as we saw in Chapter 1. This step is called tokenization, and it produces our familiar structure, a list of words and punctuation.\n",
    "\n",
    "  \n",
    "<div class=\"alert alert-block alert-info\">    \n",
    ">>> tokens = nltk.word_tokenize(raw)<br>\n",
    ">>> type(tokens)<br>\n",
    "<type 'list'><br>\n",
    ">>> len(tokens)<br>\n",
    "255809<br>\n",
    ">>> tokens[:10]<br>\n",
    "['The', 'Project', 'Gutenberg', 'EBook', 'of', 'Crime', 'and', 'Punishment', ',', 'by']\n",
    "</div>    \n",
    "    \n",
    "Notice that NLTK was needed for tokenization, but not for any of the earlier tasks of opening a URL and reading it into a string. If we now take the further step of creating an NLTK text from this list, we can carry out all of the other linguistic processing we saw in Chapter 1, along with the regular list operations, such as slicing:\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">    \n",
    ">>> text = nltk.Text(tokens)<br>\n",
    ">>> type(text)<br>\n",
    "<type 'nltk.text.Text'><br>\n",
    ">>> text[1020:1060]<br>\n",
    "['CHAPTER', 'I', 'On', 'an', 'exceptionally', 'hot', 'evening', 'early', 'in',\n",
    "'July', 'a', 'young', 'man', 'came', 'out', 'of', 'the', 'garret', 'in',<br>\n",
    "'which', 'he', 'lodged', 'in', 'S', '.', 'Place', 'and', 'walked', 'slowly',\n",
    "',', 'as', 'though', 'in', 'hesitation', ',', 'towards', 'K', '.', 'bridge', '.']<br>\n",
    ">>> text.collocations()<br>\n",
    "Katerina Ivanovna; Pulcheria Alexandrovna; Avdotya Romanovna; Pyotr\n",
    "Petrovitch; Project Gutenberg; Marfa Petrovna; Rodion Romanovitch;<br>\n",
    "Sofya Semyonovna; Nikodim Fomitch; did not; Hay Market; Andrey\n",
    "Semyonovitch; old woman; Literary Archive; Dmitri Prokofitch; great\n",
    "deal; United States; Praskovya Pavlovna; Porfiry Petrovitch; ear rings\n",
    "</div>    \n",
    "    \n",
    "Notice that Project Gutenberg appears as a collocation. This is because each text downloaded from Project Gutenberg contains a header with the name of the text, the author, the names of people who scanned and corrected the text, a license, and so on. Sometimes this information appears in a footer at the end of the file. We cannot reliably detect where the content begins and ends, and so have to resort to manual inspection of the file, to discover unique strings that mark the beginning and the end, before trimming raw to be just the content and nothing else:\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">    \n",
    ">>> raw.find(\"PART I\")<br>\n",
    "5303<br>\n",
    ">>> raw.rfind(\"End of Project Gutenberg's Crime\")<br>\n",
    "1157681<br>\n",
    ">>> raw = raw[5303:1157681] <br>\n",
    ">>> raw.find(\"PART I\") <br>\n",
    "0\n",
    "</div>    \n",
    "    \n",
    "The find() and rfind() (“reverse find”) methods help us get the right index values to use for slicing the string . We overwrite raw with this slice, so now it begins with “PART I” and goes up to (but not including) the phrase that marks the end of the content.\n",
    "\n",
    "This was our first brush with the reality of the Web: texts found on the Web may contain unwanted material, and there may not be an automatic way to remove it. But with a small amount of extra work we can extract the material we need."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4337d6e2",
   "metadata": {},
   "source": [
    "### Dealing with HTML\n",
    "\n",
    "Much of the text on the Web is in the form of HTML documents. You can use a web browser to save a page as text to a local file, then access this as described in the later section on files. However, if you’re going to do this often, it’s easiest to get Python to do the work directly. The first step is the same as before, using urlopen. For fun we’ll pick a BBC News story called “Blondes to die out in 200 years,” an urban legend passed along by the BBC as established scientific fact:\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    ">>> url = \"http://news.bbc.co.uk/2/hi/health/2284783.stm\"<br>\n",
    ">>> html = urlopen(url).read()<br>\n",
    ">>> html[:60]<br>\n",
    "  <!doctype html public \"-//W3C//DTD HTML 4.0 Transitional//EN>\n",
    "</div>\n",
    "\n",
    "You can type print html to see the HTML content in all its glory, including meta tags, an image map, JavaScript, forms, and tables.\n",
    "\n",
    "Getting text out of HTML is a sufficiently common task that NLTK provides a helper function nltk.clean_html(), which takes an HTML string and returns raw text. We can then tokenize this to get our familiar text structure:\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    ">>> raw = nltk.clean_html(html)<br>\n",
    ">>> tokens = nltk.word_tokenize(raw)<br>\n",
    ">>> tokens<br>\n",
    "['BBC', 'NEWS', '|', 'Health', '|', 'Blondes', \"'\", 'to', 'die', 'out', ...]\n",
    "</div>\n",
    "\n",
    "This still contains unwanted material concerning site navigation and related stories. With some trial and error you can find the start and end indexes of the content and select the tokens of interest, and initialize a text as before.\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">    \n",
    ">>> tokens = tokens[96:399]<br>\n",
    ">>> text = nltk.Text(tokens)<br>\n",
    ">>> text.concordance('gene')<br>\n",
    " they say too few people now carry the <b>gene</b> for blondes to last beyond the next tw<br>\n",
    "t blonde hair is caused by a recessive <b>gene</b> . In order for a child to have blonde<br>\n",
    "to have blonde hair , it must have the <b>gene</b> on both sides of the family in the gra<br>\n",
    "there is a disadvantage of having that <b>gene</b> or by chance . They don ' t disappear<br>\n",
    "ondes would disappear is if having the <b>gene</b> was a disadvantage and I do not think\n",
    "</div>   \n",
    "    \n",
    "***    \n",
    "    NOTE\n",
    "\n",
    "        For more sophisticated processing of HTML, use the Beautiful Soup package, available at\n",
    "        http://www.crummy.com/software/BeautifulSoup/.\n",
    "***    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67fc7640",
   "metadata": {},
   "source": [
    "### Processing Search Engine Results\n",
    "\n",
    "The Web can be thought of as a huge corpus of unannotated text. Web search engines provide an efficient means of searching this large quantity of text for relevant linguistic examples. The main advantage of search engines is size: since you are searching such a large set of documents, you are more likely to find any linguistic pattern you are interested in. Furthermore, you can make use of very specific patterns, which would match only one or two examples on a smaller example, but which might match tens of thousands of examples when run on the Web. A second advantage of web search engines is that they are very easy to use. Thus, they provide a very convenient tool for quickly checking a theory, to see if it is reasonable. See Table 3-1 for an example.\n",
    "\n",
    "Table 3-1. Google hits for collocations: The number of hits for collocations involving the words absolutely or definitely, followed by one of adore, love, like, or prefer. (Liberman, in LanguageLog, 2005)\n",
    "\n",
    "![table 3-1](googColl.png)\n",
    "\n",
    "Unfortunately, search engines have some significant shortcomings. First, the allowable range of search patterns is severely restricted. Unlike local corpora, where you write programs to search for arbitrarily complex patterns, search engines generally only allow you to search for individual words or strings of words, sometimes with wildcards. Second, search engines give inconsistent results, and can give widely different figures when used at different times or in different geographical regions. When content has been duplicated across multiple sites, search results may be boosted. Finally, the markup in the result returned by a search engine may change unpredictably, breaking any pattern-based method of locating particular content (a problem which is ameliorated by the use of search engine APIs).\n",
    "\n",
    "***\n",
    "    NOTE\n",
    "\n",
    "        Your Turn: Search the Web for \"the of\" (inside quotes). \n",
    "        Based on the large count, can we conclude that the of is a frequent collocation in English?\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08a2c13e",
   "metadata": {},
   "source": [
    "### Processing RSS Feeds\n",
    "\n",
    "The blogosphere is an important source of text, in both formal and informal registers. With the help of a third-party Python library called the Universal Feed Parser, freely downloadable from http://feedparser.org/, we can access the content of a blog, as shown here:\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    ">>> import feedparser<br>\n",
    ">>> llog = feedparser.parse(\"http://languagelog.ldc.upenn.edu/nll/?feed=atom\")<br>\n",
    ">>> llog['feed']['title']<br>\n",
    "u'Language Log'<br>\n",
    ">>> len(llog.entries)<br>\n",
    "15<br>\n",
    ">>> post = llog.entries[2]<br>\n",
    ">>> post.title<br>\n",
    "u\"He's My BF\"<br>\n",
    ">>> content = post.content[0].value<br>\n",
    ">>> content[:70]<br>\n",
    "u'<p>Today I was chatting with three of our visiting graduate students f'<br>\n",
    ">>> nltk.word_tokenize(nltk.html_clean(content))<br>\n",
    ">>> nltk.word_tokenize(nltk.clean_html(llog.entries[2].content[0].value))<br>\n",
    "[u'Today', u'I', u'was', u'chatting', u'with', u'three', u'of', u'our', u'visiting',<br>\n",
    "u'graduate', u'students', u'from', u'the', u'PRC', u'.', u'Thinking', u'that', u'I',<br>\n",
    "u'was', u'being', u'au', u'courant', u',', u'I', u'mentioned', u'the', u'expression',<br>\n",
    "u'DUI4XIANG4', u'\\u5c0d\\u8c61', u'(\"', u'boy', u'/', u'girl', u'friend', u'\"', ...]\n",
    "</div>\n",
    "\n",
    "Note that the resulting strings have a u prefix to indicate that they are Unicode strings (see Text Processing with Unicode). With some further work, we can write programs to create a small corpus of blog posts, and use this as the basis for our NLP work."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85c69a8c",
   "metadata": {},
   "source": [
    "### Reading Local Files\n",
    "\n",
    "In order to read a local file, we need to use Python’s built-in open() function, followed by the read() method. Supposing you have a file document.txt, you can load its contents like this:\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    ">>> f = open('document.txt')\n",
    ">>> raw = f.read()\n",
    "</div> \n",
    "    \n",
    "***    \n",
    "NOTE\n",
    "    \n",
    "    Your Turn: Create a file called document.txt using a text editor, and type in a few lines of text, and save it as plain text. If you are using IDLE, select the New Window command in the File menu, typing the required text into this window, and then saving the file as document.txt inside the directory that IDLE offers in the pop-up dialogue box. Next, in the Python interpreter, open the file using f = open('document.txt'), then inspect its contents using print f.read().\n",
    "***\n",
    "\n",
    "Various things might have gone wrong when you tried this. If the interpreter couldn’t find your file, you would have seen an error like this:\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    ">>> f = open('document.txt')\n",
    "Traceback (most recent call last):\n",
    "File \"<pyshell#7>\", line 1, in -toplevel-\n",
    "f = open('document.txt')\n",
    "IOError: [Errno 2] No such file or directory: 'document.txt'\n",
    "</div>\n",
    "\n",
    "To check that the file that you are trying to open is really in the right directory, use IDLE’s Open command in the File menu; this will display a list of all the files in the directory where IDLE is running. An alternative is to examine the current directory from within Python:\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    ">>> import os\n",
    ">>> os.listdir('.')\n",
    "</div>\n",
    "\n",
    "Another possible problem you might have encountered when accessing a text file is the newline conventions, which are different for different operating systems. The built-in open() function has a second parameter for controlling how the file is opened: open('document.txt', 'rU'). 'r' means to open the file for reading (the default), and 'U' stands for “Universal”, which lets us ignore the different conventions used for marking newlines.\n",
    "\n",
    "Assuming that you can open the file, there are several methods for reading it. The read() method creates a string with the contents of the entire file:\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    ">>> f.read()\n",
    "'Time flies like an arrow.\\nFruit flies like a banana.\\n'\n",
    "</div>\n",
    "\n",
    "Recall that the '\\n' characters are newlines; this is equivalent to pressing Enter on a keyboard and starting a new line.\n",
    "\n",
    "We can also read a file one line at a time using a for loop:\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    ">>> f = open('document.txt', 'rU')\n",
    ">>> for line in f:\n",
    "...     print line.strip()\n",
    "Time flies like an arrow.\n",
    "Fruit flies like a banana.\n",
    "</div>\n",
    "\n",
    "Here we use the strip() method to remove the newline character at the end of the input line.\n",
    "\n",
    "NLTK’s corpus files can also be accessed using these methods. We simply have to use nltk.data.find() to get the filename for any corpus item. Then we can open and read it in the way we just demonstrated:\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    ">>> path = nltk.data.find('corpora/gutenberg/melville-moby_dick.txt')\n",
    ">>> raw = open(path, 'rU').read()\n",
    "</div>    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0f9858e",
   "metadata": {},
   "source": [
    "### Extracting Text from PDF, MSWord, and Other Binary Formats\n",
    "\n",
    "ASCII text and HTML text are human-readable formats. Text often comes in binary formats—such as PDF and MSWord—that can only be opened using specialized software. Third-party libraries such as pypdf and pywin32 provide access to these formats. Extracting text from multicolumn documents is particularly challenging. For one-off conversion of a few documents, it is simpler to open the document with a suitable application, then save it as text to your local drive, and access it as described below. If the document is already on the Web, you can enter its URL in Google’s search box. The search result often includes a link to an HTML version of the document, which you can save as text."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c85a524",
   "metadata": {},
   "source": [
    "### Capturing User Input\n",
    "Sometimes we want to capture the text that a user inputs when she is interacting with our program. To prompt the user to type a line of input, call the Python function raw_input(). After saving the input to a variable, we can manipulate it just as we have done for other strings.\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    ">>> s = raw_input(\"Enter some text: \")<br>\n",
    "Enter some text: On an exceptionally hot evening early in July<br>\n",
    ">>> print \"You typed\", len(nltk.word_tokenize(s)), \"words.\"<br>\n",
    "You typed 8 words.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3d1ec7f",
   "metadata": {},
   "source": [
    "### The NLP Pipeline\n",
    "Figure 3-1 summarizes what we have covered in this section, including the process of building a vocabulary that we saw in Chapter 1. (One step, normalization, will be discussed in Normalizing Text.)\n",
    "\n",
    "![figure 3-1](nlpPipeline.png)\n",
    "\n",
    "Figure 3-1. The processing pipeline: We open a URL and read its HTML content, remove the markup and select a slice of characters; this is then tokenized and optionally converted into an nltk.Text object; we can also lowercase all the words and extract the vocabulary.\n",
    "\n",
    "There’s a lot going on in this pipeline. To understand it properly, it helps to be clear about the type of each variable that it mentions. We find out the type of any Python object x using type(x); e.g., type(1) is <int> since 1 is an integer.\n",
    "\n",
    "When we load the contents of a URL or file, and when we strip out HTML markup, we are dealing with strings, Python’s <str> data type (we will learn more about strings in Strings: Text Processing at the Lowest Level):\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    ">>> raw = open('document.txt').read()<br>\n",
    ">>> type(raw)<br>\n",
    "<type 'str'>\n",
    "</div>\n",
    "    \n",
    "When we tokenize a string we produce a list (of words), and this is Python’s <list> type. Normalizing and sorting lists produces other lists:\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    ">>> tokens = nltk.word_tokenize(raw)<br>\n",
    ">>> type(tokens)<br>\n",
    "<type 'list'><br>\n",
    ">>> words = [w.lower() for w in tokens]<br>\n",
    ">>> type(words)<br>\n",
    "<type 'list'><br>\n",
    ">>> vocab = sorted(set(words))<br>\n",
    ">>> type(vocab)<br>\n",
    "<type 'list'>\n",
    "</div>\n",
    "    \n",
    "The type of an object determines what operations you can perform on it. So, for example, we can append to a list but not to a string:\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    ">>> vocab.append('blog')<br>\n",
    ">>> raw.append('blog')<br>\n",
    "Traceback (most recent call last):<br>\n",
    "  File \"<stdin>\", line 1, in <module><br>\n",
    "AttributeError: 'str' object has no attribute 'append'\n",
    "</div>    \n",
    "    \n",
    "Similarly, we can concatenate strings with strings, and lists with lists, but we cannot concatenate strings with lists:\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    ">>> query = 'Who knows?'<br>\n",
    ">>> beatles = ['john', 'paul', 'george', 'ringo']<br>\n",
    ">>> query + beatles<br>\n",
    "Traceback (most recent call last):<br>\n",
    "  File \"<stdin>\", line 1, in <module><br>\n",
    "TypeError: cannot concatenate 'str' and 'list' objects\n",
    "</div>\n",
    "    \n",
    "In the next section, we examine strings more closely and further explore the relationship between strings and lists."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25c0d151",
   "metadata": {},
   "source": [
    "## Strings: Text Processing at the Lowest Level\n",
    "\n",
    "It’s time to study a fundamental data type that we’ve been studiously avoiding so far. In earlier chapters we focused on a text as a list of words. We didn’t look too closely at words and how they are handled in the programming language. By using NLTK’s corpus interface we were able to ignore the files that these texts had come from. The contents of a word, and of a file, are represented by programming languages as a fundamental data type known as a string. In this section, we explore strings in detail, and show the connection between strings, words, texts, and files.\n",
    "\n",
    "### Basic Operations with Strings\n",
    "Strings are specified using single quotes 1 or double quotes 2, as shown in the following code example. If a string contains a single quote, we must backslash-escape the quote 3 so Python knows a literal quote character is intended, or else put the string in double quotes 2. Otherwise, the quote inside the string 4 will be interpreted as a close quote, and the Python interpreter will report a syntax error:\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    ">>> monty = 'Monty Python' 1<br>\n",
    ">>> monty<br>\n",
    "'Monty Python'<br>\n",
    ">>> circus = \"Monty Python's Flying Circus\" 2<br>\n",
    ">>> circus<br>\n",
    "\"Monty Python's Flying Circus\"<br>\n",
    ">>> circus = 'Monty Python\\'s Flying Circus' 3<br>\n",
    ">>> circus<br>\n",
    "\"Monty Python's Flying Circus\"<br>\n",
    ">>> circus = 'Monty Python's Flying Circus' 4<br>\n",
    "  File \"<stdin>\", line 1<br>\n",
    "    circus = 'Monty Python's Flying Circus'<br>\n",
    "                           ^<br>\n",
    "SyntaxError: invalid syntax\n",
    "</div>    \n",
    "    \n",
    "Sometimes strings go over several lines. Python provides us with various ways of entering them. In the next example, a sequence of two strings is joined into a single string. We need to use backslash 1 or parentheses 2 so that the interpreter knows that the statement is not complete after the first line.\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    ">>> couplet = \"Shall I compare thee to a Summer's day?\"\\<br>\n",
    "...           \"Thou are more lovely and more temperate:\" 1<br>\n",
    ">>> print couplet<br>\n",
    "Shall I compare thee to a Summer's day?Thou are more lovely and more temperate:<br>\n",
    ">>> couplet = (\"Rough winds do shake the darling buds of May,\"<br>\n",
    "...           \"And Summer's lease hath all too short a date:\") 2<br>\n",
    ">>> print couplet<br>\n",
    "Rough winds do shake the darling buds of May,And Summer's lease hath all too short a date:<br>\n",
    "</div>    \n",
    "    \n",
    "Unfortunately these methods do not give us a newline between the two lines of the sonnet. Instead, we can use a triple-quoted string as follows:\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    ">>> couplet = \"\"\"Shall I compare thee to a Summer's day?<br>\n",
    "... Thou are more lovely and more temperate:\"\"\"<br>\n",
    ">>> print couplet<br>\n",
    "Shall I compare thee to a Summer's day?<br>\n",
    "Thou are more lovely and more temperate:<br>\n",
    ">>> couplet = '''Rough winds do shake the darling buds of May,<br>\n",
    "... And Summer's lease hath all too short a date:'''<br>\n",
    ">>> print couplet<br>\n",
    "Rough winds do shake the darling buds of May,<br>\n",
    "And Summer's lease hath all too short a date:<br>\n",
    "</div>    \n",
    "    \n",
    "Now that we can define strings, we can try some simple operations on them. First let’s look at the + operation, known as concatenation 1. It produces a new string that is a copy of the two original strings pasted together end-to-end. Notice that concatenation doesn’t do anything clever like insert a space between the words. We can even multiply strings 2:\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    ">>> 'very' + 'very' + 'very' 1<br>\n",
    "'veryveryvery'<br>\n",
    ">>> 'very' * 3 2<br>\n",
    "'veryveryvery'<br>\n",
    "</div>\n",
    "    \n",
    "***    \n",
    "    NOTE\n",
    "        Your Turn: Try running the following code, then try to use your understanding of \n",
    "        the string + and * operations to figure out how it works. Be careful to distinguish \n",
    "        between the string ' ', which is a single whitespace character, and '', which is the empty string.\n",
    "***    \n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    ">>> a = [1, 2, 3, 4, 5, 6, 7, 6, 5, 4, 3, 2, 1]<br>\n",
    ">>> b = [' ' * 2 * (7 - i) + 'very' * i for i in a]<br>\n",
    ">>> for line in b:<br>\n",
    "...     print line\n",
    "</div>\n",
    "    \n",
    "We’ve seen that the addition and multiplication operations apply to strings, not just numbers. However, note that we cannot use subtraction or division with strings:\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    ">>> 'very' - 'y'<br>\n",
    "Traceback (most recent call last):<br>\n",
    "  File \"<stdin>\", line 1, in <module><br>\n",
    "TypeError: unsupported operand type(s) for -: 'str' and 'str'<br>\n",
    ">>> 'very' / 2<br>\n",
    "Traceback (most recent call last):<br>\n",
    "  File \"<stdin>\", line 1, in <module><br>\n",
    "TypeError: unsupported operand type(s) for /: 'str' and 'int'\n",
    "</div>\n",
    "    \n",
    "These error messages are another example of Python telling us that we have got our data types in a muddle. In the first case, we are told that the operation of subtraction (i.e., -) cannot apply to objects of type str (strings), while in the second, we are told that division cannot take str and int as its two operands.\n",
    "\n",
    "### Printing Strings\n",
    "    \n",
    "So far, when we have wanted to look at the contents of a variable or see the result of a calculation, we have just typed the variable name into the interpreter. We can also see the contents of a variable using the print statement:\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    ">>> print monty<br>\n",
    "Monty Python\n",
    "</div>\n",
    "    \n",
    "Notice that there are no quotation marks this time. When we inspect a variable by typing its name in the interpreter, the interpreter prints the Python representation of its value. Since it’s a string, the result is quoted. However, when we tell the interpreter to print the contents of the variable, we don’t see quotation characters, since there are none inside the string.\n",
    "\n",
    "The print statement allows us to display more than one item on a line in various ways, as shown here:\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    ">>> grail = 'Holy Grail'<br>\n",
    ">>> print monty + grail<br>\n",
    "Monty PythonHoly Grail<br>\n",
    ">>> print monty, grail<br>\n",
    "Monty Python Holy Grail<br>\n",
    ">>> print monty, \"and the\", grail<br>\n",
    "Monty Python and the Holy Grail\n",
    "</div>\n",
    "    \n",
    "### Accessing Individual Characters\n",
    "    \n",
    "As we saw in A Closer Look at Python: Texts as Lists of Words for lists, strings are indexed, starting from zero. When we index a string, we get one of its characters (or letters). A single character is nothing special—it’s just a string of length 1.\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    ">>> monty[0]<br>\n",
    "'M'<br>\n",
    ">>> monty[3]<br>\n",
    "'t'<br>\n",
    ">>> monty[5]<br>\n",
    "' '\n",
    "</div>\n",
    "    \n",
    "As with lists, if we try to access an index that is outside of the string, we get an error:\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    ">>> monty[20]<br>\n",
    "Traceback (most recent call last):<br>\n",
    "  File \"<stdin>\", line 1, in ?<br>\n",
    "IndexError: string index out of range\n",
    "</div>    \n",
    "    \n",
    "Again as with lists, we can use negative indexes for strings, where -1 is the index of the last character 1. Positive and negative indexes give us two ways to refer to any position in a string. In this case, when the string had a length of 12, indexes 5 and -7 both refer to the same character (a space). (Notice that 5 = len(monty) - 7.)\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    ">>> monty[-1] 1<br>\n",
    "'n'<br>\n",
    ">>> monty[5]<br>\n",
    "' '<br>\n",
    ">>> monty[-7]<br>\n",
    "' '\n",
    "</div>\n",
    "    \n",
    "We can write for loops to iterate over the characters in strings. This print statement ends with a trailing comma, which is how we tell Python not to print a newline at the end.\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    ">>> sent = 'colorless green ideas sleep furiously'<br>\n",
    ">>> for char in sent:<br>\n",
    "...     print char,<br>\n",
    "...<br>\n",
    "c o l o r l e s s   g r e e n   i d e a s   s l e e p   f u r i o u s l y\n",
    "</div>    \n",
    "    \n",
    "We can count individual characters as well. We should ignore the case distinction by normalizing everything to lowercase, and filter out non-alphabetic characters:\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    ">>> from nltk.corpus import gutenberg<br>\n",
    ">>> raw = gutenberg.raw('melville-moby_dick.txt')<br>\n",
    ">>> fdist = nltk.FreqDist(ch.lower() for ch in raw if ch.isalpha())<br>\n",
    ">>> fdist.keys()<br>\n",
    "['e', 't', 'a', 'o', 'n', 'i', 's', 'h', 'r', 'l', 'd', 'u', 'm', 'c', 'w',<br>\n",
    "'f', 'g', 'p', 'b', 'y', 'v', 'k', 'q', 'j', 'x', 'z']\n",
    "</div>    \n",
    "\n",
    "This gives us the letters of the alphabet, with the most frequently occurring letters listed first (this is quite complicated and we’ll explain it more carefully later). You might like to visualize the distribution using fdist.plot(). The relative character frequencies of a text can be used in automatically identifying the language of the text.\n",
    "\n",
    "### Accessing Substrings\n",
    "    \n",
    "String slicing: The string Monty Python is shown along with its positive and negative indexes; two substrings are selected using “slice” notation. The slice [m,n] contains the characters from position m through n-1.\n",
    "Figure 3-2. String slicing: The string Monty Python is shown along with its positive and negative indexes; two substrings are selected using “slice” notation. The slice [m,n] contains the characters from position m through n-1.\n",
    "\n",
    "A substring is any continuous section of a string that we want to pull out for further processing. We can easily access substrings using the same slice notation we used for lists (see Figure 3-2). For example, the following code accesses the substring starting at index 6, up to (but not including) index 10:\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    ">>> monty[6:10]<br>\n",
    "'Pyth'\n",
    "</div>\n",
    "    \n",
    "Here we see the characters are 'P', 'y', 't', and 'h', which correspond to monty[6] ... monty[9] but not monty[10]. This is because a slice starts at the first index but finishes one before the end index.\n",
    "\n",
    "We can also slice with negative indexes—the same basic rule of starting from the start index and stopping one before the end index applies; here we stop before the space character.\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    ">>> monty[-12:-7]<br>\n",
    "'Monty'\n",
    "</div>    \n",
    "    \n",
    "As with list slices, if we omit the first value, the substring begins at the start of the string. If we omit the second value, the substring continues to the end of the string:\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    ">>> monty[:5]<br>\n",
    "'Monty'<br>\n",
    ">>> monty[6:]<br>\n",
    "'Python'\n",
    "</div>\n",
    "    \n",
    "We test if a string contains a particular substring using the in operator, as follows:\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    ">>> phrase = 'And now for something completely different'<br>\n",
    ">>> if 'thing' in phrase:<br>\n",
    "...     print 'found \"thing\"'<br>\n",
    "found \"thing\"\n",
    "</div>    \n",
    "    \n",
    "We can also find the position of a substring within a string, using find():\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    ">>> monty.find('Python')<br>\n",
    "6\n",
    "</div>    \n",
    "    \n",
    "***    \n",
    "    NOTE\n",
    "        Your Turn: Make up a sentence and assign it to a variable, e.g., sent = 'my sentence...'. \n",
    "        Now write slice expressions to pull out individual words. \n",
    "        (This is obviously not a convenient way to process the words of a text!)\n",
    "***\n",
    "    \n",
    "### More Operations on Strings\n",
    "    \n",
    "Python has comprehensive support for processing strings. A summary, including some operations we haven’t seen yet, is shown in Table 3-2. For more information on strings, type help(str) at the Python prompt.\n",
    "\n",
    "Table 3-2. Useful string methods: Operations on strings in addition to the string tests shown in Table 1-4; all methods produce a new string or list\n",
    "\n",
    "![table 3-2](regexTab1.png)\n",
    "![table 3-2](regexTab2.png)    \n",
    "\n",
    "### The Difference Between Lists and Strings\n",
    "    \n",
    "Strings and lists are both kinds of sequence. We can pull them apart by indexing and slicing them, and we can join them together by concatenating them. However, we cannot join strings and lists:\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    ">>> query = 'Who knows?'<br>\n",
    ">>> beatles = ['John', 'Paul', 'George', 'Ringo']<br>\n",
    ">>> query[2]<br>\n",
    "'o'<br>\n",
    ">>> beatles[2]<br>\n",
    "'George'<br>\n",
    ">>> query[:2]<br>\n",
    "'Wh'<br>\n",
    ">>> beatles[:2]<br>\n",
    "['John', 'Paul']<br>\n",
    ">>> query + \" I don't\"<br>\n",
    "\"Who knows? I don't\"<br>\n",
    ">>> beatles + 'Brian'<br>\n",
    "Traceback (most recent call last):<br>\n",
    "  File \"<stdin>\", line 1, in <module><br>\n",
    "TypeError: can only concatenate list (not \"str\") to list<br>\n",
    ">>> beatles + ['Brian']<br>\n",
    "['John', 'Paul', 'George', 'Ringo', 'Brian']\n",
    "</div>\n",
    "    \n",
    "When we open a file for reading into a Python program, we get a string corresponding to the contents of the whole file. If we use a for loop to process the elements of this string, all we can pick out are the individual characters—we don’t get to choose the granularity. By contrast, the elements of a list can be as big or small as we like: for example, they could be paragraphs, sentences, phrases, words, characters. So lists have the advantage that we can be flexible about the elements they contain, and correspondingly flexible about any downstream processing. Consequently, one of the first things we are likely to do in a piece of NLP code is tokenize a string into a list of strings (Regular Expressions for Tokenizing Text). Conversely, when we want to write our results to a file, or to a terminal, we will usually format them as a string (Formatting: From Lists to Strings).\n",
    "\n",
    "Lists and strings do not have exactly the same functionality. Lists have the added power that you can change their elements:\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    ">>> beatles[0] = \"John Lennon\"<br>\n",
    ">>> del beatles[-1]<br>\n",
    ">>> beatles<br>\n",
    "['John Lennon', 'Paul', 'George']\n",
    "</div>\n",
    "    \n",
    "On the other hand, if we try to do that with a string—changing the 0th character in query to 'F'—we get:\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    ">>> query[0] = 'F'<br>\n",
    "Traceback (most recent call last):<br>\n",
    "  File \"<stdin>\", line 1, in ?<br>\n",
    "TypeError: object does not support item assignment\n",
    "</div>\n",
    "    \n",
    "This is because strings are immutable: you can’t change a string once you have created it. However, lists are mutable, and their contents can be modified at any time. As a result, lists support operations that modify the original value rather than producing a new value.\n",
    "\n",
    "***    \n",
    "    NOTE\n",
    "        Your Turn: Consolidate your knowledge of strings by trying some of the exercises on strings at the end of this chapter.\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "175931b8",
   "metadata": {},
   "source": [
    "## Text Processing with Unicode\n",
    "Our programs will often need to deal with different languages, and different character sets. The concept of “plain text” is a fiction. If you live in the English-speaking world you probably use ASCII, possibly without realizing it. If you live in Europe you might use one of the extended Latin character sets, containing such characters as “ø” for Danish and Norwegian, “ő” for Hungarian, “ñ” for Spanish and Breton, and “ň” for Czech and Slovak. In this section, we will give an overview of how to use Unicode for processing texts that use non-ASCII character sets.\n",
    "\n",
    "### What Is Unicode?\n",
    "Unicode supports over a million characters. Each character is assigned a number, called a code point. In Python, code points are written in the form \\uXXXX, where XXXX is the number in four-digit hexadecimal form.\n",
    "\n",
    "Within a program, we can manipulate Unicode strings just like normal strings. However, when Unicode characters are stored in files or displayed on a terminal, they must be encoded as a stream of bytes. Some encodings (such as ASCII and Latin-2) use a single byte per code point, so they can support only a small subset of Unicode, enough for a single language. Other encodings (such as UTF-8) use multiple bytes and can represent the full range of Unicode characters.\n",
    "\n",
    "Text in files will be in a particular encoding, so we need some mechanism for translating it into Unicode—translation into Unicode is called decoding. Conversely, to write out Unicode to a file or a terminal, we first need to translate it into a suitable encoding—this translation out of Unicode is called encoding, and is illustrated in Figure 3-3.\n",
    "\n",
    "![figure 3-3](unicode.png)\n",
    "<i>Figure 3-3. Unicode decoding and encoding.</i>\n",
    "\n",
    "From a Unicode perspective, characters are abstract entities that can be realized as one or more glyphs. Only glyphs can appear on a screen or be printed on paper. A font is a mapping from characters to glyphs.\n",
    "\n",
    "### Extracting Encoded Text from Files\n",
    "Let’s assume that we have a small text file, and that we know how it is encoded. For example, polish-lat2.txt, as the name suggests, is a snippet of Polish text (from the Polish Wikipedia; see http://pl.wikipedia.org/wiki/Biblioteka_Pruska). This file is encoded as Latin-2, also known as ISO-8859-2. The function nltk.data.find() locates the file for us.\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    ">>> path = nltk.data.find('corpora/unicode_samples/polish-lat2.txt')\n",
    "</div>\n",
    "The Python codecs module provides functions to read encoded data into Unicode strings, and to write out Unicode strings in encoded form. The codecs.open() function takes an encoding parameter to specify the encoding of the file being read or written. So let’s import the codecs module, and call it with the encoding 'latin2' to open our Polish file as Unicode:\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    ">>> import codecs<br>\n",
    ">>> f = codecs.open(path, encoding='latin2')\n",
    "</div>\n",
    "    \n",
    "For a list of encoding parameters allowed by codecs, see http://docs.python.org/lib/standard-encodings.html. Note that we can write Unicode-encoded data to a file using f = codecs.open(path, 'w', encoding='utf-8').\n",
    "\n",
    "Text read from the file object f will be returned in Unicode. As we pointed out earlier, in order to view this text on a terminal, we need to encode it, using a suitable encoding. The Python-specific encoding unicode_escape is a dummy encoding that converts all non-ASCII characters into their \\uXXXX representations. Code points above the ASCII 0–127 range but below 256 are represented in the two-digit form \\xXX.\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    ">>> for line in f:<br>\n",
    "...     line = line.strip()<br>\n",
    "...     print line.encode('unicode_escape')<br>\n",
    "Pruska Biblioteka Pa\\u0144stwowa. Jej dawne zbiory znane pod nazw\\u0105<br>\n",
    "\"Berlinka\" to skarb kultury i sztuki niemieckiej. Przewiezione przez<br>\n",
    "Niemc\\xf3w pod koniec II wojny \\u015bwiatowej na Dolny \\u015al\\u0105sk, zosta\\u0142y<br>\n",
    "odnalezione po 1945 r. na terytorium Polski. Trafi\\u0142y do Biblioteki<br>\n",
    "Jagiello\\u0144skiej w Krakowie, obejmuj\\u0105 ponad 500 tys. zabytkowych<br>\n",
    "archiwali\\xf3w, m.in. manuskrypty Goethego, Mozarta, Beethovena, Bacha.\n",
    "</div>\n",
    "    \n",
    "The first line in this output illustrates a Unicode escape string preceded by the \\u escape string, namely \\u0144. The relevant Unicode character will be displayed on the screen as the glyph ń. In the third line of the preceding example, we see \\xf3, which corresponds to the glyph ó, and is within the 128–255 range.\n",
    "\n",
    "In Python, a Unicode string literal can be specified by preceding an ordinary string literal with a u, as in u'hello'. Arbitrary Unicode characters are defined using the \\uXXXX escape sequence inside a Unicode string literal. We find the integer ordinal of a character using ord(). For example:\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    ">>> ord('a')<br>\n",
    "97\n",
    "</div>\n",
    "    \n",
    "The hexadecimal four-digit notation for 97 is 0061, so we can define a Unicode string literal with the appropriate escape sequence:\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    ">>> a = u'\\u0061'<br>\n",
    ">>> a<br>\n",
    "u'a'<br>\n",
    ">>> print a<br>\n",
    "a\n",
    "</div>\n",
    "    \n",
    "Notice that the Python print statement is assuming a default encoding of the Unicode character, namely ASCII. However, ń is outside the ASCII range, so cannot be printed unless we specify an encoding. In the following example, we have specified that print should use the repr() of the string, which outputs the UTF-8 escape sequences (of the form \\xXX) rather than trying to render the glyphs.\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    ">>> nacute = u'\\u0144'<br>\n",
    ">>> nacute<br>\n",
    "u'\\u0144'<br>\n",
    ">>> nacute_utf = nacute.encode('utf8')<br>\n",
    ">>> print repr(nacute_utf)<br>\n",
    "'\\xc5\\x84'\n",
    "</div>\n",
    "    \n",
    "If your operating system and locale are set up to render UTF-8 encoded characters, you ought to be able to give the Python command print nacute_utf and see ń on your screen.\n",
    "\n",
    "***\n",
    "    NOTE\n",
    "        There are many factors determining what glyphs are rendered on your screen. \n",
    "        If you are sure that you have the correct encoding, but your Python code is still \n",
    "        failing to produce the glyphs you expected, you should also check that you \n",
    "        have the necessary fonts installed on your system.\n",
    "***\n",
    "    \n",
    "The module unicodedata lets us inspect the properties of Unicode characters. In the following example, we select all characters in the third line of our Polish text outside the ASCII range and print their UTF-8 escaped value, followed by their code point integer using the standard Unicode convention (i.e., prefixing the hex digits with U+), followed by their Unicode name.\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    ">>> import unicodedata<br>\n",
    ">>> lines = codecs.open(path, encoding='latin2').readlines()<br>\n",
    ">>> line = lines[2]<br>\n",
    ">>> print line.encode('unicode_escape')<br>\n",
    "Niemc\\xf3w pod koniec II wojny \\u015bwiatowej na Dolny \\u015al\\u0105sk, zosta\\u0142y\\n<br>\n",
    ">>> for c in line:<br>\n",
    "...     if ord(c) > 127:<br>\n",
    "...         print '%r U+%04x %s' % (c.encode('utf8'), ord(c), unicodedata.name(c))<br>\n",
    "'\\xc3\\xb3' U+00f3 LATIN SMALL LETTER O WITH ACUTE<br>\n",
    "'\\xc5\\x9b' U+015b LATIN SMALL LETTER S WITH ACUTE<br>\n",
    "'\\xc5\\x9a' U+015a LATIN CAPITAL LETTER S WITH ACUTE<br>\n",
    "'\\xc4\\x85' U+0105 LATIN SMALL LETTER A WITH OGONEK<br>\n",
    "'\\xc5\\x82' U+0142 LATIN SMALL LETTER L WITH STROKE\n",
    "</div>    \n",
    "\n",
    "If you replace the %r (which yields the repr() value) by %s in the format string of the preceding code sample, and if your system supports UTF-8, you should see an output like the following:\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "ó U+00f3 LATIN SMALL LETTER O WITH ACUTE<br>\n",
    "ś U+015b LATIN SMALL LETTER S WITH ACUTE<br>\n",
    "Ś U+015a LATIN CAPITAL LETTER S WITH ACUTE<br>\n",
    "ą U+0105 LATIN SMALL LETTER A WITH OGONEK<br>\n",
    "ł U+0142 LATIN SMALL LETTER L WITH STROKE\n",
    "</div>\n",
    "    \n",
    "Alternatively, you may need to replace the encoding 'utf8' in the example by 'latin2', again depending on the details of your system.\n",
    "\n",
    "The next examples illustrate how Python string methods and the re module accept Unicode strings.\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    ">>> line.find(u'zosta\\u0142y')<br>\n",
    "54<br>\n",
    ">>> line = line.lower()<br>\n",
    ">>> print line.encode('unicode_escape')<br>\n",
    "niemc\\xf3w pod koniec ii wojny \\u015bwiatowej na dolny \\u015bl\\u0105sk, zosta\\u0142y\\n<br>\n",
    ">>> import re<br>\n",
    ">>> m = re.search(u'\\u015b\\w*', line)<br>\n",
    ">>> m.group()<br>\n",
    "u'\\u015bwiatowej'\n",
    "</div> \n",
    "    \n",
    "NLTK tokenizers allow Unicode strings as input, and correspondingly yield Unicode strings as output.\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    ">>> nltk.word_tokenize(line)  <br>\n",
    "[u'niemc\\xf3w', u'pod', u'koniec', u'ii', u'wojny', u'\\u015bwiatowej',<br>\n",
    "u'na', u'dolny', u'\\u015bl\\u0105sk', u'zosta\\u0142y']\n",
    "</div>\n",
    "    \n",
    "### Using Your Local Encoding in Python\n",
    "If you are used to working with characters in a particular local encoding, you probably want to be able to use your standard methods for inputting and editing strings in a Python file. In order to do this, you need to include the string '# -*- coding: <coding> -*-' as the first or second line of your file. Note that <coding> has to be a string like 'latin-1', 'big5', or 'utf-8' (see Figure 3-4).\n",
    "\n",
    "![figure 1-1](encoded.png)\n",
    "\n",
    "<i>Figure 3-4. Unicode and IDLE: UTF-8 encoded string literals in the IDLE editor; this requires that an appropriate font is set in IDLE’s preferences; here we have chosen Courier CE.</i>\n",
    "    \n",
    "Figure 3-4 also illustrates how regular expressions can use encoded strings."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a34fe24",
   "metadata": {},
   "source": [
    "## Regular Expressions for Detecting Word Patterns\n",
    "Many linguistic processing tasks involve pattern matching. For example, we can find words ending with ed using endswith('ed'). We saw a variety of such “word tests” in Table 1-4. Regular expressions give us a more powerful and flexible method for describing the character patterns we are interested in.\n",
    "\n",
    "***\n",
    "    NOTE\n",
    "        There are many other published introductions to regular expressions, organized\n",
    "        around the syntax of regular expressions and applied to searching text files. \n",
    "        Instead of doing this again, we focus on the use of regular expressions at different\n",
    "        stages of linguistic processing. As usual, we’ll adopt a problem-based approach and\n",
    "        present new features only as they are needed to solve practical problems. \n",
    "        In our discussion we will mark regular expressions using chevrons like this: «patt».\n",
    "***\n",
    "\n",
    "To use regular expressions in Python, we need to import the re library using: import re. We also need a list of words to search; we’ll use the Words Corpus again (Lexical Resources). We will preprocess it to remove any proper names.\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    ">>> import re<br>\n",
    ">>> wordlist = [w for w in nltk.corpus.words.words('en') if w.islower()]\n",
    "</div>\n",
    "    \n",
    "### Using Basic Metacharacters\n",
    "Let’s find words ending with ed using the regular expression «ed\\\\$». We will use the re.search(p, s) function to check whether the pattern p can be found somewhere inside the string s. We need to specify the characters of interest, and use the dollar sign, which has a special behavior in the context of regular expressions in that it matches the end of the word:\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    ">>> [w for w in wordlist if re.search('ed\\\\$', w)]<br>\n",
    "['abaissed', 'abandoned', 'abased', 'abashed', 'abatised', 'abed', 'aborted', ...]\n",
    "</div>\n",
    "    \n",
    "The . wildcard symbol matches any single character. Suppose we have room in a crossword puzzle for an eight-letter word, with j as its third letter and t as its sixth letter. In place of each blank cell we use a period:\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    ">>> [w for w in wordlist if re.search('^..j..t..\\\\$', w)]<br>\n",
    "['abjectly', 'adjuster', 'dejected', 'dejectly', 'injector', 'majestic', ...]\n",
    "</div>\n",
    "\n",
    "***\n",
    "    NOTE\n",
    "        Your Turn: The caret symbol ^ matches the start of a string, just like the \\\\$ matches the end. \n",
    "        What results do we get with the example just shown if we leave out both of these, \n",
    "        and search for «..j..t..»?\n",
    "***\n",
    "\n",
    "Finally, the ? symbol specifies that the previous character is optional. Thus «^e-?mail\\\\\\$» will match both email and e-mail. We could count the total number of occurrences of this word (in either spelling) in a text using sum(1 for w in text if re.search('^e-?mail\\\\\\$', w)).\n",
    "\n",
    "### Ranges and Closures\n",
    "The T9 system is used for entering text on mobile phones (see Figure 3-5). Two or more words that are entered with the same sequence of keystrokes are known as textonyms. For example, both hole and golf are entered by pressing the sequence 4653. What other words could be produced with the same sequence? Here we use the regular expression «^[ghi][mno][jlk][def]\\\\$»:\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    ">>> [w for w in wordlist if re.search('^[ghi][mno][jlk][def]\\\\$', w)]<br>\n",
    "['gold', 'golf', 'hold', 'hole']\n",
    "</div>\n",
    "    \n",
    "The first part of the expression, «^[ghi]», matches the start of a word followed by g, h, or i. The next part of the expression, «[mno]», constrains the second character to be m, n, or o. The third and fourth characters are also constrained. Only four words satisfy all these constraints. Note that the order of characters inside the square brackets is not significant, so we could have written «^[hig][nom][ljk][fed]\\\\$» and matched the same words.\n",
    "\n",
    "T9: Text on 9 keys.\n",
    "![figure 1-1](t9.png)\n",
    "<i>Figure 3-5. T9: Text on 9 keys.</i>\n",
    "\n",
    "***\n",
    "    NOTE\n",
    "        Your Turn: Look for some “finger-twisters,” by searching for words that use only\n",
    "        part of the number-pad. For example «^[ghijklmno]+\\\\$», or more concisely, «^[g-o]+\\\\$», \n",
    "        will match words that only use keys 4, 5, 6 in the center row, and «^[a-fj-o]+\\\\$» \n",
    "        will match words that use keys 2, 3, 5, 6 in the top-right corner. What do - and + mean?\n",
    "***\n",
    "\n",
    "Let’s explore the + symbol a bit further. Notice that it can be applied to individual letters, or to bracketed sets of letters:\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    ">>> chat_words = sorted(set(w for w in nltk.corpus.nps_chat.words()))<br>\n",
    ">>> [w for w in chat_words if re.search('^m+i+n+e+\\\\$', w)]<br>\n",
    "['miiiiiiiiiiiiinnnnnnnnnnneeeeeeeeee', 'miiiiiinnnnnnnnnneeeeeeee', 'mine',<br>\n",
    "'mmmmmmmmiiiiiiiiinnnnnnnnneeeeeeee']\n",
    ">>> [w for w in chat_words if re.search('^[ha]+\\\\$', w)]<br>\n",
    "['a', 'aaaaaaaaaaaaaaaaa', 'aaahhhh', 'ah', 'ahah', 'ahahah', 'ahh',<br>\n",
    "'ahhahahaha', 'ahhh', 'ahhhh', 'ahhhhhh', 'ahhhhhhhhhhhhhh', 'h', 'ha', 'haaa',<br>\n",
    "'hah', 'haha', 'hahaaa', 'hahah', 'hahaha', 'hahahaa', 'hahahah', 'hahahaha', ...]\n",
    "</div>    \n",
    "    \n",
    "It should be clear that + simply means “one or more instances of the preceding item,” which could be an individual character like m, a set like [fed], or a range like [d-f]. Now let’s replace + with *, which means “zero or more instances of the preceding item.” The regular expression «^m*i*n*e*\\\\$» will match everything that we found using «^m+i+n+e+\\\\$», but also words where some of the letters don’t appear at all, e.g., me, min, and mmmmm. Note that the + and * symbols are sometimes referred to as Kleene closures, or simply closures.\n",
    "\n",
    "The ^ operator has another function when it appears as the first character inside square brackets. For example, «[^aeiouAEIOU]» matches any character other than a vowel. We can search the NPS Chat Corpus for words that are made up entirely of non-vowel characters using «^[^aeiouAEIOU]+\\\\$» to find items like these: :):):), grrr, cyb3r, and zzzzzzzz. Notice this includes non-alphabetic characters.\n",
    "\n",
    "Here are some more examples of regular expressions being used to find tokens that match a particular pattern, illustrating the use of some new symbols: \\, {}, (), and |.\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    ">>> wsj = sorted(set(nltk.corpus.treebank.words()))<br>\n",
    ">>> [w for w in wsj if re.search('^[0-9]+\\.[0-9]+\\\\$', w)]<br>\n",
    "['0.0085', '0.05', '0.1', '0.16', '0.2', '0.25', '0.28', '0.3', '0.4', '0.5',<br>\n",
    "'0.50', '0.54', '0.56', '0.60', '0.7', '0.82', '0.84', '0.9', '0.95', '0.99',<br>\n",
    "'1.01', '1.1', '1.125', '1.14', '1.1650', '1.17', '1.18', '1.19', '1.2', ...]<br>\n",
    ">>> [w for w in wsj if re.search('^[A-Z]+\\\\\\$\\\\$', w)]<br>\n",
    "['C\\\\$', 'US\\\\$']<br>\n",
    ">>> [w for w in wsj if re.search('^[0-9]{4}\\\\$', w)]<br>\n",
    "['1614', '1637', '1787', '1901', '1903', '1917', '1925', '1929', '1933', ...]<br>\n",
    ">>> [w for w in wsj if re.search('^[0-9]+-[a-z]{3,5}\\\\$', w)]<br>\n",
    "['10-day', '10-lap', '10-year', '100-share', '12-point', '12-year', ...]<br>\n",
    ">>> [w for w in wsj if re.search('^[a-z]{5,}-[a-z]{2,3}-[a-z]{,6}\\\\$', w)]<br>\n",
    "['black-and-white', 'bread-and-butter', 'father-in-law', 'machine-gun-toting',<br>\n",
    "'savings-and-loan']<br>\n",
    ">>> [w for w in wsj if re.search('(ed|ing)\\\\$', w)]<br>\n",
    "['62%-owned', 'Absorbed', 'According', 'Adopting', 'Advanced', 'Advancing', ...]<br>\n",
    "</div>\n",
    "\n",
    "***\n",
    "    NOTE\n",
    "        Your Turn: Study the previous examples and try to work out \n",
    "        what the \\, {}, (), and | notations mean before you read on.\n",
    "***\n",
    "\n",
    "You probably worked out that a backslash means that the following character is deprived of its special powers and must literally match a specific character in the word. Thus, while . is special, \\. only matches a period. The braced expressions, like {3,5}, specify the number of repeats of the previous item. The pipe character indicates a choice between the material on its left or its right. Parentheses indicate the scope of an operator, and they can be used together with the pipe (or disjunction) symbol like this: «w(i|e|ai|oo)t», matching wit, wet, wait, and woot. It is instructive to see what happens when you omit the parentheses from the last expression in the example, and search for «ed|ing\\\\$».\n",
    "\n",
    "The metacharacters we have seen are summarized in Table 3-3.\n",
    "\n",
    "![table 3-3](regexOp1.png)\n",
    "![table 3-3](regexOp2.png)\n",
    "Table 3-3. Basic regular expression metacharacters, including wildcards, ranges, and closures\n",
    "\n",
    "To the Python interpreter, a regular expression is just like any other string. If the string contains a backslash followed by particular characters, it will interpret these specially. For example, \\b would be interpreted as the backspace character. In general, when using regular expressions containing backslash, we should instruct the interpreter not to look inside the string at all, but simply to pass it directly to the re library for processing. We do this by prefixing the string with the letter r, to indicate that it is a raw string. For example, the raw string r'\\band\\b' contains two \\b symbols that are interpreted by the re library as matching word boundaries instead of backspace characters. If you get into the habit of using r'...' for regular expressions—as we will do from now on—you will avoid having to think about these complications."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81000256",
   "metadata": {},
   "source": [
    "## Useful Applications of Regular Expressions\n",
    "The previous examples all involved searching for words w that match some regular expression regexp using re.search(regexp, w). Apart from checking whether a regular expression matches a word, we can use regular expressions to extract material from words, or to modify words in specific ways.\n",
    "\n",
    "### Extracting Word Pieces\n",
    "The re.findall() (“find all”) method finds all (non-overlapping) matches of the given regular expression. Let’s find all the vowels in a word, then count them:\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    ">>> word = 'supercalifragilisticexpialidocious'<br>\n",
    ">>> re.findall(r'[aeiou]', word)<br>\n",
    "['u', 'e', 'a', 'i', 'a', 'i', 'i', 'i', 'e', 'i', 'a', 'i', 'o', 'i', 'o', 'u']<br>\n",
    ">>> len(re.findall(r'[aeiou]', word))<br>\n",
    "16\n",
    "</div>    \n",
    "    \n",
    "Let’s look for all sequences of two or more vowels in some text, and determine their relative frequency:\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    ">>> wsj = sorted(set(nltk.corpus.treebank.words()))<br>\n",
    ">>> fd = nltk.FreqDist(vs for word in wsj<br>\n",
    "...                       for vs in re.findall(r'[aeiou]{2,}', word))<br>\n",
    ">>> fd.items()<br>\n",
    "[('io', 549), ('ea', 476), ('ie', 331), ('ou', 329), ('ai', 261), ('ia', 253),<br>\n",
    "('ee', 217), ('oo', 174), ('ua', 109), ('au', 106), ('ue', 105), ('ui', 95),<br>\n",
    "('ei', 86), ('oi', 65), ('oa', 59), ('eo', 39), ('iou', 27), ('eu', 18), ...]\n",
    "</div>\n",
    "    \n",
    "***\n",
    "    NOTE\n",
    "        Your Turn: In the W3C Date Time Format, dates are represented like this: 2009-12-31. \n",
    "        Replace the ? in the following Python code with a regular expression, \n",
    "        in order to convert the string '2009-12-31' to a list of integers [2009, 12, 31]:\n",
    "\n",
    "        [int(n) for n in re.findall(?, '2009-12-31')]\n",
    "***\n",
    "    \n",
    "### Doing More with Word Pieces\n",
    "Once we can use re.findall() to extract material from words, there are interesting things to do with the pieces, such as glue them back together or plot them.\n",
    "\n",
    "It is sometimes noted that English text is highly redundant, and it is still easy to read when word-internal vowels are left out. For example, declaration becomes dclrtn, and inalienable becomes inlnble, retaining any initial or final vowel sequences. The regular expression in our next example matches initial vowel sequences, final vowel sequences, and all consonants; everything else is ignored. This three-way disjunction is processed left-to-right, and if one of the three parts matches the word, any later parts of the regular expression are ignored. We use re.findall() to extract all the matching pieces, and ''.join() to join them together (see Formatting: From Lists to Strings for more about the join operation).\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    ">>> regexp = r'^[AEIOUaeiou]+|[AEIOUaeiou]+$|[^AEIOUaeiou]'<br>\n",
    ">>> def compress(word):<br>\n",
    "...     pieces = re.findall(regexp, word)<br>\n",
    "...     return ''.join(pieces)<br>\n",
    "...<br>\n",
    ">>> english_udhr = nltk.corpus.udhr.words('English-Latin1')<br>\n",
    ">>> print nltk.tokenwrap(compress(w) for w in english_udhr[:75])<br>\n",
    "Unvrsl Dclrtn of Hmn Rghts Prmble Whrs rcgntn of the inhrnt dgnty and<br>\n",
    "of the eql and inlnble rghts of all mmbrs of the hmn fmly is the fndtn<br>\n",
    "of frdm , jstce and pce in the wrld , Whrs dsrgrd and cntmpt fr hmn<br>\n",
    "rghts hve rsltd in brbrs acts whch hve outrgd the cnscnce of mnknd ,<br>\n",
    "and the advnt of a wrld in whch hmn bngs shll enjy frdm of spch and\n",
    "</div>\n",
    "\n",
    "Next, let’s combine regular expressions with conditional frequency distributions. Here we will extract all consonant-vowel sequences from the words of Rotokas, such as ka and si. Since each of these is a pair, it can be used to initialize a conditional frequency distribution. We then tabulate the frequency of each pair:\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    ">>> rotokas_words = nltk.corpus.toolbox.words('rotokas.dic')<br>\n",
    ">>> cvs = [cv for w in rotokas_words for cv in re.findall(r'[ptksvr][aeiou]', w)]<br>\n",
    ">>> cfd = nltk.ConditionalFreqDist(cvs)<br>\n",
    ">>> cfd.tabulate()<br>\n",
    "     a    e    i    o    u<br>\n",
    "k  418  148   94  420  173<br>\n",
    "p   83   31  105   34   51<br>\n",
    "r  187   63   84   89   79<br>\n",
    "s    0    0  100    2    1<br>\n",
    "t   47    8    0  148   37<br>\n",
    "v   93   27  105   48   49\n",
    "</div>\n",
    "\n",
    "Examining the rows for s and t, we see they are in partial “complementary distribution,” which is evidence that they are not distinct phonemes in the language. Thus, we could conceivably drop s from the Rotokas alphabet and simply have a pronunciation rule that the letter t is pronounced s when followed by i. (Note that the single entry having su, namely kasuari, ‘cassowary’ is borrowed from English).\n",
    "\n",
    "If we want to be able to inspect the words behind the numbers in that table, it would be helpful to have an index, allowing us to quickly find the list of words that contains a given consonant-vowel pair. For example, cv_index['su'] should give us all words containing su. Here’s how we can do this:\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    ">>> cv_word_pairs = [(cv, w) for w in rotokas_words<br>\n",
    "...                          for cv in re.findall(r'[ptksvr][aeiou]', w)]<br>\n",
    ">>> cv_index = nltk.Index(cv_word_pairs)<br>\n",
    ">>> cv_index['su']<br>\n",
    "['kasuari']<br>\n",
    ">>> cv_index['po']<br>\n",
    "['kaapo', 'kaapopato', 'kaipori', 'kaiporipie', 'kaiporivira', 'kapo', 'kapoa',<br>\n",
    "'kapokao', 'kapokapo', 'kapokapo', 'kapokapoa', 'kapokapoa', 'kapokapora', ...]\n",
    "</div>\n",
    "\n",
    "This program processes each word w in turn, and for each one, finds every substring that matches the regular expression «[ptksvr][aeiou]». In the case of the word kasuari, it finds ka, su, and ri. Therefore, the cv_word_pairs list will contain ('ka', 'kasuari'), ('su', 'kasuari'), and ('ri', 'kasuari'). One further step, using nltk.Index(), converts this into a useful index.\n",
    "\n",
    "### Finding Word Stems\n",
    "When we use a web search engine, we usually don’t mind (or even notice) if the words in the document differ from our search terms in having different endings. A query for laptops finds documents containing laptop and vice versa. Indeed, laptop and laptops are just two forms of the same dictionary word (or lemma). For some language processing tasks we want to ignore word endings, and just deal with word stems.\n",
    "\n",
    "There are various ways we can pull out the stem of a word. Here’s a simple-minded approach that just strips off anything that looks like a suffix:\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    ">>> def stem(word):<br>\n",
    "...     for suffix in ['ing', 'ly', 'ed', 'ious', 'ies', 'ive', 'es', 's', 'ment']:<br>\n",
    "...         if word.endswith(suffix):<br>\n",
    "...             return word[:-len(suffix)]<br>\n",
    "...     return word\n",
    "</div>\n",
    "\n",
    "Although we will ultimately use NLTK’s built-in stemmers, it’s interesting to see how we can use regular expressions for this task. Our first step is to build up a disjunction of all the suffixes. We need to enclose it in parentheses in order to limit the scope of the disjunction.\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    ">>> re.findall(r'^.*(ing|ly|ed|ious|ies|ive|es|s|ment)$', 'processing')<br>\n",
    "['ing']\n",
    "</div>\n",
    "    \n",
    "Here, re.findall() just gave us the suffix even though the regular expression matched the entire word. This is because the parentheses have a second function, to select substrings to be extracted. If we want to use the parentheses to specify the scope of the disjunction, but not to select the material to be output, we have to add ?:, which is just one of many arcane subtleties of regular expressions. Here’s the revised version.\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    ">>> re.findall(r'^.*(?:ing|ly|ed|ious|ies|ive|es|s|ment)$', 'processing')<br>\n",
    "['processing']\n",
    "</div>\n",
    "\n",
    "However, we’d actually like to split the word into stem and suffix. So we should just parenthesize both parts of the regular expression:\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    ">>> re.findall(r'^(.*)(ing|ly|ed|ious|ies|ive|es|s|ment)$', 'processing')<br>\n",
    "[('process', 'ing')]\n",
    "</div>    \n",
    "    \n",
    "This looks promising, but still has a problem. Let’s look at a different word, processes:\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    ">>> re.findall(r'^(.*)(ing|ly|ed|ious|ies|ive|es|s|ment)$', 'processes')<br>\n",
    "[('processe', 's')]\n",
    "</div>\n",
    "\n",
    "The regular expression incorrectly found an -s suffix instead of an -es suffix. This demonstrates another subtlety: the star operator is “greedy” and so the .* part of the expression tries to consume as much of the input as possible. If we use the “non-greedy” version of the star operator, written *?, we get what we want:\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    ">>> re.findall(r'^(.*?)(ing|ly|ed|ious|ies|ive|es|s|ment)$', 'processes')<br>\n",
    "[('process', 'es')]\n",
    "</div>    \n",
    "    \n",
    "This works even when we allow an empty suffix, by making the content of the second parentheses optional:\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    ">>> re.findall(r'^(.*?)(ing|ly|ed|ious|ies|ive|es|s|ment)?$', 'language')<br>\n",
    "[('language', '')]\n",
    "</div>\n",
    "\n",
    "This approach still has many problems (can you spot them?), but we will move on to define a function to perform stemming, and apply it to a whole text:\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    ">>> def stem(word):<br>\n",
    "...     regexp = r'^(.*?)(ing|ly|ed|ious|ies|ive|es|s|ment)?$'<br>\n",
    "...     stem, suffix = re.findall(regexp, word)[0]<br>\n",
    "...     return stem<br>\n",
    "...<br>\n",
    ">>> raw = \"\"\"DENNIS: Listen, strange women lying in ponds distributing swords<br>\n",
    "... is no basis for a system of government.  Supreme executive power derives from<br>\n",
    "... a mandate from the masses, not from some farcical aquatic ceremony.\"\"\"<br>\n",
    ">>> tokens = nltk.word_tokenize(raw)<br>\n",
    ">>> [stem(t) for t in tokens]<br>\n",
    "['DENNIS', ':', 'Listen', ',', 'strange', 'women', 'ly', 'in', 'pond',<br>\n",
    "'distribut', 'sword', 'i', 'no', 'basi', 'for', 'a', 'system', 'of', 'govern',<br>\n",
    "'.', 'Supreme', 'execut', 'power', 'deriv', 'from', 'a', 'mandate', 'from',<br>\n",
    "'the', 'mass', ',', 'not', 'from', 'some', 'farcical', 'aquatic', 'ceremony', '.']\n",
    "</div>    \n",
    "    \n",
    "Notice that our regular expression removed the s from ponds but also from is and basis. It produced some non-words, such as distribut and deriv, but these are acceptable stems in some applications.\n",
    "\n",
    "### Searching Tokenized Text\n",
    "You can use a special kind of regular expression for searching across multiple words in a text (where a text is a list of tokens). For example, \"\\<a\\> \\<man\\>\" finds all instances of a man in the text. The angle brackets are used to mark token boundaries, and any whitespace between the angle brackets is ignored (behaviors that are unique to NLTK’s findall() method for texts). In the following example, we include <.*> 1, which will match any single token, and enclose it in parentheses so only the matched word (e.g., monied) and not the matched phrase (e.g., a monied man) is produced. The second example finds three-word phrases ending with the word bro 2. The last example finds sequences of three or more words starting with the letter l 3.\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    ">>> from nltk.corpus import gutenberg, nps_chat<br>\n",
    ">>> moby = nltk.Text(gutenberg.words('melville-moby_dick.txt'))<br>\n",
    ">>> moby.findall(r\"<a> (<.*>) <man>\") 1<br>\n",
    "monied; nervous; dangerous; white; white; white; pious; queer; good;<br>\n",
    "mature; white; Cape; great; wise; wise; butterless; white; fiendish;<br>\n",
    "pale; furious; better; certain; complete; dismasted; younger; brave;<br>\n",
    "brave; brave; brave<br>\n",
    ">>> chat = nltk.Text(nps_chat.words())<br>\n",
    ">>> chat.findall(r\"<.*> <.*> <bro>\") 2<br>\n",
    "you rule bro; telling you bro; u twizted bro<br>\n",
    ">>> chat.findall(r\"<l.*>{3,}\") 3<br>\n",
    "lol lol lol; lmao lol lol; lol lol lol; la la la la la; la la la; la<br>\n",
    "la la; lovely lol lol love; lol lol lol.; la la la; la la la\n",
    "</div>\n",
    "    \n",
    "***    \n",
    "    NOTE\n",
    "        Your Turn: Consolidate your understanding of regular expression patterns and \n",
    "        substitutions using nltk.re_show(p, s), which annotates the string s to show \n",
    "        every place where pattern p was matched, and nltk.app.nemo(), which provides \n",
    "        a graphical interface for exploring regular expressions. For more practice, \n",
    "        try some of the exercises on regular expressions at the end of this chapter.\n",
    "***\n",
    "\n",
    "It is easy to build search patterns when the linguistic phenomenon we’re studying is tied to particular words. In some cases, a little creativity will go a long way. For instance, searching a large text corpus for expressions of the form x and other ys allows us to discover hypernyms (see WordNet):\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    ">>> from nltk.corpus import brown<br>\n",
    ">>> hobbies_learned = nltk.Text(brown.words(categories=['hobbies', 'learned']))<br>\n",
    ">>> hobbies_learned.findall(r\"<\\w*> <and> <other> <\\w*s>\")<br>\n",
    "speed and other activities; water and other liquids; tomb and other<br>\n",
    "landmarks; Statues and other monuments; pearls and other jewels;<br>\n",
    "charts and other items; roads and other features; figures and other<br>\n",
    "objects; military and other areas; demands and other factors;<br>\n",
    "abstracts and other compilations; iron and other metals\n",
    "</div>    \n",
    "    \n",
    "With enough text, this approach would give us a useful store of information about the taxonomy of objects, without the need for any manual labor. However, our search results will usually contain false positives, i.e., cases that we would want to exclude. For example, the result demands and other factors suggests that demand is an instance of the type factor, but this sentence is actually about wage demands. Nevertheless, we could construct our own ontology of English concepts by manually correcting the output of such searches.\n",
    "\n",
    "***\n",
    "    NOTE\n",
    "        This combination of automatic and manual processing is the most common way \n",
    "        for new corpora to be constructed. We will return to this in Chapter 11.\n",
    "***\n",
    "\n",
    "Searching corpora also suffers from the problem of false negatives, i.e., omitting cases that we would want to include. It is risky to conclude that some linguistic phenomenon doesn’t exist in a corpus just because we couldn’t find any instances of a search pattern. Perhaps we just didn’t think carefully enough about suitable patterns.\n",
    "\n",
    "***\n",
    "    NOTE\n",
    "        Your Turn: Look for instances of the pattern as x as y to discover \n",
    "        information about entities and their properties.\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaa8444c",
   "metadata": {},
   "source": [
    "## Normalizing Text\n",
    "In earlier program examples we have often converted text to lowercase before doing anything with its words, e.g., set(w.lower() for w in text). By using lower(), we have normalized the text to lowercase so that the distinction between The and the is ignored. Often we want to go further than this and strip off any affixes, a task known as stemming. A further step is to make sure that the resulting form is a known word in a dictionary, a task known as lemmatization. We discuss each of these in turn. First, we need to define the data we will use in this section:\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    ">>> raw = \"\"\"DENNIS: Listen, strange women lying in ponds distributing swords<br>\n",
    "... is no basis for a system of government.  Supreme executive power derives from<br>\n",
    "... a mandate from the masses, not from some farcical aquatic ceremony.\"\"\"<br>\n",
    ">>> tokens = nltk.word_tokenize(raw)\n",
    "</div>\n",
    "\n",
    "### Stemmers\n",
    "NLTK includes several off-the-shelf stemmers, and if you ever need a stemmer, you should use one of these in preference to crafting your own using regular expressions, since NLTK’s stemmers handle a wide range of irregular cases. The Porter and Lancaster stemmers follow their own rules for stripping affixes. Observe that the Porter stemmer correctly handles the word lying (mapping it to lie), whereas the Lancaster stemmer does not.\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    ">>> porter = nltk.PorterStemmer()<br>\n",
    ">>> lancaster = nltk.LancasterStemmer()<br>\n",
    ">>> [porter.stem(t) for t in tokens]<br>\n",
    "['DENNI', ':', 'Listen', ',', 'strang', 'women', 'lie', 'in', 'pond',<br>\n",
    "'distribut', 'sword', 'is', 'no', 'basi', 'for', 'a', 'system', 'of', 'govern',<br>\n",
    "'.', 'Suprem', 'execut', 'power', 'deriv', 'from', 'a', 'mandat', 'from',<br>\n",
    "'the', 'mass', ',', 'not', 'from', 'some', 'farcic', 'aquat', 'ceremoni', '.']<br>\n",
    ">>> [lancaster.stem(t) for t in tokens]<br>\n",
    "['den', ':', 'list', ',', 'strange', 'wom', 'lying', 'in', 'pond', 'distribut',<br>\n",
    "'sword', 'is', 'no', 'bas', 'for', 'a', 'system', 'of', 'govern', '.', 'suprem',<br>\n",
    "'execut', 'pow', 'der', 'from', 'a', 'mand', 'from', 'the', 'mass', ',', 'not',<br>\n",
    "'from', 'som', 'farc', 'aqu', 'ceremony', '.']\n",
    "</div>\n",
    "    \n",
    "Stemming is not a well-defined process, and we typically pick the stemmer that best suits the application we have in mind. The Porter Stemmer is a good choice if you are indexing some texts and want to support search using alternative forms of words (illustrated in Example 3-1, which uses object-oriented programming techniques that are outside the scope of this book, string formatting techniques to be covered in Formatting: From Lists to Strings, and the enumerate() function to be explained in Sequences).\n",
    "\n",
    "<i>Example 3-1. Indexing a text using a stemmer.</i>\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "class IndexedText(object):<br>\n",
    "<br>\n",
    "def __init__(self, stemmer, text):<br>\n",
    "    self._text = text<br>\n",
    "    self._stemmer = stemmer<br>\n",
    "    self._index = nltk.Index((self._stem(word), i)<br>\n",
    "                             for (i, word) in enumerate(text))<br>\n",
    "<br>\n",
    "def concordance(self, word, width=40):<br>\n",
    "    key = self._stem(word)<br>\n",
    "    wc = width/4                # words of context<br>\n",
    "    for i in self._index[key]:<br>\n",
    "        lcontext = ' '.join(self._text[i-wc:i])<br>\n",
    "        rcontext = ' '.join(self._text[i:i+wc])<br>\n",
    "        ldisplay = '%*s'  % (width, lcontext[-width:])<br>\n",
    "        rdisplay = '%-*s' % (width, rcontext[:width])<br>\n",
    "        print ldisplay, rdisplay<br>\n",
    "<br>\n",
    "def _stem(self, word):<br>\n",
    "    return self._stemmer.stem(word).lower()\n",
    "</div>\n",
    "    \n",
    "<div class=\"alert alert-block alert-info\">\n",
    ">>> porter = nltk.PorterStemmer()<br>\n",
    ">>> grail = nltk.corpus.webtext.words('grail.txt')<br>\n",
    ">>> text = IndexedText(porter, grail)<br>\n",
    ">>> text.concordance('lie')<br>\n",
    "r king ! DENNIS : Listen , strange women lying in ponds distributing swords is no<br>\n",
    " beat a very brave retreat . ROBIN : All lies ! MINSTREL : [ singing ] Bravest of<br>\n",
    "       Nay . Nay . Come . Come . You may lie here . Oh , but you are wounded !<br>\n",
    "doctors immediately ! No , no , please ! Lie down . [ clap clap ] PIGLET : Well<br>\n",
    "ere is much danger , for beyond the cave lies the Gorge of Eternal Peril , which<br>\n",
    "   you . Oh ... TIM : To the north there lies a cave -- the cave of Caerbannog --<br>\n",
    "h it and lived ! Bones of full fifty men lie strewn about its lair . So , brave k<br>\n",
    "not stop our fight ' til each one of you lies dead , and the Holy Grail returns t<br>\n",
    "</div>\n",
    "    \n",
    "### Lemmatization\n",
    "The WordNet lemmatizer removes affixes only if the resulting word is in its dictionary. This additional checking process makes the lemmatizer slower than the stemmers just mentioned. Notice that it doesn’t handle lying, but it converts women to woman.\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    ">>> wnl = nltk.WordNetLemmatizer()<br>\n",
    ">>> [wnl.lemmatize(t) for t in tokens]<br>\n",
    "['DENNIS', ':', 'Listen', ',', 'strange', 'woman', 'lying', 'in', 'pond',<br>\n",
    "'distributing', 'sword', 'is', 'no', 'basis', 'for', 'a', 'system', 'of',<br>\n",
    "'government', '.', 'Supreme', 'executive', 'power', 'derives', 'from', 'a',<br>\n",
    "'mandate', 'from', 'the', 'mass', ',', 'not', 'from', 'some', 'farcical',<br>\n",
    "'aquatic', 'ceremony', '.']\n",
    "</div>\n",
    "    \n",
    "The WordNet lemmatizer is a good choice if you want to compile the vocabulary of some texts and want a list of valid lemmas (or lexicon headwords).\n",
    "\n",
    "***    \n",
    "    NOTE\n",
    "        Another normalization task involves identifying non-standard words, including numbers, \n",
    "        abbreviations, and dates, and mapping any such tokens to a special vocabulary. \n",
    "        For example, every decimal number could be mapped to a single token 0.0, and every \n",
    "        acronym could be mapped to AAA. This keeps the vocabulary small and improves the \n",
    "        accuracy of many language modeling tasks.\n",
    "***    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b524b4c6",
   "metadata": {},
   "source": [
    "## Regular Expressions for Tokenizing Text\n",
    "Tokenization is the task of cutting a string into identifiable linguistic units that constitute a piece of language data. Although it is a fundamental task, we have been able to delay it until now because many corpora are already tokenized, and because NLTK includes some tokenizers. Now that you are familiar with regular expressions, you can learn how to use them to tokenize text, and to have much more control over the process.\n",
    "\n",
    "### Simple Approaches to Tokenization\n",
    "The very simplest method for tokenizing text is to split on whitespace. Consider the following text from Alice’s Adventures in Wonderland:\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    ">>> raw = \"\"\"'When I'M a Duchess,' she said to herself, (not in a very hopeful tone<br>\n",
    "... though), 'I won't have any pepper in my kitchen AT ALL. Soup does very<br>\n",
    "... well without--Maybe it's always pepper that makes people hot-tempered,'...\"\"\"\n",
    "</div>\n",
    "    \n",
    "We could split this raw text on whitespace using raw.split(). To do the same using a regular expression, it is not enough to match any space characters in the string , since this results in tokens that contain a \\n newline character; instead, we need to match any number of spaces, tabs, or newlines :\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    ">>> re.split(r' ', raw) <br>\n",
    "[\"'When\", \"I'M\", 'a', \"Duchess,'\", 'she', 'said', 'to', 'herself,', '(not', 'in',<br>\n",
    "'a', 'very', 'hopeful', 'tone\\nthough),', \"'I\", \"won't\", 'have', 'any', 'pepper',<br>\n",
    "'in', 'my', 'kitchen', 'AT', 'ALL.', 'Soup', 'does', 'very\\nwell', 'without--Maybe',<br>\n",
    "\"it's\", 'always', 'pepper', 'that', 'makes', 'people', \"hot-tempered,'...\"]<br>\n",
    ">>> re.split(r'[ \\t\\n]+', raw) <br>\n",
    "[\"'When\", \"I'M\", 'a', \"Duchess,'\", 'she', 'said', 'to', 'herself,', '(not', 'in',<br>\n",
    "'a', 'very', 'hopeful', 'tone', 'though),', \"'I\", \"won't\", 'have', 'any', 'pepper',<br>\n",
    "'in', 'my', 'kitchen', 'AT', 'ALL.', 'Soup', 'does', 'very', 'well', 'without--Maybe',<br>\n",
    "\"it's\", 'always', 'pepper', 'that', 'makes', 'people', \"hot-tempered,'...\"]\n",
    "</div>    \n",
    "    \n",
    "The regular expression «[ \\t\\n]+» matches one or more spaces, tabs (\\t), or newlines (\\n). Other whitespace characters, such as carriage return and form feed, should really be included too. Instead, we will use a built-in re abbreviation, \\s, which means any whitespace character. The second statement in the preceding example can be rewritten as re.split(r'\\s+', raw).\n",
    "\n",
    "***    \n",
    "    NOTE\n",
    "        Important: Remember to prefix regular expressions with the letter r (meaning “raw”), \n",
    "        which instructs the Python interpreter to treat the string literally, \n",
    "        rather than processing any backslashed characters it contains.\n",
    "***    \n",
    "\n",
    "Splitting on whitespace gives us tokens like '(not' and 'herself,'. An alternative is to use the fact that Python provides us with a character class \\w for word characters, equivalent to [a-zA-Z0-9_]. It also defines the complement of this class, \\W, i.e., all characters other than letters, digits, or underscore. We can use \\W in a simple regular expression to split the input on anything other than a word character:\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    ">>> re.split(r'\\W+', raw)<br>\n",
    "['', 'When', 'I', 'M', 'a', 'Duchess', 'she', 'said', 'to', 'herself', 'not', 'in',<br>\n",
    "'a', 'very', 'hopeful', 'tone', 'though', 'I', 'won', 't', 'have', 'any', 'pepper',<br>\n",
    "'in', 'my', 'kitchen', 'AT', 'ALL', 'Soup', 'does', 'very', 'well', 'without',<br>\n",
    "'Maybe', 'it', 's', 'always', 'pepper', 'that', 'makes', 'people', 'hot', 'tempered',<br>\n",
    "'']\n",
    "</div> \n",
    "    \n",
    "Observe that this gives us empty strings at the start and the end (to understand why, try doing 'xx'.split('x')). With re.findall(r'\\w+', raw), we get the same tokens, but without the empty strings, using a pattern that matches the words instead of the spaces. Now that we’re matching the words, we’re in a position to extend the regular expression to cover a wider range of cases. The regular expression «\\w+|\\S\\w*» will first try to match any sequence of word characters. If no match is found, it will try to match any non-whitespace character (\\S is the complement of \\s) followed by further word characters. This means that punctuation is grouped with any following letters (e.g., ’s) but that sequences of two or more punctuation characters are separated.\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    ">>> re.findall(r'\\w+|\\S\\w*', raw)<br>\n",
    "[\"'When\", 'I', \"'M\", 'a', 'Duchess', ',', \"'\", 'she', 'said', 'to', 'herself', ',',<br>\n",
    "'(not', 'in', 'a', 'very', 'hopeful', 'tone', 'though', ')', ',', \"'I\", 'won', \"'t\",<br>\n",
    "'have', 'any', 'pepper', 'in', 'my', 'kitchen', 'AT', 'ALL', '.', 'Soup', 'does',<br>\n",
    "'very', 'well', 'without', '-', '-Maybe', 'it', \"'s\", 'always', 'pepper', 'that',<br>\n",
    "'makes', 'people', 'hot', '-tempered', ',', \"'\", '.', '.', '.']\n",
    "</div>\n",
    "    \n",
    "Let’s generalize the \\w+ in the preceding expression to permit word-internal hyphens and apostrophes: «\\w+([-']\\w+)*». This expression means \\w+ followed by zero or more instances of [-']\\w+; it would match hot-tempered and it’s. (We need to include ?: in this expression for reasons discussed earlier.) We’ll also add a pattern to match quote characters so these are kept separate from the text they enclose.\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    ">>> print re.findall(r\"\\w+(?:[-']\\w+)*|'|[-.(]+|\\S\\w*\", raw)<br>\n",
    "[\"'\", 'When', \"I'M\", 'a', 'Duchess', ',', \"'\", 'she', 'said', 'to', 'herself', ',',<br>\n",
    "'(', 'not', 'in', 'a', 'very', 'hopeful', 'tone', 'though', ')', ',', \"'\", 'I',<br>\n",
    "\"won't\", 'have', 'any', 'pepper', 'in', 'my', 'kitchen', 'AT', 'ALL', '.', 'Soup',<br>\n",
    "'does', 'very', 'well', 'without', '--', 'Maybe', \"it's\", 'always', 'pepper',<br>\n",
    "'that', 'makes', 'people', 'hot-tempered', ',', \"'\", '...']\n",
    "</div>    \n",
    "    \n",
    "The expression in this example also included «[-.(]+», which causes the double hyphen, ellipsis, and open parenthesis to be tokenized separately.\n",
    "\n",
    "Table 3-4 lists the regular expression character class symbols we have seen in this section, in addition to some other useful symbols.\n",
    "![table 3-4](regexSym.png)\n",
    "    <i>Table 3-4. Regular expression symbols</i>\n",
    "\n",
    "### NLTK’s Regular Expression Tokenizer\n",
    "The function nltk.regexp_tokenize() is similar to re.findall() (as we’ve been using it for tokenization). However, nltk.regexp_tokenize() is more efficient for this task, and avoids the need for special treatment of parentheses. For readability we break up the regular expression over several lines and add a comment about each line. The special (?x) “verbose flag” tells Python to strip out the embedded whitespace and comments.\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    ">>> text = 'That U.S.A. poster-print costs \\$12.40...'<br>\n",
    ">>> pattern = r'''(?x)    # set flag to allow verbose regexps<br>\n",
    "...     ([A-Z]\\.)+        # abbreviations, e.g. U.S.A.<br>\n",
    "...   | \\w+(-\\w+)*        # words with optional internal hyphens<br>\n",
    "...   | \\$?\\d+(\\.\\d+)?%?  # currency and percentages, e.g. \\$12.40, 82%<br>\n",
    "...   | \\.\\.\\.            # ellipsis<br>\n",
    "...   | [][.,;\"'?():-_`]  # these are separate tokens<br>\n",
    "... '''<br>\n",
    ">>> nltk.regexp_tokenize(text, pattern)<br>\n",
    "['That', 'U.S.A.', 'poster-print', 'costs', '\\$12.40', '...']\n",
    "</div>    \n",
    "    \n",
    "When using the verbose flag, you can no longer use ' ' to match a space character; use \\s instead. The regexp_tokenize() function has an optional gaps parameter. When set to True, the regular expression specifies the gaps between tokens, as with re.split().\n",
    "\n",
    "***    \n",
    "    NOTE\n",
    "        We can evaluate a tokenizer by comparing the resulting tokens with a wordlist, \n",
    "        and then report any tokens that don’t appear in the wordlist, using set(tokens).difference(wordlist). \n",
    "        You’ll probably want to lowercase all the tokens first.\n",
    "***    \n",
    "\n",
    "### Further Issues with Tokenization\n",
    "Tokenization turns out to be a far more difficult task than you might have expected. No single solution works well across the board, and we must decide what counts as a token depending on the application domain.\n",
    "\n",
    "When developing a tokenizer it helps to have access to raw text which has been manually tokenized, in order to compare the output of your tokenizer with high-quality (or “gold-standard”) tokens. The NLTK corpus collection includes a sample of Penn Treebank data, including the raw Wall Street Journal text (nltk.corpus.treebank_raw.raw()) and the tokenized version (nltk.corpus.treebank.words()).\n",
    "\n",
    "A final issue for tokenization is the presence of contractions, such as didn’t. If we are analyzing the meaning of a sentence, it would probably be more useful to normalize this form to two separate forms: did and n’t (or not). We can do this work with the help of a lookup table."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c7c8290",
   "metadata": {},
   "source": [
    "## Segmentation\n",
    "This section discusses more advanced concepts, which you may prefer to skip on the first time through this chapter.\n",
    "\n",
    "Tokenization is an instance of a more general problem of segmentation. In this section, we will look at two other instances of this problem, which use radically different techniques to the ones we have seen so far in this chapter.\n",
    "\n",
    "### Sentence Segmentation\n",
    "Manipulating texts at the level of individual words often presupposes the ability to divide a text into individual sentences. As we have seen, some corpora already provide access at the sentence level. In the following example, we compute the average number of words per sentence in the Brown Corpus:\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    ">>> len(nltk.corpus.brown.words()) / len(nltk.corpus.brown.sents())<br>\n",
    "20.250994070456922\n",
    "</div>\n",
    "    \n",
    "In other cases, the text is available only as a stream of characters. Before tokenizing the text into words, we need to segment it into sentences. NLTK facilitates this by including the Punkt sentence segmenter (Kiss & Strunk, 2006). Here is an example of its use in segmenting the text of a novel. (Note that if the segmenter’s internal data has been updated by the time you read this, you will see different output.)\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    ">>> sent_tokenizer=nltk.data.load('tokenizers/punkt/english.pickle')<br>\n",
    ">>> text = nltk.corpus.gutenberg.raw('chesterton-thursday.txt')<br>\n",
    ">>> sents = sent_tokenizer.tokenize(text)<br>\n",
    ">>> pprint.pprint(sents[171:181])<br>\n",
    "['\"Nonsense!',<br>\n",
    " '\" said Gregory, who was very rational when anyone else\\nattempted paradox.',<br>\n",
    " '\"Why do all the clerks and navvies in the\\nrailway trains look so sad and tired,...',<br>\n",
    " 'I will\\ntell you.',<br>\n",
    " 'It is because they know that the train is going right.',<br>\n",
    " 'It\\nis because they know that whatever place they have taken a ticket\\nfor that ...',<br>\n",
    " 'It is because after they have\\npassed Sloane Square they know that the next stat...',<br>\n",
    " 'Oh, their wild rapture!',<br>\n",
    " 'oh,\\ntheir eyes like stars and their souls again in Eden, if the next\\nstation w...'<br>\n",
    " '\"\\n\\n\"It is you who are unpoetical,\" replied the poet Syme.']\n",
    "</div>    \n",
    "    \n",
    "Notice that this example is really a single sentence, reporting the speech of Mr. Lucian Gregory. However, the quoted speech contains several sentences, and these have been split into individual strings. This is reasonable behavior for most applications.\n",
    "\n",
    "Sentence segmentation is difficult because a period is used to mark abbreviations, and some periods simultaneously mark an abbreviation and terminate a sentence, as often happens with acronyms like U.S.A.\n",
    "\n",
    "For another approach to sentence segmentation, see Further Examples of Supervised Classification.\n",
    "\n",
    "### Word Segmentation\n",
    "For some writing systems, tokenizing text is made more difficult by the fact that there is no visual representation of word boundaries. For example, in Chinese, the three-character string: 爱国人 (ai4 “love” [verb], guo3 “country”, ren2 “person”) could be tokenized as 爱国 / 人, “country-loving person,” or as 爱 / 国人, “love country-person.”\n",
    "\n",
    "A similar problem arises in the processing of spoken language, where the hearer must segment a continuous speech stream into individual words. A particularly challenging version of this problem arises when we don’t know the words in advance. This is the problem faced by a language learner, such as a child hearing utterances from a parent. Consider the following artificial example, where word boundaries have been removed:\n",
    "\n",
    "Example 3-2. \n",
    "<ol>\n",
    "<li>doyouseethekitty</li>\n",
    "\n",
    "<li>seethedoggy</li>\n",
    "\n",
    "<li>doyoulikethekitty</li>\n",
    "\n",
    "<li>likethedoggy</li>\n",
    "</ol>\n",
    "\n",
    "Our first challenge is simply to represent the problem: we need to find a way to separate text content from the segmentation. We can do this by annotating each character with a boolean value to indicate whether or not a word-break appears after the character (an idea that will be used heavily for “chunking” in Chapter 7). Let’s assume that the learner is given the utterance breaks, since these often correspond to extended pauses. Here is a possible representation, including the initial and target segmentations:\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    ">>> text = \"doyouseethekittyseethedoggydoyoulikethekittylikethedoggy\"<br>\n",
    ">>> seg1 = \"0000000000000001000000000010000000000000000100000000000\"<br>\n",
    ">>> seg2 = \"0100100100100001001001000010100100010010000100010010000\"<br>\n",
    "</div>    \n",
    "Observe that the segmentation strings consist of zeros and ones. They are one character shorter than the source text, since a text of length n can be broken up in only n–1 places. The segment() function in Example 3-3 demonstrates that we can get back to the original segmented text from its representation.\n",
    "\n",
    "<i>Example 3-3. Reconstruct segmented text from string representation: seg1 and seg2 represent the initial and final segmentations of some hypothetical child-directed speech; the segment() function can use them to reproduce the segmented text.</i>\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "def segment(text, segs):<br>\n",
    "    words = []<br>\n",
    "    last = 0<br>\n",
    "    for i in range(len(segs)):<br>\n",
    "        if segs[i] == '1':<br>\n",
    "            words.append(text[last:i+1])<br>\n",
    "            last = i+1<br>\n",
    "    words.append(text[last:])<br>\n",
    "    return words<br>\n",
    ">>> text = \"doyouseethekittyseethedoggydoyoulikethekittylikethedoggy\"<br>\n",
    ">>> seg1 = \"0000000000000001000000000010000000000000000100000000000\"<br>\n",
    ">>> seg2 = \"0100100100100001001001000010100100010010000100010010000\"<br>\n",
    ">>> segment(text, seg1)<br>\n",
    "['doyouseethekitty', 'seethedoggy', 'doyoulikethekitty', 'likethedoggy']<br>\n",
    ">>> segment(text, seg2)<br>\n",
    "['do', 'you', 'see', 'the', 'kitty', 'see', 'the', 'doggy', 'do', 'you',<br>\n",
    " 'like', 'the', kitty', 'like', 'the', 'doggy']\n",
    "</div>    \n",
    "    \n",
    "Now the segmentation task becomes a search problem: find the bit string that causes the text string to be correctly segmented into words. We assume the learner is acquiring words and storing them in an internal lexicon. Given a suitable lexicon, it is possible to reconstruct the source text as a sequence of lexical items. Following (Brent & Cartwright, 1995), we can define an objective function, a scoring function whose value we will try to optimize, based on the size of the lexicon and the amount of information needed to reconstruct the source text from the lexicon. We illustrate this in Figure 3-6.\n",
    "    \n",
    "![figure 3-6](segmentation.png)\n",
    "\n",
    "<i>Figure 3-6. Calculation of objective function: Given a hypothetical segmentation of the source text (on the left), derive a lexicon and a derivation table that permit the source text to be reconstructed, then total up the number of characters used by each lexical item (including a boundary marker) and each derivation, to serve as a score of the quality of the segmentation; smaller values of the score indicate a better segmentation.</i>\n",
    "\n",
    "It is a simple matter to implement this objective function, as shown in Example 3-4.\n",
    "\n",
    "<i>Example 3-4. Computing the cost of storing the lexicon and reconstructing the source text.</i>\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "def evaluate(text, segs):<br>\n",
    "    words = segment(text, segs)<br>\n",
    "    text_size = len(words)<br>\n",
    "    lexicon_size = len(' '.join(list(set(words))))<br>\n",
    "    return text_size + lexicon_size\n",
    "</div>\n",
    "    \n",
    "<div class=\"alert alert-block alert-info\">\n",
    ">>> text = \"doyouseethekittyseethedoggydoyoulikethekittylikethedoggy\"<br>\n",
    ">>> seg1 = \"0000000000000001000000000010000000000000000100000000000\"<br>\n",
    ">>> seg2 = \"0100100100100001001001000010100100010010000100010010000\"<br>\n",
    ">>> seg3 = \"0000100100000011001000000110000100010000001100010000001\"<br>\n",
    ">>> segment(text, seg3)<br>\n",
    "['doyou', 'see', 'thekitt', 'y', 'see', 'thedogg', 'y', 'doyou', 'like',<br>\n",
    " 'thekitt', 'y', 'like', 'thedogg', 'y']<br>\n",
    ">>> evaluate(text, seg3)<br>\n",
    "46<br>\n",
    ">>> evaluate(text, seg2)<br>\n",
    "47<br>\n",
    ">>> evaluate(text, seg1)<br>\n",
    "63<br>\n",
    "</div>\n",
    "\n",
    "The final step is to search for the pattern of zeros and ones that minimizes this objective function, shown in Example 3-5. Notice that the best segmentation includes “words” like thekitty, since there’s not enough evidence in the data to split this any further.\n",
    "\n",
    "<i>Example 3-5. Non-deterministic search using simulated annealing: Begin searching with phrase segmentations only; randomly perturb the zeros and ones proportional to the “temperature”; with each iteration the temperature is lowered and the perturbation of boundaries is reduced.</i>\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "from random import randint<br>\n",
    "<br>\n",
    "def flip(segs, pos):<br>\n",
    "    return segs[:pos] + str(1-int(segs[pos])) + segs[pos+1:] <br>\n",
    "<br>\n",
    "def flip_n(segs, n):<br>\n",
    "    for i in range(n):<br>\n",
    "        segs = flip(segs, randint(0,len(segs)-1))<br>\n",
    "    return segs<br>\n",
    "<br>\n",
    "def anneal(text, segs, iterations, cooling_rate):<br>\n",
    "    temperature = float(len(segs))<br>\n",
    "    while temperature > 0.5:<br>\n",
    "        best_segs, best = segs, evaluate(text, segs)<br>\n",
    "        for i in range(iterations):<br>\n",
    "            guess = flip_n(segs, int(round(temperature)))<br>\n",
    "            score = evaluate(text, guess)<br>\n",
    "            if score < best:<br>\n",
    "                best, best_segs = score, guess<br>\n",
    "        score, segs = best, best_segs<br>\n",
    "        temperature = temperature / cooling_rate<br>\n",
    "        print evaluate(text, segs), segment(text, segs)<br>\n",
    "    print<br>\n",
    "    return segs\n",
    "</div>\n",
    "                            \n",
    "<div class=\"alert alert-block alert-info\">\n",
    ">>> text = \"doyouseethekittyseethedoggydoyoulikethekittylikethedoggy\"<br>\n",
    ">>> seg1 = \"0000000000000001000000000010000000000000000100000000000\"<br>\n",
    ">>> anneal(text, seg1, 5000, 1.2)<br>\n",
    "60 ['doyouseetheki', 'tty', 'see', 'thedoggy', 'doyouliketh', 'ekittylike', 'thedoggy']<br>\n",
    "58 ['doy', 'ouseetheki', 'ttysee', 'thedoggy', 'doy', 'o', 'ulikethekittylike', 'thedoggy']<br>\n",
    "56 ['doyou', 'seetheki', 'ttysee', 'thedoggy', 'doyou', 'liketh', 'ekittylike', 'thedoggy']<br>\n",
    "54 ['doyou', 'seethekit', 'tysee', 'thedoggy', 'doyou', 'likethekittylike', 'thedoggy']<br>\n",
    "53 ['doyou', 'seethekit', 'tysee', 'thedoggy', 'doyou', 'like', 'thekitty', 'like', 'thedoggy']<br>\n",
    "51 ['doyou', 'seethekittysee', 'thedoggy', 'doyou', 'like', 'thekitty', 'like', 'thedoggy']<br>\n",
    "42 ['doyou', 'see', 'thekitty', 'see', 'thedoggy', 'doyou', 'like', 'thekitty', 'like', 'thedoggy']<br>\n",
    "'0000100100000001001000000010000100010000000100010000000'\n",
    "</div>\n",
    "    \n",
    "With enough data, it is possible to automatically segment text into words with a reasonable degree of accuracy. Such methods can be applied to tokenization for writing systems that don’t have any visual representation of word boundaries."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5f8d52d",
   "metadata": {},
   "source": [
    "## Formatting: From Lists to Strings\n",
    "Often we write a program to report a single data item, such as a particular element in a corpus that meets some complicated criterion, or a single summary statistic such as a word-count or the performance of a tagger. More often, we write a program to produce a structured result; for example, a tabulation of numbers or linguistic forms, or a reformatting of the original data. When the results to be presented are linguistic, textual output is usually the most natural choice. However, when the results are numerical, it may be preferable to produce graphical output. In this section, you will learn about a variety of ways to present program output.\n",
    "\n",
    "### From Lists to Strings\n",
    "The simplest kind of structured object we use for text processing is lists of words. When we want to output these to a display or a file, we must convert these lists into strings. To do this in Python we use the join() method, and specify the string to be used as the “glue”:\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    ">>> silly = ['We', 'called', 'him', 'Tortoise', 'because', 'he', 'taught', 'us', '.']<br>\n",
    ">>> ' '.join(silly)<br>\n",
    "'We called him Tortoise because he taught us .'<br>\n",
    ">>> ';'.join(silly)<br>\n",
    "'We;called;him;Tortoise;because;he;taught;us;.'<br>\n",
    ">>> ''.join(silly)<br>\n",
    "'WecalledhimTortoisebecausehetaughtus.'<br>\n",
    "</div> \n",
    "    \n",
    "So ' '.join(silly) means: take all the items in silly and concatenate them as one big string, using ' ' as a spacer between the items. I.e., join() is a method of the string that you want to use as the glue. (Many people find this notation for join() counter-intuitive.) The join() method only works on a list of strings—what we have been calling a text—a complex type that enjoys some privileges in Python.\n",
    "\n",
    "### Strings and Formats\n",
    "We have seen that there are two ways to display the contents of an object:\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    ">>> word = 'cat'<br>\n",
    ">>> sentence = \"\"\"hello<br>\n",
    "... world\"\"\"<br>\n",
    ">>> print word<br>\n",
    "cat<br>\n",
    ">>> print sentence<br>\n",
    "hello<br>\n",
    "world<br>\n",
    ">>> word<br>\n",
    "'cat'<br>\n",
    ">>> sentence<br>\n",
    "'hello\\nworld'\n",
    "</div> \n",
    "    \n",
    "The print command yields Python’s attempt to produce the most human-readable form of an object. The second method—naming the variable at a prompt—shows us a string that can be used to recreate this object. It is important to keep in mind that both of these are just strings, displayed for the benefit of you, the user. They do not give us any clue as to the actual internal representation of the object.\n",
    "\n",
    "There are many other useful ways to display an object as a string of characters. This may be for the benefit of a human reader, or because we want to export our data to a particular file format for use in an external program.\n",
    "\n",
    "Formatted output typically contains a combination of variables and pre-specified strings. For example, given a frequency distribution fdist, we could do:\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    ">>> fdist = nltk.FreqDist(['dog', 'cat', 'dog', 'cat', 'dog', 'snake', 'dog', 'cat'])<br>\n",
    ">>> for word in fdist:<br>\n",
    "...     print word, '->', fdist[word], ';',<br>\n",
    "dog -> 4 ; cat -> 3 ; snake -> 1 ;\n",
    "</div>\n",
    "    \n",
    "Apart from the problem of unwanted whitespace, print statements that contain alternating variables and constants can be difficult to read and maintain. A better solution is to use string formatting expressions.\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    ">>> for word in fdist:<br>\n",
    "...    print '%s->%d;' % (word, fdist[word]),<br>\n",
    "dog->4; cat->3; snake->1;\n",
    "</div>\n",
    "    \n",
    "To understand what is going on here, let’s test out the string formatting expression on its own. (By now this will be your usual method of exploring new syntax.)\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    ">>> '%s->%d;' % ('cat', 3)<br>\n",
    "'cat->3;'<br>\n",
    ">>> '%s->%d;' % 'cat'<br>\n",
    "Traceback (most recent call last):<br>\n",
    "  File \"<stdin>\", line 1, in <module><br>\n",
    "TypeError: not enough arguments for format string<br>\n",
    "</div>\n",
    "    \n",
    "The special symbols %s and %d are placeholders for strings and (decimal) integers. We can embed these inside a string, then use the % operator to combine them. Let’s unpack this code further, in order to see this behavior up close:\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    ">>> '%s->' % 'cat'<br>\n",
    "'cat->'<br>\n",
    ">>> '%d' % 3<br>\n",
    "'3'<br>\n",
    ">>> 'I want a %s right now' % 'coffee'<br>\n",
    "'I want a coffee right now'<br>\n",
    "</div>\n",
    "    \n",
    "We can have a number of placeholders, but following the % operator we need to specify a tuple with exactly the same number of values:\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    ">>> \"%s wants a %s %s\" % (\"Lee\", \"sandwich\", \"for lunch\")<br>\n",
    "'Lee wants a sandwich for lunch'\n",
    "</div> \n",
    "    \n",
    "We can also provide the values for the placeholders indirectly. Here’s an example using a for loop:\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    ">>> template = 'Lee wants a %s right now'<br>\n",
    ">>> menu = ['sandwich', 'spam fritter', 'pancake']<br>\n",
    ">>> for snack in menu:<br>\n",
    "...     print template % snack<br>\n",
    "...<br>\n",
    "Lee wants a sandwich right now<br>\n",
    "Lee wants a spam fritter right now<br>\n",
    "Lee wants a pancake right now\n",
    "</div> \n",
    "    \n",
    "The %s and %d symbols are called conversion specifiers. They start with the % character and end with a conversion character such as s (for string) or d (for decimal integer) The string containing conversion specifiers is called a format string. We combine a format string with the % operator and a tuple of values to create a complete string formatting expression.\n",
    "\n",
    "### Lining Things Up\n",
    "So far our formatting strings generated output of arbitrary width on the page (or screen), such as %s and %d. We can specify a width as well, such as %6s, producing a string that is padded to width 6. It is right-justified by default 1, but we can include a minus sign to make it left-justified 2. In case we don’t know in advance how wide a displayed value should be, the width value can be replaced with a star in the formatting string, then specified using a variable 3.\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    ">>> '%6s' % 'dog' 1<br>\n",
    "'   dog'<br>\n",
    ">>> '%-6s' % 'dog' 2<br>\n",
    "'dog   '<br>\n",
    ">>> width = 6<br>\n",
    ">>> '%-*s' % (width, 'dog') 3<br>\n",
    "'dog   '\n",
    "</div>    \n",
    "    \n",
    "Other control characters are used for decimal integers and floating-point numbers. Since the percent character % has a special interpretation in formatting strings, we have to precede it with another % to get it in the output.\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    ">>> count, total = 3205, 9375<br>\n",
    ">>> \"accuracy for %d words: %2.4f%%\" % (total, 100 * count / total)<br>\n",
    "'accuracy for 9375 words: 34.1867%'\n",
    "</div>\n",
    "    \n",
    "An important use of formatting strings is for tabulating data. Recall that in Accessing Text Corpora we saw data being tabulated from a conditional frequency distribution. Let’s perform the tabulation ourselves, exercising full control of headings and column widths, as shown in Example 3-6. Note the clear separation between the language processing work, and the tabulation of results.\n",
    "\n",
    "<i>Example 3-6. Frequency of modals in different sections of the Brown Corpus.</i>\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "def tabulate(cfdist, words, categories):<br>\n",
    "    print '%-16s' % 'Category',<br>\n",
    "    for word in words:                                  # column headings<br>\n",
    "        print '%6s' % word,<br>\n",
    "    print<br>\n",
    "    for category in categories:<br>\n",
    "        print '%-16s' % category,                       # row heading<br>\n",
    "        for word in words:                              # for each word<br>\n",
    "            print '%6d' % cfdist[category][word],       # print table cell<br>\n",
    "        print                                           # end the row<br>\n",
    "</div>\n",
    "    \n",
    "<div class=\"alert alert-block alert-info\">\n",
    ">>> from nltk.corpus import brown<br>\n",
    ">>> cfd = nltk.ConditionalFreqDist(<br>\n",
    "...           (genre, word)<br>\n",
    "...           for genre in brown.categories()<br>\n",
    "...           for word in brown.words(categories=genre))<br>\n",
    ">>> genres = ['news', 'religion', 'hobbies', 'science_fiction', 'romance', 'humor']<br>\n",
    ">>> modals = ['can', 'could', 'may', 'might', 'must', 'will']<br>\n",
    ">>> tabulate(cfd, modals, genres)<br>\n",
    "Category            can  could    may  might   must   will<br>\n",
    "news                 93     86     66     38     50    389<br>\n",
    "religion             82     59     78     12     54     71<br>\n",
    "hobbies             268     58    131     22     83    264<br>\n",
    "science_fiction      16     49      4     12      8     16<br>\n",
    "romance              74    193     11     51     45     43<br>\n",
    "humor                16     30      8      8      9     13<br>\n",
    "</div>\n",
    "    \n",
    "Recall from the listing in Example 3-1 that we used a formatting string \"%*s\". This allows us to specify the width of a field using a variable.\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    ">>> '%*s' % (15, \"Monty Python\")<br>\n",
    "'   Monty Python'\n",
    "</div>\n",
    "    \n",
    "We could use this to automatically customize the column to be just wide enough to accommodate all the words, using width = max(len(w) for w in words). Remember that the comma at the end of print statements adds an extra space, and this is sufficient to prevent the column headings from running into each other.\n",
    "\n",
    "### Writing Results to a File\n",
    "We have seen how to read text from files (Accessing Text from the Web and from Disk). It is often useful to write output to files as well. The following code opens a file output.txt for writing, and saves the program output to the file.\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    ">>> output_file = open('output.txt', 'w')<br>\n",
    ">>> words = set(nltk.corpus.genesis.words('english-kjv.txt'))<br>\n",
    ">>> for word in sorted(words):<br>\n",
    "...     output_file.write(word + \"\\n\")\n",
    "</div>\n",
    "    \n",
    "***    \n",
    "    NOTE\n",
    "        Your Turn: What is the effect of appending \\n to each string before we write it to the file? \n",
    "        If you’re using a Windows machine, you may want to use word + \"\\r\\n\" instead. What happens if we do?\n",
    "    \n",
    "<div class=\"alert alert-block alert-info\">\n",
    "output_file.write(word)\n",
    "</div>\n",
    "    \n",
    "***    \n",
    "    \n",
    "When we write non-text data to a file, we must convert it to a string first. We can do this conversion using formatting strings, as we saw earlier. Let’s write the total number of words to our file, before closing it.\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    ">>> len(words)<br>\n",
    "2789<br>\n",
    ">>> str(len(words))<br>\n",
    "'2789'<br>\n",
    ">>> output_file.write(str(len(words)) + \"\\n\")<br>\n",
    ">>> output_file.close()\n",
    "</div>\n",
    "    \n",
    "<div class=\"alert alert-block alert-danger\">\n",
    "    <b>Caution!</b>\n",
    "You should avoid filenames that contain space characters, such as output file.txt, or that are identical except for case distinctions, e.g., Output.txt and output.TXT.\n",
    "</div>\n",
    "    \n",
    "### Text Wrapping\n",
    "When the output of our program is text-like, instead of tabular, it will usually be necessary to wrap it so that it can be displayed conveniently. Consider the following output, which overflows its line, and which uses a complicated print statement:\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    ">>> saying = ['After', 'all', 'is', 'said', 'and', 'done', ',',<br>\n",
    "...           'more', 'is', 'said', 'than', 'done', '.']<br>\n",
    ">>> for word in saying:<br>\n",
    "...     print word, '(' + str(len(word)) + '),',\n",
    "</div>\n",
    "    \n",
    "After (5), all (3), is (2), said (4), and (3), done (4), , (1), more (4), is (2), said (4), \n",
    "We can take care of line wrapping with the help of Python’s textwrap module. For maximum clarity we will separate each step onto its own line:\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    ">>> from textwrap import fill<br>\n",
    ">>> format = '%s (%d),'<br>\n",
    ">>> pieces = [format % (word, len(word)) for word in saying]<br>\n",
    ">>> output = ' '.join(pieces)<br>\n",
    ">>> wrapped = fill(output)<br>\n",
    ">>> print wrapped<br>\n",
    "After (5), all (3), is (2), said (4), and (3), done (4), , (1), more<br>\n",
    "(4), is (2), said (4), than (4), done (4), . (1),\n",
    "</div>    \n",
    "    \n",
    "Notice that there is a linebreak between more and its following number. If we wanted to avoid this, we could redefine the formatting string so that it contained no spaces (e.g., '%s_(%d),'), then instead of printing the value of wrapped, we could print wrapped.replace('_', ' ')."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a72bddf",
   "metadata": {},
   "source": [
    "## Summary\n",
    "In this book we view a text as a list of words. A “raw text” is a potentially long string containing words and whitespace formatting, and is how we typically store and visualize a text.\n",
    "\n",
    "A string is specified in Python using single or double quotes: 'Monty Python', \"Monty Python\".\n",
    "\n",
    "The characters of a string are accessed using indexes, counting from zero: 'Monty Python'[0] gives the value M. The length of a string is found using len().\n",
    "\n",
    "Substrings are accessed using slice notation: 'Monty Python'[1:5] gives the value onty. If the start index is omitted, the substring begins at the start of the string; if the end index is omitted, the slice continues to the end of the string.\n",
    "\n",
    "Strings can be split into lists: 'Monty Python'.split() gives ['Monty', 'Python']. Lists can be joined into strings: '/'.join(['Monty', 'Python']) gives 'Monty/Python'.\n",
    "\n",
    "We can read text from a file f using text = open(f).read(). We can read text from a URL u using text = urlopen(u).read(). We can iterate over the lines of a text file using for line in open(f).\n",
    "\n",
    "Texts found on the Web may contain unwanted material (such as headers, footers, and markup), that need to be removed before we do any linguistic processing.\n",
    "\n",
    "Tokenization is the segmentation of a text into basic units—or tokens—such as words and punctuation. Tokenization based on whitespace is inadequate for many applications because it bundles punctuation together with words. NLTK provides an off-the-shelf tokenizer nltk.word_tokenize().\n",
    "\n",
    "Lemmatization is a process that maps the various forms of a word (such as appeared, appears) to the canonical or citation form of the word, also known as the lexeme or lemma (e.g., appear).\n",
    "\n",
    "Regular expressions are a powerful and flexible method of specifying patterns. Once we have imported the re module, we can use re.findall() to find all substrings in a string that match a pattern.\n",
    "\n",
    "If a regular expression string includes a backslash, you should tell Python not to preprocess the string, by using a raw string with an r prefix: r'regexp'.\n",
    "\n",
    "When backslash is used before certain characters, e.g., \\n, this takes on a special meaning (newline character); however, when backslash is used before regular expression wildcards and operators, e.g., \\., \\|, \\$, these characters lose their special meaning and are matched literally.\n",
    "\n",
    "A string formatting expression template % arg_tuple consists of a format string template that contains conversion specifiers like %-6s and %0.2d."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62e82dff",
   "metadata": {},
   "source": [
    "## Further Reading\n",
    "Extra materials for this chapter are posted at http://www.nltk.org/, including links to freely available resources on the Web. Remember to consult the Python reference materials at http://docs.python.org/. (For example, this documentation covers “universal newline support,” explaining how to work with the different newline conventions used by various operating systems.)\n",
    "\n",
    "For more examples of processing words with NLTK, see the tokenization, stemming, and corpus HOWTOs at http://www.nltk.org/howto. Chapters 2 and 3 of (Jurafsky & Martin, 2008) contain more advanced material on regular expressions and morphology. For more extensive discussion of text processing with Python, see (Mertz, 2003). For information about normalizing non-standard words, see (Sproat et al., 2001).\n",
    "\n",
    "There are many references for regular expressions, both practical and theoretical. For an introductory tutorial to using regular expressions in Python, see Kuchling’s Regular Expression HOWTO, http://www.amk.ca/python/howto/regex/. For a comprehensive and detailed manual in using regular expressions, covering their syntax in most major programming languages, including Python, see (Friedl, 2002). Other presentations include Section 2.1 of (Jurafsky & Martin, 2008), and Chapter 3 of (Mertz, 2003).\n",
    "\n",
    "There are many online resources for Unicode. Useful discussions of Python’s facilities for handling Unicode are:\n",
    "\n",
    "PEP-100 http://www.python.org/dev/peps/pep-0100/\n",
    "\n",
    "Jason Orendorff, Unicode for Programmers, http://www.jorendorff.com/articles/unicode/\n",
    "\n",
    "A. M. Kuchling, Unicode HOWTO, http://www.amk.ca/python/howto/unicode\n",
    "\n",
    "Frederik Lundh, Python Unicode Objects, http://effbot.org/zone/unicode-objects.htm\n",
    "\n",
    "Joel Spolsky, The Absolute Minimum Every Software Developer Absolutely, Positively Must Know About Unicode and Character Sets (No Excuses!), http://www.joelonsoftware.com/articles/Unicode.html\n",
    "\n",
    "The problem of tokenizing Chinese text is a major focus of SIGHAN, the ACL Special Interest Group on Chinese Language Processing (http://sighan.org/). Our method for segmenting English text follows (Brent & Cartwright, 1995); this work falls in the area of language acquisition (Niyogi, 2006).\n",
    "\n",
    "Collocations are a special case of multiword expressions. A multiword expression is a small phrase whose meaning and other properties cannot be predicted from its words alone, e.g., part-of-speech (Baldwin & Kim, 2010).\n",
    "\n",
    "Simulated annealing is a heuristic for finding a good approximation to the optimum value of a function in a large, discrete search space, based on an analogy with annealing in metallurgy. The technique is described in many Artificial Intelligence texts.\n",
    "\n",
    "The approach to discovering hyponyms in text using search patterns like x and other ys is described by (Hearst, 1992)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07b7ef9e",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "○ Define a string s = 'colorless'. Write a Python statement that changes this to “colourless” using only the slice and concatenation operations.\n",
    "\n",
    "○ We can use the slice notation to remove morphological endings on words. For example, 'dogs'[:-1] removes the last character of dogs, leaving dog. Use slice notation to remove the affixes from these words (we’ve inserted a hyphen to indicate the affix boundary, but omit this from your strings): dish-es, run-ning, nation-ality, un-do, pre-heat.\n",
    "\n",
    "○ We saw how we can generate an IndexError by indexing beyond the end of a string. Is it possible to construct an index that goes too far to the left, before the start of the string?\n",
    "\n",
    "○ We can specify a “step” size for the slice. The following returns every second character within the slice: monty[6:11:2]. It also works in the reverse direction: monty[10:5:-2]. Try these for yourself, and then experiment with different step values.\n",
    "\n",
    "○ What happens if you ask the interpreter to evaluate monty[::-1]? Explain why this is a reasonable result.\n",
    "\n",
    "○ Describe the class of strings matched by the following regular expressions:\n",
    "<ul>\n",
    "<li>[a-zA-Z]+</li>\n",
    "\n",
    "<li>[A-Z][a-z]*</li>\n",
    "\n",
    "<li>p[aeiou]{,2}t</li>\n",
    "\n",
    "<li>\\d+(\\.\\d+)?</li>\n",
    "\n",
    "<li>([^aeiou][aeiou][^aeiou])*</li>\n",
    "\n",
    "<li>\\w+|[^\\w\\s]+</li>\n",
    "</ul>\n",
    "Test your answers using nltk.re_show().\n",
    "\n",
    "○ Write regular expressions to match the following classes of strings:\n",
    "\n",
    "A single determiner (assume that a, an, and the are the only determiners)\n",
    "\n",
    "An arithmetic expression using integers, addition, and multiplication, such as 2*3+8\n",
    "\n",
    "○ Write a utility function that takes a URL as its argument, and returns the contents of the URL, with all HTML markup removed. Use urllib.urlopen to access the contents of the URL, e.g.:\n",
    "\n",
    "raw_contents = urllib.urlopen('http://www.nltk.org/').read()\n",
    "○ Save some text into a file corpus.txt. Define a function load(f) that reads from the file named in its sole argument, and returns a string containing the text of the file.\n",
    "\n",
    "Use nltk.regexp_tokenize() to create a tokenizer that tokenizes the various kinds of punctuation in this text. Use one multiline regular expression inline comments, using the verbose flag (?x).\n",
    "\n",
    "Use nltk.regexp_tokenize() to create a tokenizer that tokenizes the following kinds of expressions: monetary amounts; dates; names of people and organizations.\n",
    "\n",
    "○ Rewrite the following loop as a list comprehension:\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    ">>> sent = ['The', 'dog', 'gave', 'John', 'the', 'newspaper']<br>\n",
    ">>> result = []<br>\n",
    ">>> for word in sent:<br>\n",
    "...     word_len = (word, len(word))<br>\n",
    "...     result.append(word_len)<br>\n",
    ">>> result<br>\n",
    "[('The', 3), ('dog', 3), ('gave', 4), ('John', 4), ('the', 3), ('newspaper', 9)]\n",
    "</div>    \n",
    "    \n",
    "○ Define a string raw containing a sentence of your own choosing. Now, split raw on some character other than space, such as 's'.\n",
    "\n",
    "○ Write a for loop to print out the characters of a string, one per line.\n",
    "\n",
    "○ What is the difference between calling split on a string with no argument and one with ' ' as the argument, e.g., sent.split() versus sent.split(' ')? What happens when the string being split contains tab characters, consecutive space characters, or a sequence of tabs and spaces? (In IDLE you will need to use '\\t' to enter a tab character.)\n",
    "\n",
    "○ Create a variable words containing a list of words. Experiment with words.sort() and sorted(words). What is the difference?\n",
    "\n",
    "○ Explore the difference between strings and integers by typing the following at a Python prompt: \"3\" * 7 and 3 * 7. Try converting between strings and integers using int(\"3\") and str(3).\n",
    "\n",
    "○ Earlier, we asked you to use a text editor to create a file called test.py, containing the single line monty = 'Monty Python'. If you haven’t already done this (or can’t find the file), go ahead and do it now. Next, start up a new session with the Python interpreter, and enter the expression monty at the prompt. You will get an error from the interpreter. Now, try the following (note that you have to leave off the .py part of the filename):\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    ">>> from test import msg<br>\n",
    ">>> msg<br>\n",
    "</div>    \n",
    "    \n",
    "This time, Python should return with a value. You can also try import test, in which case Python should be able to evaluate the expression test.monty at the prompt.\n",
    "\n",
    "○ What happens when the formatting strings %6s and %-6s are used to display strings that are longer than six characters?\n",
    "\n",
    " Read in some text from a corpus, tokenize it, and print the list of all wh-word types that occur. (wh-words in English are used in questions, relative clauses, and exclamations: who, which, what, and so on.) Print them in order. Are any words duplicated in this list, because of the presence of case distinctions or punctuation?\n",
    "\n",
    " Create a file consisting of words and (made up) frequencies, where each line consists of a word, the space character, and a positive integer, e.g., fuzzy 53. Read the file into a Python list using open(filename).readlines(). Next, break each line into its two fields using split(), and convert the number into an integer using int(). The result should be a list of the form: [['fuzzy', 53], ...].\n",
    "\n",
    " Write code to access a favorite web page and extract some text from it. For example, access a weather site and extract the forecast top temperature for your town or city today.\n",
    "\n",
    " Write a function unknown() that takes a URL as its argument, and returns a list of unknown words that occur on that web page. In order to do this, extract all substrings consisting of lowercase letters (using re.findall()) and remove any items from this set that occur in the Words Corpus (nltk.corpus.words). Try to categorize these words manually and discuss your findings.\n",
    "\n",
    " Examine the results of processing the URL http://news.bbc.co.uk/ using the regular expressions suggested above. You will see that there is still a fair amount of non-textual data there, particularly JavaScript commands. You may also find that sentence breaks have not been properly preserved. Define further regular expressions that improve the extraction of text from this web page.\n",
    "\n",
    " Are you able to write a regular expression to tokenize text in such a way that the word don’t is tokenized into do and n’t? Explain why this regular expression won’t work: «n't|\\w+».\n",
    "\n",
    " Try to write code to convert text into hAck3r, using regular expressions and substitution, where e → 3, i → 1, o → 0, l → |, s → 5, . → 5w33t!, ate → 8. Normalize the text to lowercase before converting it. Add more substitutions of your own. Now try to map s to two different values: $ for word-initial s, and 5 for word-internal s.\n",
    "\n",
    " Pig Latin is a simple transformation of English text. Each word of the text is converted as follows: move any consonant (or consonant cluster) that appears at the start of the word to the end, then append ay, e.g., string → ingstray, idle → idleay (see http://en.wikipedia.org/wiki/Pig_Latin).\n",
    "\n",
    "Write a function to convert a word to Pig Latin.\n",
    "\n",
    "Write code that converts text, instead of individual words.\n",
    "\n",
    "Extend it further to preserve capitalization, to keep qu together (so that quiet becomes ietquay, for example), and to detect when y is used as a consonant (e.g., yellow) versus a vowel (e.g., style).\n",
    "\n",
    " Download some text from a language that has vowel harmony (e.g., Hungarian), extract the vowel sequences of words, and create a vowel bigram table.\n",
    "\n",
    " Python’s random module includes a function choice() which randomly chooses an item from a sequence; e.g., choice(\"aehh \") will produce one of four possible characters, with the letter h being twice as frequent as the others. Write a generator expression that produces a sequence of 500 randomly chosen letters drawn from the string \"aehh \", and put this expression inside a call to the ''.join() function, to concatenate them into one long string. You should get a result that looks like uncontrolled sneezing or maniacal laughter: he haha ee heheeh eha. Use split() and join() again to normalize the whitespace in this string.\n",
    "\n",
    " Consider the numeric expressions in the following sentence from the MedLine Corpus: The corresponding free cortisol fractions in these sera were 4.53 +/- 0.15% and 8.16 +/- 0.23%, respectively. Should we say that the numeric expression 4.53 +/- 0.15% is three words? Or should we say that it’s a single compound word? Or should we say that it is actually nine words, since it’s read “four point five three, plus or minus fifteen percent”? Or should we say that it’s not a “real” word at all, since it wouldn’t appear in any dictionary? Discuss these different possibilities. Can you think of application domains that motivate at least two of these answers?\n",
    "\n",
    " Readability measures are used to score the reading difficulty of a text, for the purposes of selecting texts of appropriate difficulty for language learners. Let us define μw to be the average number of letters per word, and μs to be the average number of words per sentence, in a given text. The Automated Readability Index (ARI) of the text is defined to be: 4.71 μw + 0.5 μs - 21.43. Compute the ARI score for various sections of the Brown Corpus, including section f (popular lore) and j (learned). Make use of the fact that nltk.corpus.brown.words() produces a sequence of words, whereas nltk.corpus.brown.sents() produces a sequence of sentences.\n",
    "\n",
    " Use the Porter Stemmer to normalize some tokenized text, calling the stemmer on each word. Do the same thing with the Lancaster Stemmer, and see if you observe any differences.\n",
    "\n",
    " Define the variable saying to contain the list ['After', 'all', 'is', 'said', 'and', 'done', ',', 'more', 'is', 'said', 'than', 'done', '.']. Process the list using a for loop, and store the result in a new list lengths. Hint: begin by assigning the empty list to lengths, using lengths = []. Then each time through the loop, use append() to add another length value to the list.\n",
    "\n",
    " Define a variable silly to contain the string: 'newly formed bland ideas are inexpressible in an infuriating way'. (This happens to be the legitimate interpretation that bilingual English-Spanish speakers can assign to Chomsky’s famous nonsense phrase colorless green ideas sleep furiously, according to Wikipedia). Now write code to perform the following tasks:\n",
    "\n",
    "Split silly into a list of strings, one per word, using Python’s split() operation, and save this to a variable called bland.\n",
    "\n",
    "Extract the second letter of each word in silly and join them into a string, to get 'eoldrnnnna'.\n",
    "\n",
    "Combine the words in bland back into a single string, using join(). Make sure the words in the resulting string are separated with whitespace.\n",
    "\n",
    "Print the words of silly in alphabetical order, one per line.\n",
    "\n",
    " The index() function can be used to look up items in sequences. For example, 'inexpressible'.index('e') tells us the index of the first position of the letter e.\n",
    "\n",
    "What happens when you look up a substring, e.g., 'inexpressible'.index('re')?\n",
    "\n",
    "Define a variable words containing a list of words. Now use words.index() to look up the position of an individual word.\n",
    "\n",
    "Define a variable silly as in Exercise 32. Use the index() function in combination with list slicing to build a list phrase consisting of all the words up to (but not including) in in silly.\n",
    "\n",
    " Write code to convert nationality adjectives such as Canadian and Australian to their corresponding nouns Canada and Australia (see http://en.wikipedia.org/wiki/List_of_adjectival_forms_of_place_names).\n",
    "\n",
    " Read the LanguageLog post on phrases of the form as best as p can and as best p can, where p is a pronoun. Investigate this phenomenon with the help of a corpus and the findall() method for searching tokenized text described in Useful Applications of Regular Expressions. The post is at http://itre.cis.upenn.edu/~myl/languagelog/archives/002733.html.\n",
    "\n",
    " Study the lolcat version of the book of Genesis, accessible as nltk.corpus.genesis.words('lolcat.txt'), and the rules for converting text into lolspeak at http://www.lolcatbible.com/index.php?title=How_to_speak_lolcat. Define regular expressions to convert English words into corresponding lolspeak words.\n",
    "\n",
    " Read about the re.sub() function for string substitution using regular expressions, using help(re.sub) and by consulting the further readings for this chapter. Use re.sub in writing code to remove HTML tags from an HTML file, and to normalize whitespace.\n",
    "\n",
    "● An interesting challenge for tokenization is words that have been split across a linebreak. E.g., if long-term is split, then we have the string long-\\nterm.\n",
    "\n",
    "Write a regular expression that identifies words that are hyphenated at a line-break. The expression will need to include the \\n character.\n",
    "\n",
    "Use re.sub() to remove the \\n character from these words.\n",
    "\n",
    "How might you identify words that should not remain hyphenated once the newline is removed, e.g., 'encyclo-\\npedia'?\n",
    "\n",
    "● Read the Wikipedia entry on Soundex. Implement this algorithm in Python.\n",
    "\n",
    "● Obtain raw texts from two or more genres and compute their respective reading difficulty scores as in the earlier exercise on reading difficulty. E.g., compare ABC Rural News and ABC Science News (nltk.corpus.abc). Use Punkt to perform sentence segmentation.\n",
    "\n",
    "● Rewrite the following nested loop as a nested list comprehension:\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    ">>> words = ['attribution', 'confabulation', 'elocution',<br>\n",
    "...          'sequoia', 'tenacious', 'unidirectional']<br>\n",
    ">>> vsequences = set()<br>\n",
    ">>> for word in words:<br>\n",
    "...     vowels = []<br>\n",
    "...     for char in word:<br>\n",
    "...         if char in 'aeiou':<br>\n",
    "...             vowels.append(char)<br>\n",
    "...     vsequences.add(''.join(vowels))<br>\n",
    ">>> sorted(vsequences)<br>\n",
    "['aiuio', 'eaiou', 'eouio', 'euoia', 'oauaio', 'uiieioa']\n",
    "</div>\n",
    "\n",
    "● Use WordNet to create a semantic index for a text collection. Extend the concordance search program in Example 3-1, indexing each word using the offset of its first synset, e.g., wn.synsets('dog')[0].offset (and optionally the offset of some of its ancestors in the hypernym hierarchy).\n",
    "\n",
    "● With the help of a multilingual corpus such as the Universal Declaration of Human Rights Corpus (nltk.corpus.udhr), along with NLTK’s frequency distribution and rank correlation functionality (nltk.FreqDist, nltk.spearman_correlation), develop a system that guesses the language of a previously unseen text. For simplicity, work with a single character encoding and just a few languages.\n",
    "\n",
    "● Write a program that processes a text and discovers cases where a word has been used with a novel sense. For each word, compute the WordNet similarity between all synsets of the word and all synsets of the words in its context. (Note that this is a crude approach; doing it well is a difficult, open research problem.)\n",
    "\n",
    "● Read the article on normalization of non-standard words (Sproat et al., 2001), and implement a similar system for text normalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c971738",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
